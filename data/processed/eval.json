[
  {
    "input": "What paper has ArXiv ID 2102.02779?",
    "output": "ViT: An Image is Worth 16x16 Words"
  },
  {
    "input": "Where can I find the code for Neural Machine Translation by Jointly Learning to Align and Translate?",
    "output": "The code for Neural Machine Translation by Jointly Learning to Align and Translate is available at https://github.com/lisa-groundhog/GroundHog (lisa-groundhog/GroundHog)."
  },
  {
    "input": "Tell me the title of ArXiv paper 2303.06865.",
    "output": "arXiv Query: search_query=&id_list=2303.06865&start=0&max_results=10"
  },
  {
    "input": "Give me an overview of PaLM: Scaling Language Modeling with Pathways.",
    "output": "PaLM: Scaling Language Modeling with Pathways is a 2022 paper in Computation and Language (Natural Language Processing). The implementation is available at lucidrains/PaLM-pytorch which implementation of the specific transformer architecture from palm - scaling language modeling with pathways It is written in Python. The repository has 826 stars, indicating high community interest."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2504.02732&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2504.02732&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at MAXNORM8650/MedAgentSim which medagentsim: self-evolving multi-agent simulations for realistic clinical interactions, miccai 2025 (oral and early accepted) It is written in Python. The repository has 94 stars, indicating high community interest."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2406.07394&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2406.07394&start=0&max_results=10 is a 2024 paper in Machine Learning. The implementation is available at hpcaitech/Open-Sora which open-sora: democratizing efficient video production for all It is written in Python. The repository has 27,981 stars, indicating high community interest."
  },
  {
    "input": "What is the GitHub repository for MRKL Systems: A modular approach to AGI?",
    "output": "The code for MRKL Systems: A modular approach to AGI is available at https://github.com/langchain-ai/langchain (langchain-ai/langchain)."
  },
  {
    "input": "What is the ArXiv category of MMLU: Measuring Massive Multitask Language Understanding?",
    "output": "MMLU: Measuring Massive Multitask Language Understanding is in the cs.CY (cs.CY) category."
  },
  {
    "input": "How many stars does the XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale repository have?",
    "output": "The facebookresearch/fairseq repository has 31,985 stars and 6,630 forks on GitHub."
  },
  {
    "input": "When was the Flamingo: A Visual Language Model for Few-Shot Learning paper published?",
    "output": "The Flamingo: A Visual Language Model for Few-Shot Learning paper was published in 2022."
  },
  {
    "input": "What language is the google-research/bert repository written in?",
    "output": "The BERT: Pre-training of Deep Bidirectional Transformers implementation is written in Python."
  },
  {
    "input": "What's the implementation repository for Animate Anyone: Consistent and Controllable Image-to-Video Synthesis?",
    "output": "The code for Animate Anyone: Consistent and Controllable Image-to-Video Synthesis is available at https://github.com/HumanAIGC/AnimateAnyone (HumanAIGC/AnimateAnyone)."
  },
  {
    "input": "What topics does the Deep Graph Library: A Graph-Centric, Highly-Performant Package implementation cover?",
    "output": "The dmlc/dgl repository covers: deep-learning, graph-neural-networks."
  },
  {
    "input": "Explain the PyTorch: An Imperative Style, High-Performance Deep Learning Library paper.",
    "output": "PyTorch: An Imperative Style, High-Performance Deep Learning Library is a 2019 paper in Machine Learning. The implementation is available at pytorch/pytorch which tensors and dynamic neural networks in python with strong gpu acceleration It is written in Python. The repository has 95,348 stars, indicating high community interest."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2501.19393&start=0&max_results=10 implementation?",
    "output": "The simplescaling/s1 repository has 6,605 stars and 763 forks on GitHub."
  },
  {
    "input": "What year was Neural Message Passing for Quantum Chemistry released?",
    "output": "The Neural Message Passing for Quantum Chemistry paper was published in 2017."
  },
  {
    "input": "What's the implementation repository for Wav2Vec 2.0: A Framework for Self-Supervised Learning of Speech?",
    "output": "The code for Wav2Vec 2.0: A Framework for Self-Supervised Learning of Speech is available at https://github.com/facebookresearch/fairseq (facebookresearch/fairseq)."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2303.04129&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2303.04129&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at HazyResearch/safari which convolutions for sequence modeling It is written in Assembly. The repository has 904 stars, indicating high community interest."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2301.13688&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2301.13688&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Give me an overview of EfficientDet: Scalable and Efficient Object Detection.",
    "output": "EfficientDet: Scalable and Efficient Object Detection is a 2020 paper in Computer Vision and Pattern Recognition. The implementation is available at google/automl which google brain automl It is written in Jupyter Notebook. The repository has 6,425 stars, indicating high community interest."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2504.00698&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2504.00698&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning about?",
    "output": "AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning is a 2022 paper in Machine Learning. The implementation is available at QingruZhang/AdaLoRA which adalora: adaptive budget allocation for parameter-efficient fine-tuning (iclr 2023).  It is written in Python. The repository has 361 stars, indicating high community interest."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2302.03011?",
    "output": "arXiv Query: search_query=&id_list=2302.03011&start=0&max_results=10"
  },
  {
    "input": "Tell me the title of ArXiv paper 2503.00735.",
    "output": "arXiv Query: search_query=&id_list=2503.00735&start=0&max_results=10"
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2109.03264&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2109.03264&start=0&max_results=10 is a 2021 paper in Machine Learning. The implementation is available at facebookresearch/fairseq which facebook ai research sequence-to-sequence toolkit written in python. It is written in Python. The repository has 31,988 stars, indicating high community interest."
  },
  {
    "input": "How many stars does the Mistral 7B repository have?",
    "output": "The mistralai/mistral-inference repository has 10,550 stars and 990 forks on GitHub."
  },
  {
    "input": "What category is the Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context paper in?",
    "output": "Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Where can I find the code for Animate Anyone: Consistent and Controllable Image-to-Video Synthesis?",
    "output": "The code for Animate Anyone: Consistent and Controllable Image-to-Video Synthesis is available at https://github.com/HumanAIGC/AnimateAnyone (HumanAIGC/AnimateAnyone)."
  },
  {
    "input": "What category is the SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities paper in?",
    "output": "SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2004.13637&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2004.13637&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 1412.6980?",
    "output": "Adam: A Method for Stochastic Optimization"
  },
  {
    "input": "Explain the tkipf/gcn implementation of Semi-Supervised Classification with Graph Convolutional Networks.",
    "output": "Implementation of Graph Convolutional Networks in TensorFlow"
  },
  {
    "input": "Describe the facebookresearch/contriever repository that implements Contriever: Unsupervised Dense Information Retrieval.",
    "output": "Contriever: Unsupervised Dense Information Retrieval with Contrastive Learning"
  },
  {
    "input": "What is the ArXiv category of Text Embeddings by Weakly-Supervised Contrastive Pre-training?",
    "output": "Text Embeddings by Weakly-Supervised Contrastive Pre-training is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2302.03917&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2302.03917&start=0&max_results=10 is available at https://github.com/mit-han-lab/offsite-tuning (mit-han-lab/offsite-tuning)."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2302.00923&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2302.00923&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at amazon-science/mm-cot which official implementation for \"multimodal chain-of-thought reasoning in language models\" (stay tuned and more will be updated) It is written in Python. The repository has 3,982 stars, indicating high community interest."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2406.10209&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2406.10209&start=0&max_results=10 is available at https://github.com/hpcaitech/Open-Sora (hpcaitech/Open-Sora)."
  },
  {
    "input": "What is the GitHub repository for Question and Answer Test-Train Overlap in Open-Domain QA Datasets?",
    "output": "The code for Question and Answer Test-Train Overlap in Open-Domain QA Datasets is available at https://github.com/google-research-datasets/natural-questions (google-research-datasets/natural-questions)."
  },
  {
    "input": "Describe the stanford-futuredata/ColBERT repository that implements ColBERT: Efficient Passage Search via Contextualized Late Interaction.",
    "output": "ColBERT: state-of-the-art neural search (SIGIR'20, TACL'21, NeurIPS'21, NAACL'22, CIKM'22, ACL'23, EMNLP'23)"
  },
  {
    "input": "Summarize the Learning to summarize from human feedback paper.",
    "output": "Learning to summarize from human feedback is a 2020 paper in Computation and Language (Natural Language Processing). The implementation is available at openai/summarize-from-feedback which code for \"learning to summarize from human feedback\" It is written in Python. The repository has 1,055 stars, indicating high community interest."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2503.01141&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2503.01141&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at LTH14/fractalgen which pytorch implementation of fractalgen https://arxiv.org/abs/2502.17437 It is written in Python. The repository has 1,196 stars, indicating high community interest."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2410.12896&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2410.12896&start=0&max_results=10 is available at https://github.com/ibm-granite/granite-3.0-language-models (ibm-granite/granite-3.0-language-models)."
  },
  {
    "input": "Explain the Code Llama: Open Foundation Models for Code paper.",
    "output": "Code Llama: Open Foundation Models for Code is a 2023 paper in Machine Learning. The implementation is available at meta-llama/codellama which inference code for codellama models It is written in Python. The repository has 16,365 stars, indicating high community interest."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2504.20734&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2504.20734&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2301.00303&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2301.00303&start=0&max_results=10 is a 2022 paper in Machine Learning. The implementation is available at facebookresearch/ConvNeXt-V2 which code release for convnext v2 model It is written in Python. The repository has 1,890 stars, indicating high community interest."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2504.02732&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2504.02732&start=0&max_results=10 is available at https://github.com/MAXNORM8650/MedAgentSim (MAXNORM8650/MedAgentSim)."
  },
  {
    "input": "What are the key topics in the deepspeedai/DeepSpeed repository?",
    "output": "The deepspeedai/DeepSpeed repository covers: deep-learning, pytorch, gpu, machine-learning, billion-parameters, data-parallelism, model-parallelism, inference, pipeline-parallelism, compression, mixture-of-experts, trillion-parameters, zero."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2006.03511&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2006.03511&start=0&max_results=10 is a 2020 paper in Machine Learning. The implementation is available at facebookresearch/TransCoder which public release of the transcoder research project https://arxiv.org/pdf/2006.03511.pdf It is written in Python. The repository has 1,722 stars, indicating high community interest."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2303.02913?",
    "output": "arXiv Query: search_query=&id_list=2303.02913&start=0&max_results=10"
  },
  {
    "input": "What is the GitHub repository for ChemCrow: Augmenting large-language models with chemistry tools?",
    "output": "The code for ChemCrow: Augmenting large-language models with chemistry tools is available at https://github.com/ur-whitelab/chemcrow-public (ur-whitelab/chemcrow-public)."
  },
  {
    "input": "In what year did StarCoder: May the source be with you! come out?",
    "output": "The StarCoder: May the source be with you! paper was published in 2023."
  },
  {
    "input": "Describe the KaimingHe/deep-residual-networks repository that implements Deep Residual Learning for Image Recognition.",
    "output": "Deep Residual Learning for Image Recognition "
  },
  {
    "input": "Describe the facebookresearch/mae repository that implements Masked Autoencoders Are Scalable Vision Learners.",
    "output": "PyTorch implementation of MAE https//arxiv.org/abs/2111.06377"
  },
  {
    "input": "When was the Attention Is All You Need paper published?",
    "output": "The Attention Is All You Need paper was published in 2017."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2312.02120&start=0&max_results=10 repository have?",
    "output": "The ContextualAI/HALOs repository has 893 stars and 49 forks on GitHub."
  },
  {
    "input": "Where can I find the code for Horovod: fast and easy distributed deep learning in TensorFlow?",
    "output": "The code for Horovod: fast and easy distributed deep learning in TensorFlow is available at https://github.com/horovod/horovod (horovod/horovod)."
  },
  {
    "input": "What does the microsoft/Swin-Transformer repository do?",
    "output": "This is an official implementation for \"Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\"."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2303.04226&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2303.04226&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2302.04023&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2302.04023&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at mit-han-lab/offsite-tuning which offsite-tuning: transfer learning without full model It is written in Python. The repository has 382 stars, indicating high community interest."
  },
  {
    "input": "How popular is the Direct Preference Optimization: Your Language Model is a Reward Model implementation?",
    "output": "The eric-mitchell/direct-preference-optimization repository has 2,789 stars and 231 forks on GitHub."
  },
  {
    "input": "Where can I find the code for Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning?",
    "output": "The code for Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning is available at https://github.com/rail-berkeley/softlearning (rail-berkeley/softlearning)."
  },
  {
    "input": "Explain the HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction paper.",
    "output": "HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction is a 2021 paper in Computation and Language (Natural Language Processing). The implementation is available at facebookresearch/fairseq which facebook ai research sequence-to-sequence toolkit written in python. It is written in Python. The repository has 31,985 stars, indicating high community interest."
  },
  {
    "input": "What programming language is HuggingGPT: Solving AI Tasks with ChatGPT implemented in?",
    "output": "The HuggingGPT: Solving AI Tasks with ChatGPT implementation is written in Python."
  },
  {
    "input": "What year was Denoising Diffusion Probabilistic Models released?",
    "output": "The Denoising Diffusion Probabilistic Models paper was published in 2020."
  },
  {
    "input": "Tell me the title of ArXiv paper 2303.09431.",
    "output": "arXiv Query: search_query=&id_list=2303.09431&start=0&max_results=10"
  },
  {
    "input": "What programming language is YOLOX: Exceeding YOLO Series in 2021 implemented in?",
    "output": "The YOLOX: Exceeding YOLO Series in 2021 implementation is written in Python."
  },
  {
    "input": "Summarize the JAX: Composable transformations of Python+NumPy programs paper.",
    "output": "JAX: Composable transformations of Python+NumPy programs is a 2019 paper in cs.MS. The implementation is available at jax-ml/jax which composable transformations of python+numpy programs: differentiate, vectorize, jit to gpu/tpu, and more It is written in Python. The repository has 34,091 stars, indicating high community interest."
  },
  {
    "input": "In what year did Efficient Estimation of Word Representations in Vector Space come out?",
    "output": "The Efficient Estimation of Word Representations in Vector Space paper was published in 2013."
  },
  {
    "input": "Which field does Faster R-CNN: Towards Real-Time Object Detection belong to?",
    "output": "Faster R-CNN: Towards Real-Time Object Detection is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "Where can I find the code for Segment Anything in Medical Images?",
    "output": "The code for Segment Anything in Medical Images is available at https://github.com/bowang-lab/MedSAM (bowang-lab/MedSAM)."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2501.11067&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2501.11067&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Give me an overview of Swin Transformer: Hierarchical Vision Transformer using Shifted Windows.",
    "output": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows is a 2021 paper in Computer Vision and Pattern Recognition. The implementation is available at microsoft/Swin-Transformer which this is an official implementation for \"swin transformer: hierarchical vision transformer using shifted windows\". It is written in Python. The repository has 15,466 stars, indicating high community interest."
  },
  {
    "input": "What is the ArXiv category of BEiT: BERT Pre-Training of Image Transformers?",
    "output": "BEiT: BERT Pre-Training of Image Transformers is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "What year was Decoupled Weight Decay Regularization released?",
    "output": "The Decoupled Weight Decay Regularization paper was published in 2017."
  },
  {
    "input": "What are the key topics in the mit-han-lab/offsite-tuning repository?",
    "output": "The mit-han-lab/offsite-tuning repository covers: deep-learning, transfer-learning."
  },
  {
    "input": "Where can I find the code for Enriching Word Vectors with Subword Information?",
    "output": "The code for Enriching Word Vectors with Subword Information is available at https://github.com/facebookresearch/fastText (facebookresearch/fastText)."
  },
  {
    "input": "Give me an overview of P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning.",
    "output": "P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning is a 2021 paper in Computation and Language (Natural Language Processing). The implementation is available at THUDM/P-tuning-v2 which an optimized deep prompt tuning strategy comparable to fine-tuning across scales and tasks It is written in Python. The repository has 2,067 stars, indicating high community interest."
  },
  {
    "input": "Tell me the title of ArXiv paper 2505.22954.",
    "output": "arXiv Query: search_query=&id_list=2505.22954&start=0&max_results=10"
  },
  {
    "input": "What is the GitHub repository for Distilling the Knowledge in a Neural Network?",
    "output": "The code for Distilling the Knowledge in a Neural Network is available at https://github.com/haitongli/knowledge-distillation-pytorch (haitongli/knowledge-distillation-pytorch)."
  },
  {
    "input": "Give me an overview of Whisper: Robust Speech Recognition via Large-Scale Weak Supervision.",
    "output": "Whisper: Robust Speech Recognition via Large-Scale Weak Supervision is a 2022 paper in Computation and Language (Natural Language Processing). The implementation is available at openai/whisper which robust speech recognition via large-scale weak supervision It is written in Python. The repository has 91,236 stars, indicating high community interest."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2405.04324&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2405.04324&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2406.12824&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2406.12824&start=0&max_results=10 paper was published in 2024."
  },
  {
    "input": "What is the GitHub repository for Attention Is All You Need?",
    "output": "The code for Attention Is All You Need is available at https://github.com/tensorflow/tensor2tensor (tensorflow/tensor2tensor)."
  },
  {
    "input": "What paper has ArXiv ID 2505.23006?",
    "output": "arXiv Query: search_query=&id_list=2505.23006&start=0&max_results=10"
  },
  {
    "input": "Summarize the Mistral 7B paper.",
    "output": "Mistral 7B is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at mistralai/mistral-inference which official inference library for mistral models It is written in Jupyter Notebook. The repository has 10,550 stars, indicating high community interest."
  },
  {
    "input": "What is the GitHub repository for Depthwise Separable Convolutions for Neural Machine Translation?",
    "output": "The code for Depthwise Separable Convolutions for Neural Machine Translation is available at https://github.com/google-research/google-research (google-research/google-research)."
  },
  {
    "input": "In what year did HuggingGPT: Solving AI Tasks with ChatGPT come out?",
    "output": "The HuggingGPT: Solving AI Tasks with ChatGPT paper was published in 2023."
  },
  {
    "input": "Explain the HazyResearch/safari implementation of arXiv Query: search_query=&id_list=2303.04129&start=0&max_results=10.",
    "output": "Convolutions for Sequence Modeling"
  },
  {
    "input": "Where can I find the code for Neural Message Passing for Quantum Chemistry?",
    "output": "The code for Neural Message Passing for Quantum Chemistry is available at https://github.com/priba/nmp_qc (priba/nmp_qc)."
  },
  {
    "input": "What year was LLaVA-1.5: Improved Baselines with Visual Instruction Tuning released?",
    "output": "The LLaVA-1.5: Improved Baselines with Visual Instruction Tuning paper was published in 2023."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2410.12896&start=0&max_results=10 repository have?",
    "output": "The ibm-granite/granite-3.0-language-models repository has 267 stars and 28 forks on GitHub."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2302.08242&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2302.08242&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at haoliuhl/language-quantized-autoencoders which language quantized autoencoders It is written in Python. The repository has 111 stars, indicating high community interest."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2505.23006&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2505.23006&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "What language is the oughtinc/ice repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2301.01947&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "When was the Synthetic Data Generation with Large Language Models paper published?",
    "output": "The Synthetic Data Generation with Large Language Models paper was published in 2023."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2505.22101&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2505.22101&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2302.09778&start=0&max_results=10 implementation?",
    "output": "The ali-vilab/composer repository has 1,560 stars and 49 forks on GitHub."
  },
  {
    "input": "Explain the lucidrains/flamingo-pytorch implementation of Flamingo: A Visual Language Model for Few-Shot Learning.",
    "output": "Implementation of \ud83e\udda9 Flamingo, state-of-the-art few-shot visual question answering attention net out of Deepmind, in Pytorch"
  },
  {
    "input": "In what year did Llama 2: Open Foundation and Fine-Tuned Chat Models come out?",
    "output": "The Llama 2: Open Foundation and Fine-Tuned Chat Models paper was published in 2023."
  },
  {
    "input": "What category is the The Flan Collection: Designing Data and Methods for Effective Instruction Tuning paper in?",
    "output": "The Flan Collection: Designing Data and Methods for Effective Instruction Tuning is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2303.02913&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2303.02913&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What is ReAct: Synergizing Reasoning and Acting in Language Models about?",
    "output": "ReAct: Synergizing Reasoning and Acting in Language Models is a 2022 paper in Computation and Language (Natural Language Processing). The implementation is available at ysymyth/ReAct which [iclr 2023] react: synergizing reasoning and acting in language models It is written in Jupyter Notebook. The repository has 3,225 stars, indicating high community interest."
  },
  {
    "input": "What programming language is Finetuned Language Models Are Zero-Shot Learners implemented in?",
    "output": "The Finetuned Language Models Are Zero-Shot Learners implementation is written in Python."
  },
  {
    "input": "What paper has ArXiv ID 2310.03744?",
    "output": "LLaVA-1.5: Improved Baselines with Visual Instruction Tuning"
  },
  {
    "input": "Give me an overview of Reformer: The Efficient Transformer.",
    "output": "Reformer: The Efficient Transformer is a 2020 paper in Machine Learning. The implementation is available at google/trax which trax \u2014 deep learning with clear code and speed It is written in Python. The repository has 8,293 stars, indicating high community interest."
  },
  {
    "input": "How many stars does the Enriching Word Vectors with Subword Information repository have?",
    "output": "The facebookresearch/fastText repository has 26,432 stars and 4,813 forks on GitHub."
  },
  {
    "input": "What are the key topics in the facebookresearch/CovidPrognosis repository?",
    "output": "The facebookresearch/CovidPrognosis repository covers: medical-imaging, deep-learning, radiography, x-ray, pytorch, covid-19, medical-image-analysis."
  },
  {
    "input": "Give me an overview of PyTorch: An Imperative Style, High-Performance Deep Learning Library.",
    "output": "PyTorch: An Imperative Style, High-Performance Deep Learning Library is a 2019 paper in Machine Learning. The implementation is available at pytorch/pytorch which tensors and dynamic neural networks in python with strong gpu acceleration It is written in Python. The repository has 95,348 stars, indicating high community interest."
  },
  {
    "input": "What paper has ArXiv ID 2410.12896?",
    "output": "arXiv Query: search_query=&id_list=2410.12896&start=0&max_results=10"
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2301.11305&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2301.11305&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at autonomousvision/stylegan-t which [icml'23] stylegan-t: unlocking the power of gans for fast large-scale text-to-image synthesis It is written in Python. The repository has 1,196 stars, indicating high community interest."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2206.01861?",
    "output": "LLM.int8(): 8-bit Matrix Multiplication for Transformers"
  },
  {
    "input": "How popular is the Universal Language Model Fine-tuning for Text Classification implementation?",
    "output": "The fastai/fastai repository has 27,611 stars and 7,663 forks on GitHub."
  },
  {
    "input": "What is the ArXiv category of P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning?",
    "output": "P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What programming language is Universal Language Model Fine-tuning for Text Classification implemented in?",
    "output": "The Universal Language Model Fine-tuning for Text Classification implementation is written in Jupyter Notebook."
  },
  {
    "input": "How many stars does the Efficient Estimation of Word Representations in Vector Space repository have?",
    "output": "The tmikolov/word2vec repository has 1,569 stars and 546 forks on GitHub."
  },
  {
    "input": "Tell me the title of ArXiv paper 1912.01703.",
    "output": "PyTorch: An Imperative Style, High-Performance Deep Learning Library"
  },
  {
    "input": "In what year did Wav2Vec 2.0: A Framework for Self-Supervised Learning of Speech come out?",
    "output": "The Wav2Vec 2.0: A Framework for Self-Supervised Learning of Speech paper was published in 2021."
  },
  {
    "input": "How many stars does the RoFormer: Enhanced Transformer with Rotary Position Embedding repository have?",
    "output": "The ZhuiyiTechnology/roformer repository has 1,051 stars and 59 forks on GitHub."
  },
  {
    "input": "Describe the openai/evals repository that implements GPT-4 Technical Report.",
    "output": "Evals is a framework for evaluating LLMs and LLM systems, and an open-source registry of benchmarks."
  },
  {
    "input": "What category is the Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena paper in?",
    "output": "Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What year was Segment Anything released?",
    "output": "The Segment Anything paper was published in 2023."
  },
  {
    "input": "Explain the LIMA: Less Is More for Alignment paper.",
    "output": "LIMA: Less Is More for Alignment is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at meta-llama/llama which inference code for llama models It is written in Python. The repository has 58,946 stars, indicating high community interest."
  },
  {
    "input": "What paper has ArXiv ID 2503.23513?",
    "output": "arXiv Query: search_query=&id_list=2503.23513&start=0&max_results=10"
  },
  {
    "input": "Summarize the SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities paper.",
    "output": "SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at 0nutation/SpeechGPT which speechgpt series: speech large language models It is written in Python. The repository has 1,396 stars, indicating high community interest."
  },
  {
    "input": "Give me an overview of GPT-4 Technical Report.",
    "output": "GPT-4 Technical Report is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at openai/evals which evals is a framework for evaluating llms and llm systems, and an open-source registry of benchmarks. It is written in Python. The repository has 17,349 stars, indicating high community interest."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2503.20201&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2503.20201&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at sentient-agi/OpenDeepSearch which sota search powered llm It is written in Python. The repository has 3,723 stars, indicating high community interest."
  },
  {
    "input": "Summarize the Llama 2: Open Foundation and Fine-Tuned Chat Models paper.",
    "output": "Llama 2: Open Foundation and Fine-Tuned Chat Models is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at meta-llama/llama which inference code for llama models It is written in Python. The repository has 58,946 stars, indicating high community interest."
  },
  {
    "input": "What does the danijar/dreamer repository do?",
    "output": "Dream to Control: Learning Behaviors by Latent Imagination"
  },
  {
    "input": "What's the implementation repository for Flamingo: A Visual Language Model for Few-Shot Learning?",
    "output": "The code for Flamingo: A Visual Language Model for Few-Shot Learning is available at https://github.com/lucidrains/flamingo-pytorch (lucidrains/flamingo-pytorch)."
  },
  {
    "input": "What programming language is FlashAttention-2: Faster Attention with Better Parallelism implemented in?",
    "output": "The FlashAttention-2: Faster Attention with Better Parallelism implementation is written in Python."
  },
  {
    "input": "Explain the salesforce/GeDi implementation of arXiv Query: search_query=&id_list=2009.06367&start=0&max_results=10.",
    "output": "GeDi: Generative Discriminator Guided Sequence Generation"
  },
  {
    "input": "In what year did Neural Machine Translation by Jointly Learning to Align and Translate come out?",
    "output": "The Neural Machine Translation by Jointly Learning to Align and Translate paper was published in 2015."
  },
  {
    "input": "Give me an overview of Feature Pyramid Networks for Object Detection.",
    "output": "Feature Pyramid Networks for Object Detection is a 2016 paper in Computer Vision and Pattern Recognition. The implementation is available at facebookresearch/Detectron which fair's research platform for object detection research, implementing popular algorithms like mask r-cnn and retinanet. It is written in Python. The repository has 26,396 stars, indicating high community interest."
  },
  {
    "input": "Which field does SimCSE: Simple Contrastive Learning of Sentence Embeddings belong to?",
    "output": "SimCSE: Simple Contrastive Learning of Sentence Embeddings is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What does the ysymyth/ReAct repository do?",
    "output": "[ICLR 2023] ReAct: Synergizing Reasoning and Acting in Language Models"
  },
  {
    "input": "Summarize the Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context paper.",
    "output": "Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context is a 2019 paper in Computation and Language (Natural Language Processing). The implementation is available at kimiyoung/transformer-xl It is written in Python. The repository has 3,679 stars, indicating high community interest."
  },
  {
    "input": "What does the facebookresearch/Detectron repository do?",
    "output": "FAIR's research platform for object detection research, implementing popular algorithms like Mask R-CNN and RetinaNet."
  },
  {
    "input": "How popular is the MRKL Systems: A modular approach to AGI implementation?",
    "output": "The langchain-ai/langchain repository has 120,451 stars and 19,851 forks on GitHub."
  },
  {
    "input": "How popular is the Linformer: Self-Attention with Linear Complexity implementation?",
    "output": "The tatp22/linformer-pytorch repository has 421 stars and 37 forks on GitHub."
  },
  {
    "input": "What is Graph Attention Networks about?",
    "output": "Graph Attention Networks is a 2017 paper in Machine Learning. The implementation is available at PetarV-/GAT which graph attention networks (https://arxiv.org/abs/1710.10903) It is written in Python. The repository has 3,460 stars, indicating high community interest."
  },
  {
    "input": "What year was FlashAttention: Fast and Memory-Efficient Exact Attention released?",
    "output": "The FlashAttention: Fast and Memory-Efficient Exact Attention paper was published in 2022."
  },
  {
    "input": "Tell me the title of ArXiv paper 2302.04870.",
    "output": "arXiv Query: search_query=&id_list=2302.04870&start=0&max_results=10"
  },
  {
    "input": "What language is the oughtinc/ice repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2301.01751&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What are the key topics in the FMInference/FlexLLMGen repository?",
    "output": "The FMInference/FlexLLMGen repository covers: deep-learning, gpt-3, high-throughput, large-language-models, machine-learning, offloading, opt."
  },
  {
    "input": "What topics does the LLM.int8(): 8-bit Matrix Multiplication for Transformers implementation cover?",
    "output": "The bitsandbytes-foundation/bitsandbytes repository covers: llm, machine-learning, pytorch, qlora, quantization."
  },
  {
    "input": "What are the key topics in the salesforce/LAVIS repository?",
    "output": "The salesforce/LAVIS repository covers: deep-learning, deep-learning-library, image-captioning, salesforce, vision-and-language, vision-framework, vision-language-pretraining, vision-language-transformer, visual-question-anwsering, multimodal-datasets, multimodal-deep-learning."
  },
  {
    "input": "What does the lisa-groundhog/GroundHog repository do?",
    "output": "Library for implementing RNNs with Theano"
  },
  {
    "input": "What year was StarCoder: May the source be with you! released?",
    "output": "The StarCoder: May the source be with you! paper was published in 2023."
  },
  {
    "input": "What is the ArXiv category of AWQ: Activation-aware Weight Quantization for LLM Compression?",
    "output": "AWQ: Activation-aware Weight Quantization for LLM Compression is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Explain the meta-llama/llama implementation of Llama 2: Open Foundation and Fine-Tuned Chat Models.",
    "output": "Inference code for Llama models"
  },
  {
    "input": "What does the haitongli/knowledge-distillation-pytorch repository do?",
    "output": "A PyTorch implementation for exploring deep and shallow knowledge distillation (KD) experiments with flexibility"
  },
  {
    "input": "What category is the Segment Anything in Medical Images paper in?",
    "output": "Segment Anything in Medical Images is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "Explain the LLaMA: Open and Efficient Foundation Language Models paper.",
    "output": "LLaMA: Open and Efficient Foundation Language Models is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at meta-llama/llama which inference code for llama models It is written in Python. The repository has 58,946 stars, indicating high community interest."
  },
  {
    "input": "What category is the Denoising Diffusion Probabilistic Models paper in?",
    "output": "Denoising Diffusion Probabilistic Models is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Give me an overview of ChemCrow: Augmenting large-language models with chemistry tools.",
    "output": "ChemCrow: Augmenting large-language models with chemistry tools is a 2023 paper in Artificial Intelligence. The implementation is available at ur-whitelab/chemcrow-public which chemcrow It is written in Python. The repository has 838 stars, indicating high community interest."
  },
  {
    "input": "How many stars does the Animate Anyone: Consistent and Controllable Image-to-Video Synthesis repository have?",
    "output": "The HumanAIGC/AnimateAnyone repository has 14,797 stars and 1,008 forks on GitHub."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2303.08518&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2303.08518&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at FMInference/FlexLLMGen which running large language models on a single gpu for throughput-oriented scenarios. It is written in Python. The repository has 9,379 stars, indicating high community interest."
  },
  {
    "input": "What language is the Shark-NLP/OpenICL repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2303.02913&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What year was TinyBERT: Distilling BERT for Natural Language Understanding released?",
    "output": "The TinyBERT: Distilling BERT for Natural Language Understanding paper was published in 2019."
  },
  {
    "input": "Which field does GPT-3: Language Models are Few-Shot Learners belong to?",
    "output": "GPT-3: Language Models are Few-Shot Learners is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2312.02120&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2312.02120&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What is the ArXiv category of Mamba: Linear-Time Sequence Modeling with Selective State Spaces?",
    "output": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is the ArXiv category of LLaMA-Adapter: Efficient Fine-tuning of Language Models?",
    "output": "LLaMA-Adapter: Efficient Fine-tuning of Language Models is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2006.16779&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2006.16779&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What year was ALBERT: A Lite BERT released?",
    "output": "The ALBERT: A Lite BERT paper was published in 2019."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2303.03378&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2303.03378&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What language is the google-research/google-research repository written in?",
    "output": "The Med-PaLM 2: Towards Expert-Level Medical Question Answering implementation is written in Jupyter Notebook."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2303.08518&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2303.08518&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2504.20734&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2504.20734&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2006.16779&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2006.16779&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Tell me the title of ArXiv paper 2310.11511.",
    "output": "Zephyr: Direct Distillation of LM Alignment"
  },
  {
    "input": "What language is the autonomousvision/stylegan-t repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2301.10343&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What is High-Resolution Image Synthesis with Latent Diffusion Models about?",
    "output": "High-Resolution Image Synthesis with Latent Diffusion Models is a 2021 paper in Computer Vision and Pattern Recognition. The implementation is available at CompVis/latent-diffusion which high-resolution image synthesis with latent diffusion models It is written in Jupyter Notebook. The repository has 13,589 stars, indicating high community interest."
  },
  {
    "input": "What programming language is SimCSE: Simple Contrastive Learning of Sentence Embeddings implemented in?",
    "output": "The SimCSE: Simple Contrastive Learning of Sentence Embeddings implementation is written in Python."
  },
  {
    "input": "What year was BERT: Pre-training of Deep Bidirectional Transformers released?",
    "output": "The BERT: Pre-training of Deep Bidirectional Transformers paper was published in 2018."
  },
  {
    "input": "What is the ArXiv category of Mistral 7B?",
    "output": "Mistral 7B is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "When was the Universal Language Model Fine-tuning for Text Classification paper published?",
    "output": "The Universal Language Model Fine-tuning for Text Classification paper was published in 2018."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2007.16122&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2007.16122&start=0&max_results=10 paper was published in 2020."
  },
  {
    "input": "What language is the amazon-science/mm-cot repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2302.00923&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What is the ArXiv category of Contriever: Unsupervised Dense Information Retrieval?",
    "output": "Contriever: Unsupervised Dense Information Retrieval is in the Information Retrieval (cs.IR) category."
  },
  {
    "input": "What does the lucidrains/PaLM-pytorch repository do?",
    "output": "Implementation of the specific Transformer architecture from PaLM - Scaling Language Modeling with Pathways"
  },
  {
    "input": "What is the ArXiv category of Sequence to Sequence Learning with Neural Networks?",
    "output": "Sequence to Sequence Learning with Neural Networks is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2006.16779&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2006.16779&start=0&max_results=10 is a 2020 paper in Machine Learning. The implementation is available at google-research/pegasus It is written in Python. The repository has 1,644 stars, indicating high community interest."
  },
  {
    "input": "Explain the plurai-ai/intellagent implementation of arXiv Query: search_query=&id_list=2501.13824&start=0&max_results=10.",
    "output": "A framework for comprehensive diagnosis and optimization of agents using simulated, realistic synthetic interactions"
  },
  {
    "input": "What programming language is Zephyr: Direct Distillation of LM Alignment implemented in?",
    "output": "The Zephyr: Direct Distillation of LM Alignment implementation is written in Python."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2302.00923&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2302.00923&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What programming language is AudioPaLM: A Large Language Model for Speech Understanding implemented in?",
    "output": "The AudioPaLM: A Large Language Model for Speech Understanding implementation is written in Python."
  },
  {
    "input": "In what year did EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification come out?",
    "output": "The EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification paper was published in 2019."
  },
  {
    "input": "Summarize the StarCoder: May the source be with you! paper.",
    "output": "StarCoder: May the source be with you! is a 2023 paper in Machine Learning. The implementation is available at bigcode-project/starcoder which home of starcoder: fine-tuning & inference! It is written in Python. The repository has 7,473 stars, indicating high community interest."
  },
  {
    "input": "What are the key topics in the haoliuhl/language-quantized-autoencoders repository?",
    "output": "The haoliuhl/language-quantized-autoencoders repository covers: autoencoders, bert, large-language-models, multimodal, roberta, vq, vqvae."
  },
  {
    "input": "What language is the facebookresearch/faiss repository written in?",
    "output": "The Billion-scale similarity search with GPUs implementation is written in C++."
  },
  {
    "input": "Explain the rajpurkar/SQuAD-explorer implementation of Know What You Don't Know: Unanswerable Questions for SQuAD.",
    "output": "Visually Explore the Stanford Question Answering Dataset"
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2101.04909&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2101.04909&start=0&max_results=10 is a 2021 paper in Machine Learning. The implementation is available at facebookresearch/CovidPrognosis which covid deterioration prediction based on chest x-ray radiographs via moco-trained image representations It is written in Python. The repository has 158 stars, indicating high community interest."
  },
  {
    "input": "What paper has ArXiv ID 2102.01192?",
    "output": "arXiv Query: search_query=&id_list=2102.01192&start=0&max_results=10"
  },
  {
    "input": "What category is the Flamingo: A Visual Language Model for Few-Shot Learning paper in?",
    "output": "Flamingo: A Visual Language Model for Few-Shot Learning is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "Tell me the title of ArXiv paper 2310.03744.",
    "output": "LLaVA-1.5: Improved Baselines with Visual Instruction Tuning"
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2101.04909?",
    "output": "arXiv Query: search_query=&id_list=2101.04909&start=0&max_results=10"
  },
  {
    "input": "What programming language is Diffusion Models Beat GANs on Image Synthesis implemented in?",
    "output": "The Diffusion Models Beat GANs on Image Synthesis implementation is written in Python."
  },
  {
    "input": "When was the BLIP: Bootstrapping Language-Image Pre-training paper published?",
    "output": "The BLIP: Bootstrapping Language-Image Pre-training paper was published in 2022."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2301.00774&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2301.00774&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at facebookresearch/ConvNeXt-V2 which code release for convnext v2 model It is written in Python. The repository has 1,890 stars, indicating high community interest."
  },
  {
    "input": "What year was Swin Transformer: Hierarchical Vision Transformer using Shifted Windows released?",
    "output": "The Swin Transformer: Hierarchical Vision Transformer using Shifted Windows paper was published in 2021."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2303.09431&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2303.09431&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "Where can I find the code for RoBERTa: A Robustly Optimized BERT Pretraining Approach?",
    "output": "The code for RoBERTa: A Robustly Optimized BERT Pretraining Approach is available at https://github.com/facebookresearch/fairseq (facebookresearch/fairseq)."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2406.09403&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2406.09403&start=0&max_results=10 is available at https://github.com/lamini-ai/Lamini-Memory-Tuning (lamini-ai/Lamini-Memory-Tuning)."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2301.02828&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2301.02828&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2302.03011&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2302.03011&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "In what year did Masked Autoencoders Are Scalable Vision Learners come out?",
    "output": "The Masked Autoencoders Are Scalable Vision Learners paper was published in 2022."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2404.03592&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2404.03592&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2505.23006&start=0&max_results=10 implementation?",
    "output": "The ruixin31/Spurious_Rewards repository has 342 stars and 20 forks on GitHub."
  },
  {
    "input": "What is the ArXiv category of FlashAttention-2: Faster Attention with Better Parallelism?",
    "output": "FlashAttention-2: Faster Attention with Better Parallelism is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 1512.03385?",
    "output": "Deep Residual Learning for Image Recognition"
  },
  {
    "input": "What is the ArXiv category of TinyBERT: Distilling BERT for Natural Language Understanding?",
    "output": "TinyBERT: Distilling BERT for Natural Language Understanding is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "How many stars does the Deep Graph Library: A Graph-Centric, Highly-Performant Package repository have?",
    "output": "The dmlc/dgl repository has 14,149 stars and 3,057 forks on GitHub."
  },
  {
    "input": "Tell me the title of ArXiv paper 2110.07602.",
    "output": "P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning"
  },
  {
    "input": "What category is the LIMA: Less Is More for Alignment paper in?",
    "output": "LIMA: Less Is More for Alignment is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What paper has ArXiv ID 2105.05233?",
    "output": "Diffusion Models Beat GANs on Image Synthesis"
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2301.01751&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2301.01751&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at oughtinc/ice which interactive composition explorer: a debugger for compositional language model programs It is written in Python. The repository has 563 stars, indicating high community interest."
  },
  {
    "input": "What are the key topics in the plurai-ai/intellagent repository?",
    "output": "The plurai-ai/intellagent repository covers: agent, evaluation, llmops, simulator, synthetic-data."
  },
  {
    "input": "What is the ArXiv category of Code Llama: Open Foundation Models for Code?",
    "output": "Code Llama: Open Foundation Models for Code is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What paper has ArXiv ID 2405.04324?",
    "output": "arXiv Query: search_query=&id_list=2405.04324&start=0&max_results=10"
  },
  {
    "input": "Where can I find the code for AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning?",
    "output": "The code for AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning is available at https://github.com/QingruZhang/AdaLoRA (QingruZhang/AdaLoRA)."
  },
  {
    "input": "How many stars does the CLIP: Learning Transferable Visual Models From Natural Language repository have?",
    "output": "The openai/CLIP repository has 31,706 stars and 3,848 forks on GitHub."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2303.03378&start=0&max_results=10 implementation?",
    "output": "The NVlabs/prismer repository has 1,307 stars and 74 forks on GitHub."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2302.04870&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2302.04870&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2503.22708&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2503.22708&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What paper has ArXiv ID 1910.13461?",
    "output": "ALBERT: A Lite BERT"
  },
  {
    "input": "Which field does AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning belong to?",
    "output": "AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "How popular is the Decoupled Weight Decay Regularization implementation?",
    "output": "The pytorch/pytorch repository has 95,348 stars and 26,003 forks on GitHub."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2504.00698&start=0&max_results=10 repository have?",
    "output": "The allenai/codescientist repository has 301 stars and 37 forks on GitHub."
  },
  {
    "input": "Summarize the Swin Transformer: Hierarchical Vision Transformer using Shifted Windows paper.",
    "output": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows is a 2021 paper in Computer Vision and Pattern Recognition. The implementation is available at microsoft/Swin-Transformer which this is an official implementation for \"swin transformer: hierarchical vision transformer using shifted windows\". It is written in Python. The repository has 15,466 stars, indicating high community interest."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2301.09515&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2301.09515&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Explain the High-Resolution Image Synthesis with Latent Diffusion Models paper.",
    "output": "High-Resolution Image Synthesis with Latent Diffusion Models is a 2021 paper in Computer Vision and Pattern Recognition. The implementation is available at CompVis/latent-diffusion which high-resolution image synthesis with latent diffusion models It is written in Jupyter Notebook. The repository has 13,589 stars, indicating high community interest."
  },
  {
    "input": "In what year did TensorFlow: A System for Large-Scale Machine Learning come out?",
    "output": "The TensorFlow: A System for Large-Scale Machine Learning paper was published in 2016."
  },
  {
    "input": "What is the ArXiv category of The Flan Collection: Designing Data and Methods for Effective Instruction Tuning?",
    "output": "The Flan Collection: Designing Data and Methods for Effective Instruction Tuning is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What paper has ArXiv ID 1706.02216?",
    "output": "Inductive Representation Learning on Large Graphs"
  },
  {
    "input": "What category is the Decoupled Weight Decay Regularization paper in?",
    "output": "Decoupled Weight Decay Regularization is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2107.03374?",
    "output": "Codex: Evaluating Large Language Models Trained on Code"
  },
  {
    "input": "What is the GitHub repository for Finetuned Language Models Are Zero-Shot Learners?",
    "output": "The code for Finetuned Language Models Are Zero-Shot Learners is available at https://github.com/google-research/FLAN (google-research/FLAN)."
  },
  {
    "input": "How many stars does the Decoupled Weight Decay Regularization repository have?",
    "output": "The pytorch/pytorch repository has 95,348 stars and 26,003 forks on GitHub."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2406.09308&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2406.09308&start=0&max_results=10 paper was published in 2024."
  },
  {
    "input": "Describe the NVlabs/prismer repository that implements arXiv Query: search_query=&id_list=2303.02506&start=0&max_results=10.",
    "output": "The implementation of \"Prismer: A Vision-Language Model with Multi-Task Experts\"."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2302.14838&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2302.14838&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2301.00704&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2301.00704&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What paper has ArXiv ID 2106.07447?",
    "output": "HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction"
  },
  {
    "input": "Describe the facebookresearch/DPR repository that implements Dense Passage Retrieval for Open-Domain Question Answering.",
    "output": "Dense Passage Retriever - is a set of tools and models for open domain Q&A task."
  },
  {
    "input": "Describe the mit-han-lab/offsite-tuning repository that implements arXiv Query: search_query=&id_list=2302.03917&start=0&max_results=10.",
    "output": "Offsite-Tuning: Transfer Learning without Full Model"
  },
  {
    "input": "What does the meta-llama/llama repository do?",
    "output": "Inference code for Llama models"
  },
  {
    "input": "Tell me the title of ArXiv paper 2302.09778.",
    "output": "arXiv Query: search_query=&id_list=2302.09778&start=0&max_results=10"
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2302.10866&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2302.10866&start=0&max_results=10 is available at https://github.com/HazyResearch/safari (HazyResearch/safari)."
  },
  {
    "input": "Where can I find the code for PyTorch Geometric: Library for Geometric Deep Learning?",
    "output": "The code for PyTorch Geometric: Library for Geometric Deep Learning is available at https://github.com/pyg-team/pytorch_geometric (pyg-team/pytorch_geometric)."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2303.04226&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2303.04226&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Where can I find the code for InstructBLIP: Towards General-purpose Vision-Language Models?",
    "output": "The code for InstructBLIP: Towards General-purpose Vision-Language Models is available at https://github.com/salesforce/LAVIS (salesforce/LAVIS)."
  },
  {
    "input": "What language is the jax-ml/jax repository written in?",
    "output": "The JAX: Composable transformations of Python+NumPy programs implementation is written in Python."
  },
  {
    "input": "What language is the tkipf/gcn repository written in?",
    "output": "The Semi-Supervised Classification with Graph Convolutional Networks implementation is written in Python."
  },
  {
    "input": "What language is the google-research/prompt-tuning repository written in?",
    "output": "The The Power of Scale for Parameter-Efficient Prompt Tuning implementation is written in Python."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2302.09778&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2302.09778&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2110.07602?",
    "output": "P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning"
  },
  {
    "input": "What language is the ShishirPatil/gorilla repository written in?",
    "output": "The Gorilla: Large Language Model Connected with Massive APIs implementation is written in Python."
  },
  {
    "input": "What paper has ArXiv ID 2004.04906?",
    "output": "Dense Passage Retrieval for Open-Domain Question Answering"
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2501.11067&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2501.11067&start=0&max_results=10 is available at https://github.com/plurai-ai/intellagent (plurai-ai/intellagent)."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2106.09685?",
    "output": "LoRA: Low-Rank Adaptation of Large Language Models"
  },
  {
    "input": "What year was FlashAttention-2: Faster Attention with Better Parallelism released?",
    "output": "The FlashAttention-2: Faster Attention with Better Parallelism paper was published in 2023."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2405.02803&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2405.02803&start=0&max_results=10 is a 2024 paper in Machine Learning. The implementation is available at ibm-granite/granite-code-models which granite code models: a family of open foundation models for code intelligence The repository has 1,240 stars, indicating high community interest."
  },
  {
    "input": "What are the key topics in the plurai-ai/intellagent repository?",
    "output": "The plurai-ai/intellagent repository covers: agent, evaluation, llmops, simulator, synthetic-data."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2503.00735&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2503.00735&start=0&max_results=10 is available at https://github.com/LTH14/fractalgen (LTH14/fractalgen)."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2302.14838&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2302.14838&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at ruiqi-zhong/D5 which the github repo for goal driven discovery of distributional differences via language descriptions It is written in Python. The repository has 71 stars, indicating high community interest."
  },
  {
    "input": "Explain the PaLM: Scaling Language Modeling with Pathways paper.",
    "output": "PaLM: Scaling Language Modeling with Pathways is a 2022 paper in Computation and Language (Natural Language Processing). The implementation is available at lucidrains/PaLM-pytorch which implementation of the specific transformer architecture from palm - scaling language modeling with pathways It is written in Python. The repository has 826 stars, indicating high community interest."
  },
  {
    "input": "When was the LoRA: Low-Rank Adaptation of Large Language Models paper published?",
    "output": "The LoRA: Low-Rank Adaptation of Large Language Models paper was published in 2021."
  },
  {
    "input": "Tell me the title of ArXiv paper 1711.05101.",
    "output": "Decoupled Weight Decay Regularization"
  },
  {
    "input": "Summarize the QLoRA: Efficient Finetuning of Quantized LLMs paper.",
    "output": "QLoRA: Efficient Finetuning of Quantized LLMs is a 2023 paper in Machine Learning. The implementation is available at artidoro/qlora which qlora: efficient finetuning of quantized llms It is written in Jupyter Notebook. The repository has 10,764 stars, indicating high community interest."
  },
  {
    "input": "What language is the OpenBMB/Eurus repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2403.17887&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Describe the HazyResearch/safari repository that implements arXiv Query: search_query=&id_list=2302.10866&start=0&max_results=10.",
    "output": "Convolutions for Sequence Modeling"
  },
  {
    "input": "Where can I find the code for LoRA: Low-Rank Adaptation of Large Language Models?",
    "output": "The code for LoRA: Low-Rank Adaptation of Large Language Models is available at https://github.com/microsoft/LoRA (microsoft/LoRA)."
  },
  {
    "input": "How popular is the WizardCoder: Empowering Code LLMs with Evol-Instruct implementation?",
    "output": "The nlpxucan/WizardLM repository has 9,460 stars and 761 forks on GitHub."
  },
  {
    "input": "Tell me the title of ArXiv paper 2010.11929.",
    "output": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"
  },
  {
    "input": "When was the HuggingGPT: Solving AI Tasks with ChatGPT paper published?",
    "output": "The HuggingGPT: Solving AI Tasks with ChatGPT paper was published in 2023."
  },
  {
    "input": "What category is the FlashAttention: Fast and Memory-Efficient Exact Attention paper in?",
    "output": "FlashAttention: Fast and Memory-Efficient Exact Attention is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2502.12115&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2502.12115&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "What language is the facebookresearch/ConvNeXt-V2 repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2301.00303&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2303.09431&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2303.09431&start=0&max_results=10 is available at https://github.com/FMInference/FlexLLMGen (FMInference/FlexLLMGen)."
  },
  {
    "input": "What is the ArXiv category of XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale?",
    "output": "XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Tell me the title of ArXiv paper 1512.03385.",
    "output": "Deep Residual Learning for Image Recognition"
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2504.00698&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2504.00698&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "What year was The Flan Collection: Designing Data and Methods for Effective Instruction Tuning released?",
    "output": "The The Flan Collection: Designing Data and Methods for Effective Instruction Tuning paper was published in 2023."
  },
  {
    "input": "Summarize the Wide & Deep Learning for Recommender Systems paper.",
    "output": "Wide & Deep Learning for Recommender Systems is a 2016 paper in Information Retrieval. The implementation is available at tensorflow/models which models and examples built with tensorflow It is written in Python. The repository has 77,677 stars, indicating high community interest."
  },
  {
    "input": "What does the tensorflow/tensor2tensor repository do?",
    "output": "Library of deep learning models and datasets designed to make deep learning more accessible and accelerate ML research."
  },
  {
    "input": "What is the GitHub repository for Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning?",
    "output": "The code for Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning is available at https://github.com/rail-berkeley/softlearning (rail-berkeley/softlearning)."
  },
  {
    "input": "What's the implementation repository for Longformer: The Long-Document Transformer?",
    "output": "The code for Longformer: The Long-Document Transformer is available at https://github.com/allenai/longformer (allenai/longformer)."
  },
  {
    "input": "What's the implementation repository for LoRA: Low-Rank Adaptation of Large Language Models?",
    "output": "The code for LoRA: Low-Rank Adaptation of Large Language Models is available at https://github.com/microsoft/LoRA (microsoft/LoRA)."
  },
  {
    "input": "How popular is the PyTorch: An Imperative Style, High-Performance Deep Learning Library implementation?",
    "output": "The pytorch/pytorch repository has 95,348 stars and 26,003 forks on GitHub."
  },
  {
    "input": "Give me an overview of Code Llama: Open Foundation Models for Code.",
    "output": "Code Llama: Open Foundation Models for Code is a 2023 paper in Machine Learning. The implementation is available at meta-llama/codellama which inference code for codellama models It is written in Python. The repository has 16,365 stars, indicating high community interest."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2502.17437&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2502.17437&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2502.09992&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2502.09992&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What does the horovod/horovod repository do?",
    "output": "Distributed training framework for TensorFlow, Keras, PyTorch, and Apache MXNet."
  },
  {
    "input": "What category is the RetNet: Retentive Network: A Successor to Transformer paper in?",
    "output": "RetNet: Retentive Network: A Successor to Transformer is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is the ArXiv category of Graph Attention Networks?",
    "output": "Graph Attention Networks is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What's the implementation repository for Contriever: Unsupervised Dense Information Retrieval?",
    "output": "The code for Contriever: Unsupervised Dense Information Retrieval is available at https://github.com/facebookresearch/contriever (facebookresearch/contriever)."
  },
  {
    "input": "What are the key topics in the google-research/electra repository?",
    "output": "The google-research/electra repository covers: nlp, deep-learning, tensorflow."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2403.17887&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2403.17887&start=0&max_results=10 paper was published in 2024."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2301.04856&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2301.04856&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at frankxu2004/knnlm-why which repo for icml23 \"why do nearest neighbor language models work?\" It is written in Python. The repository has 59 stars, indicating high community interest."
  },
  {
    "input": "What's the implementation repository for PaLM: Scaling Language Modeling with Pathways?",
    "output": "The code for PaLM: Scaling Language Modeling with Pathways is available at https://github.com/lucidrains/PaLM-pytorch (lucidrains/PaLM-pytorch)."
  },
  {
    "input": "What is the ArXiv category of CLIP: Learning Transferable Visual Models From Natural Language?",
    "output": "CLIP: Learning Transferable Visual Models From Natural Language is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2312.02783&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2312.02783&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Describe the MoonshotAI/Kimi-Audio repository that implements arXiv Query: search_query=&id_list=2504.20734&start=0&max_results=10.",
    "output": "Kimi-Audio, an open-source audio foundation model excelling in audio understanding, generation, and conversation"
  },
  {
    "input": "Which field does LLaMA-Adapter: Efficient Fine-tuning of Language Models belong to?",
    "output": "LLaMA-Adapter: Efficient Fine-tuning of Language Models is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What programming language is Neural Machine Translation by Jointly Learning to Align and Translate implemented in?",
    "output": "The Neural Machine Translation by Jointly Learning to Align and Translate implementation is written in Python."
  },
  {
    "input": "What language is the MAXNORM8650/MedAgentSim repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2503.23513&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2007.16122&start=0&max_results=10 repository have?",
    "output": "The linkedin/gdmix repository has 133 stars and 18 forks on GitHub."
  },
  {
    "input": "What paper has ArXiv ID 2305.15334?",
    "output": "Gorilla: Large Language Model Connected with Massive APIs"
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2107.07346&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2107.07346&start=0&max_results=10 is a 2021 paper in Machine Learning. The implementation is available at jacopotagliabue/you-dont-need-a-bigger-boat which an end-to-end implementation of intent prediction with metaflow and other cool tools It is written in Python. The repository has 873 stars, indicating high community interest."
  },
  {
    "input": "What topics does the DreamFusion: Text-to-3D using 2D Diffusion implementation cover?",
    "output": "The ashawkey/stable-dreamfusion repository covers: text-to-3d, gui, nerf, stable-diffusion, dreamfusion, image-to-3d."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2504.16084&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2504.16084&start=0&max_results=10 is available at https://github.com/Crista23/goal_directedness_llms (Crista23/goal_directedness_llms)."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2405.03548&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2405.03548&start=0&max_results=10 is a 2024 paper in Machine Learning. The implementation is available at ibm-granite/granite-code-models which granite code models: a family of open foundation models for code intelligence The repository has 1,240 stars, indicating high community interest."
  },
  {
    "input": "What category is the HuggingGPT: Solving AI Tasks with ChatGPT paper in?",
    "output": "HuggingGPT: Solving AI Tasks with ChatGPT is in the Artificial Intelligence (cs.AI) category."
  },
  {
    "input": "What does the microsoft/JARVIS repository do?",
    "output": "JARVIS, a system to connect LLMs with ML community. Paper: https://arxiv.org/pdf/2303.17580.pdf"
  },
  {
    "input": "Summarize the Neural Machine Translation by Jointly Learning to Align and Translate paper.",
    "output": "Neural Machine Translation by Jointly Learning to Align and Translate is a 2015 paper in Computation and Language (Natural Language Processing). The implementation is available at lisa-groundhog/GroundHog which library for implementing rnns with theano It is written in Python. The repository has 613 stars, indicating high community interest."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2304.08485?",
    "output": "MiniGPT-4: Enhancing Vision-Language Understanding"
  },
  {
    "input": "What programming language is BERT Rediscovers the Classical NLP Pipeline implemented in?",
    "output": "The BERT Rediscovers the Classical NLP Pipeline implementation is written in Jupyter Notebook."
  },
  {
    "input": "Explain the JAX: Composable transformations of Python+NumPy programs paper.",
    "output": "JAX: Composable transformations of Python+NumPy programs is a 2019 paper in cs.MS. The implementation is available at jax-ml/jax which composable transformations of python+numpy programs: differentiate, vectorize, jit to gpu/tpu, and more It is written in Python. The repository has 34,091 stars, indicating high community interest."
  },
  {
    "input": "What category is the QLoRA: Efficient Finetuning of Quantized LLMs paper in?",
    "output": "QLoRA: Efficient Finetuning of Quantized LLMs is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Explain the CLIP: Learning Transferable Visual Models From Natural Language paper.",
    "output": "CLIP: Learning Transferable Visual Models From Natural Language is a 2021 paper in Computer Vision and Pattern Recognition. The implementation is available at openai/CLIP which clip (contrastive language-image pretraining),  predict the most relevant text snippet given an image It is written in Jupyter Notebook. The repository has 31,706 stars, indicating high community interest."
  },
  {
    "input": "Tell me the title of ArXiv paper 2301.04104.",
    "output": "arXiv Query: search_query=&id_list=2301.04104&start=0&max_results=10"
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2301.04856&start=0&max_results=10 repository have?",
    "output": "The frankxu2004/knnlm-why repository has 59 stars and 3 forks on GitHub."
  },
  {
    "input": "What are the key topics in the oughtinc/ice repository?",
    "output": "The oughtinc/ice repository covers: debugging, gpt-3, python, language-model."
  },
  {
    "input": "What paper has ArXiv ID 2304.15004?",
    "output": "Vicuna: An Open-Source Chatbot"
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2503.22678&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2503.22678&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is the ArXiv category of Feature Pyramid Networks for Object Detection?",
    "output": "Feature Pyramid Networks for Object Detection is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "Explain the XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale paper.",
    "output": "XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale is a 2021 paper in Computation and Language (Natural Language Processing). The implementation is available at facebookresearch/fairseq which facebook ai research sequence-to-sequence toolkit written in python. It is written in Python. The repository has 31,985 stars, indicating high community interest."
  },
  {
    "input": "What paper has ArXiv ID 2210.17323?",
    "output": "GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers"
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2505.22954?",
    "output": "arXiv Query: search_query=&id_list=2505.22954&start=0&max_results=10"
  },
  {
    "input": "What topics does the arXiv Query: search_query=&id_list=2301.00704&start=0&max_results=10 implementation cover?",
    "output": "The lucidrains/muse-maskgit-pytorch repository covers: artificial-intelligence, deep-learning, text-to-image, attention-mechanisms, transformers."
  },
  {
    "input": "How many stars does the InPars: Data Augmentation for Information Retrieval repository have?",
    "output": "The zetaalphavector/InPars repository has 198 stars and 24 forks on GitHub."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2212.10375?",
    "output": "InPars: Data Augmentation for Information Retrieval"
  },
  {
    "input": "What is the ArXiv category of HuggingGPT: Solving AI Tasks with ChatGPT?",
    "output": "HuggingGPT: Solving AI Tasks with ChatGPT is in the Artificial Intelligence (cs.AI) category."
  },
  {
    "input": "What are the key topics in the horovod/horovod repository?",
    "output": "The horovod/horovod repository covers: tensorflow, uber, machine-learning, machinelearning, mpi, baidu, deep-learning, deeplearning, keras, pytorch, mxnet, spark, ray."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2503.01141&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2503.01141&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is the GitHub repository for Deep Graph Library: A Graph-Centric, Highly-Performant Package?",
    "output": "The code for Deep Graph Library: A Graph-Centric, Highly-Performant Package is available at https://github.com/dmlc/dgl (dmlc/dgl)."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2503.01141&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2503.01141&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2303.04129&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2303.04129&start=0&max_results=10 is available at https://github.com/HazyResearch/safari (HazyResearch/safari)."
  },
  {
    "input": "Explain the HazyResearch/safari implementation of arXiv Query: search_query=&id_list=2303.04226&start=0&max_results=10.",
    "output": "Convolutions for Sequence Modeling"
  },
  {
    "input": "How many stars does the High-Resolution Image Synthesis with Latent Diffusion Models repository have?",
    "output": "The CompVis/latent-diffusion repository has 13,589 stars and 1,697 forks on GitHub."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2006.03511&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2006.03511&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Describe the meta-llama/codellama repository that implements Code Llama: Open Foundation Models for Code.",
    "output": "Inference code for CodeLlama models"
  },
  {
    "input": "What does the Open-Reasoner-Zero/Open-Reasoner-Zero repository do?",
    "output": "Official Repo for Open-Reasoner-Zero"
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2301.01181&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2301.01181&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at JohnNay/llm-lobbyist which code for the paper: \"large language models as corporate lobbyists\" (2023). It is written in Jupyter Notebook. The repository has 172 stars, indicating high community interest."
  },
  {
    "input": "Explain the fastai/fastai implementation of Universal Language Model Fine-tuning for Text Classification.",
    "output": "The fastai deep learning library"
  },
  {
    "input": "What language is the PetarV-/GAT repository written in?",
    "output": "The Graph Attention Networks implementation is written in Python."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2406.10209&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2406.10209&start=0&max_results=10 paper was published in 2024."
  },
  {
    "input": "What programming language is P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning implemented in?",
    "output": "The P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning implementation is written in Python."
  },
  {
    "input": "Describe the facebookresearch/fastText repository that implements Enriching Word Vectors with Subword Information.",
    "output": "Library for fast text representation and classification."
  },
  {
    "input": "Explain the microsoft/DeBERTa implementation of DeBERTa: Decoding-enhanced BERT with Disentangled Attention.",
    "output": "The implementation of DeBERTa"
  },
  {
    "input": "Describe the google-research/prompt-tuning repository that implements The Power of Scale for Parameter-Efficient Prompt Tuning.",
    "output": "Original Implementation of Prompt Tuning from Lester, et al, 2021"
  },
  {
    "input": "What does the lm-sys/FastChat repository do?",
    "output": "An open platform for training, serving, and evaluating large language models. Release repo for Vicuna and Chatbot Arena."
  },
  {
    "input": "What does the microsoft/LoRA repository do?",
    "output": "Code for loralib, an implementation of \"LoRA: Low-Rank Adaptation of Large Language Models\""
  },
  {
    "input": "Describe the amazon-science/mm-cot repository that implements arXiv Query: search_query=&id_list=2301.13688&start=0&max_results=10.",
    "output": "Official implementation for \"Multimodal Chain-of-Thought Reasoning in Language Models\" (stay tuned and more will be updated)"
  },
  {
    "input": "What language is the THUDM/P-tuning-v2 repository written in?",
    "output": "The P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning implementation is written in Python."
  },
  {
    "input": "Which field does RoBERTa: A Robustly Optimized BERT Pretraining Approach belong to?",
    "output": "RoBERTa: A Robustly Optimized BERT Pretraining Approach is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What programming language is MMLU: Measuring Massive Multitask Language Understanding implemented in?",
    "output": "The MMLU: Measuring Massive Multitask Language Understanding implementation is written in Python."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2303.01469&start=0&max_results=10 implementation?",
    "output": "The ruiqi-zhong/D5 repository has 71 stars and 13 forks on GitHub."
  },
  {
    "input": "What year was SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding released?",
    "output": "The SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding paper was published in 2019."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2303.08518&start=0&max_results=10 implementation?",
    "output": "The FMInference/FlexLLMGen repository has 9,379 stars and 584 forks on GitHub."
  },
  {
    "input": "What language is the ruiqi-zhong/D5 repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2303.01469&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What paper has ArXiv ID 2104.09864?",
    "output": "RoFormer: Enhanced Transformer with Rotary Position Embedding"
  },
  {
    "input": "What category is the BERT Rediscovers the Classical NLP Pipeline paper in?",
    "output": "BERT Rediscovers the Classical NLP Pipeline is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Explain the Prefix-Tuning: Optimizing Continuous Prompts for Generation paper.",
    "output": "Prefix-Tuning: Optimizing Continuous Prompts for Generation is a 2021 paper in Computation and Language (Natural Language Processing). The implementation is available at XiangLi1999/PrefixTuning which prefix-tuning: optimizing continuous prompts for generation It is written in Python. The repository has 954 stars, indicating high community interest."
  },
  {
    "input": "What is the ArXiv category of AlpacaEval: An Automatic Evaluator for Instruction-following?",
    "output": "AlpacaEval: An Automatic Evaluator for Instruction-following is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "When was the Neural Machine Translation by Jointly Learning to Align and Translate paper published?",
    "output": "The Neural Machine Translation by Jointly Learning to Align and Translate paper was published in 2015."
  },
  {
    "input": "How popular is the InstructBLIP: Towards General-purpose Vision-Language Models implementation?",
    "output": "The salesforce/LAVIS repository has 11,040 stars and 1,086 forks on GitHub."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2504.11844&start=0&max_results=10 implementation?",
    "output": "The Crista23/goal_directedness_llms repository has 12 stars and 2 forks on GitHub."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2406.07394&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2406.07394&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What language is the ruixin31/Spurious_Rewards repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2505.23006&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Where can I find the code for Playing Atari with Deep Reinforcement Learning?",
    "output": "The code for Playing Atari with Deep Reinforcement Learning is available at https://github.com/google-deepmind/dqn (google-deepmind/dqn)."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2101.04909&start=0&max_results=10 repository have?",
    "output": "The facebookresearch/CovidPrognosis repository has 158 stars and 38 forks on GitHub."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2301.01947&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2301.01947&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What category is the PyTorch: An Imperative Style, High-Performance Deep Learning Library paper in?",
    "output": "PyTorch: An Imperative Style, High-Performance Deep Learning Library is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Explain the Adam: A Method for Stochastic Optimization paper.",
    "output": "Adam: A Method for Stochastic Optimization is a 2014 paper in Machine Learning. The implementation is available at pytorch/pytorch which tensors and dynamic neural networks in python with strong gpu acceleration It is written in Python. The repository has 95,348 stars, indicating high community interest."
  },
  {
    "input": "How many stars does the SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities repository have?",
    "output": "The 0nutation/SpeechGPT repository has 1,396 stars and 95 forks on GitHub."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2405.04324&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2405.04324&start=0&max_results=10 is available at https://github.com/ibm-granite/granite-code-models (ibm-granite/granite-code-models)."
  },
  {
    "input": "What programming language is StarCoder: May the source be with you! implemented in?",
    "output": "The StarCoder: May the source be with you! implementation is written in Python."
  },
  {
    "input": "Which field does XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale belong to?",
    "output": "XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2302.08242&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2302.08242&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at haoliuhl/language-quantized-autoencoders which language quantized autoencoders It is written in Python. The repository has 111 stars, indicating high community interest."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2406.09403&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2406.09403&start=0&max_results=10 paper was published in 2024."
  },
  {
    "input": "When was the The Flan Collection: Designing Data and Methods for Effective Instruction Tuning paper published?",
    "output": "The The Flan Collection: Designing Data and Methods for Effective Instruction Tuning paper was published in 2023."
  },
  {
    "input": "What programming language is Longformer: The Long-Document Transformer implemented in?",
    "output": "The Longformer: The Long-Document Transformer implementation is written in Python."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2312.02783&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2312.02783&start=0&max_results=10 is available at https://github.com/ContextualAI/HALOs (ContextualAI/HALOs)."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2503.00735&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2503.00735&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "Tell me the title of ArXiv paper 2301.00303.",
    "output": "arXiv Query: search_query=&id_list=2301.00303&start=0&max_results=10"
  },
  {
    "input": "What is the GitHub repository for Denoising Diffusion Probabilistic Models?",
    "output": "The code for Denoising Diffusion Probabilistic Models is available at https://github.com/hojonathanho/diffusion (hojonathanho/diffusion)."
  },
  {
    "input": "What does the microsoft/unilm repository do?",
    "output": "Large-scale Self-supervised Pre-training Across Tasks, Languages, and Modalities"
  },
  {
    "input": "Tell me the title of ArXiv paper 2405.03520.",
    "output": "arXiv Query: search_query=&id_list=2405.03520&start=0&max_results=10"
  },
  {
    "input": "In what year did P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning come out?",
    "output": "The P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning paper was published in 2021."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2502.09992&start=0&max_results=10 repository have?",
    "output": "The Open-Reasoner-Zero/Open-Reasoner-Zero repository has 2,068 stars and 119 forks on GitHub."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2205.12755?",
    "output": "Beir: A Heterogeneous Benchmark for IR"
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2012.06009&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2012.06009&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2303.04671&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2303.04671&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What programming language is Inductive Representation Learning on Large Graphs implemented in?",
    "output": "The Inductive Representation Learning on Large Graphs implementation is written in Python."
  },
  {
    "input": "Summarize the LoRA: Low-Rank Adaptation of Large Language Models paper.",
    "output": "LoRA: Low-Rank Adaptation of Large Language Models is a 2021 paper in Computation and Language (Natural Language Processing). The implementation is available at microsoft/LoRA which code for loralib, an implementation of \"lora: low-rank adaptation of large language models\" It is written in Python. The repository has 12,979 stars, indicating high community interest."
  },
  {
    "input": "What is the ArXiv category of Inductive Representation Learning on Large Graphs?",
    "output": "Inductive Representation Learning on Large Graphs is in the cs.SI (cs.SI) category."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2309.16588?",
    "output": "Segment Anything in Medical Images"
  },
  {
    "input": "When was the Distilling the Knowledge in a Neural Network paper published?",
    "output": "The Distilling the Knowledge in a Neural Network paper was published in 2015."
  },
  {
    "input": "How popular is the MiniGPT-4: Enhancing Vision-Language Understanding implementation?",
    "output": "The Vision-CAIR/MiniGPT-4 repository has 25,756 stars and 2,934 forks on GitHub."
  },
  {
    "input": "What is the GitHub repository for XLNet: Generalized Autoregressive Pretraining?",
    "output": "The code for XLNet: Generalized Autoregressive Pretraining is available at https://github.com/zihangdai/xlnet (zihangdai/xlnet)."
  },
  {
    "input": "What language is the Open-Reasoner-Zero/Open-Reasoner-Zero repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2502.12115&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What is the GitHub repository for PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel?",
    "output": "The code for PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel is available at https://github.com/pytorch/pytorch (pytorch/pytorch)."
  },
  {
    "input": "What language is the tianjunz/HIR repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2302.05206&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What year was Attention Is All You Need released?",
    "output": "The Attention Is All You Need paper was published in 2017."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2405.02803&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2405.02803&start=0&max_results=10 paper was published in 2024."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2405.04324&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2405.04324&start=0&max_results=10 is available at https://github.com/ibm-granite/granite-code-models (ibm-granite/granite-code-models)."
  },
  {
    "input": "What's the implementation repository for Linformer: Self-Attention with Linear Complexity?",
    "output": "The code for Linformer: Self-Attention with Linear Complexity is available at https://github.com/tatp22/linformer-pytorch (tatp22/linformer-pytorch)."
  },
  {
    "input": "Explain the google-deepmind/dqn implementation of Playing Atari with Deep Reinforcement Learning.",
    "output": "Lua/Torch implementation of DQN (Nature, 2015)"
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2102.01192&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2102.01192&start=0&max_results=10 is available at https://github.com/facebookresearch/fairseq (facebookresearch/fairseq)."
  },
  {
    "input": "What year was SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities released?",
    "output": "The SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities paper was published in 2023."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2302.00902&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2302.00902&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2009.06367&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2009.06367&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2301.11316&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2301.11316&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at autonomousvision/stylegan-t which [icml'23] stylegan-t: unlocking the power of gans for fast large-scale text-to-image synthesis It is written in Python. The repository has 1,196 stars, indicating high community interest."
  },
  {
    "input": "What programming language is An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale implemented in?",
    "output": "The An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale implementation is written in Jupyter Notebook."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2301.12652&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2301.12652&start=0&max_results=10 is available at https://github.com/amazon-science/mm-cot (amazon-science/mm-cot)."
  },
  {
    "input": "What paper has ArXiv ID 2204.14198?",
    "output": "Flamingo: A Visual Language Model for Few-Shot Learning"
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2501.11067&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2501.11067&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at plurai-ai/intellagent which a framework for comprehensive diagnosis and optimization of agents using simulated, realistic synthetic interactions It is written in Python. The repository has 1,148 stars, indicating high community interest."
  },
  {
    "input": "Explain the ReAct: Synergizing Reasoning and Acting in Language Models paper.",
    "output": "ReAct: Synergizing Reasoning and Acting in Language Models is a 2022 paper in Computation and Language (Natural Language Processing). The implementation is available at ysymyth/ReAct which [iclr 2023] react: synergizing reasoning and acting in language models It is written in Jupyter Notebook. The repository has 3,225 stars, indicating high community interest."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2501.13545&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2501.13545&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "Where can I find the code for Zephyr: Direct Distillation of LM Alignment?",
    "output": "The code for Zephyr: Direct Distillation of LM Alignment is available at https://github.com/huggingface/alignment-handbook (huggingface/alignment-handbook)."
  },
  {
    "input": "What paper has ArXiv ID 1810.04805?",
    "output": "BERT: Pre-training of Deep Bidirectional Transformers"
  },
  {
    "input": "Describe the openai/frontier-evals repository that implements arXiv Query: search_query=&id_list=2504.01848&start=0&max_results=10.",
    "output": "OpenAI Frontier Evals"
  },
  {
    "input": "What is FlashAttention-2: Faster Attention with Better Parallelism about?",
    "output": "FlashAttention-2: Faster Attention with Better Parallelism is a 2023 paper in Machine Learning. The implementation is available at Dao-AILab/flash-attention which fast and memory-efficient exact attention It is written in Python. The repository has 20,719 stars, indicating high community interest."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2505.22101&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2505.22101&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at ruixin31/Spurious_Rewards It is written in Python. The repository has 342 stars, indicating high community interest."
  },
  {
    "input": "What paper has ArXiv ID 1807.11164?",
    "output": "YOLOX: Exceeding YOLO Series in 2021"
  },
  {
    "input": "Explain the Vision-CAIR/MiniGPT-4 implementation of MiniGPT-4: Enhancing Vision-Language Understanding.",
    "output": "Open-sourced codes for MiniGPT-4 and MiniGPT-v2 (https://minigpt-4.github.io, https://minigpt-v2.github.io/)"
  },
  {
    "input": "When was the Analyzing the Structure of Attention in a Transformer Language Model paper published?",
    "output": "The Analyzing the Structure of Attention in a Transformer Language Model paper was published in 2020."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2301.11305&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2301.11305&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What language is the google-research/FLAN repository written in?",
    "output": "The The Flan Collection: Designing Data and Methods for Effective Instruction Tuning implementation is written in Python."
  },
  {
    "input": "Describe the ibm-granite/granite-code-models repository that implements arXiv Query: search_query=&id_list=2405.03548&start=0&max_results=10.",
    "output": "Granite Code Models: A Family of Open Foundation Models for Code Intelligence"
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2007.16122&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2007.16122&start=0&max_results=10 is available at https://github.com/linkedin/gdmix (linkedin/gdmix)."
  },
  {
    "input": "What language is the haotian-liu/LLaVA repository written in?",
    "output": "The LLaVA: Large Language and Vision Assistant implementation is written in Python."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2007.16122&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2007.16122&start=0&max_results=10 paper was published in 2020."
  },
  {
    "input": "What programming language is ALBERT: A Lite BERT implemented in?",
    "output": "The ALBERT: A Lite BERT implementation is written in Python."
  },
  {
    "input": "Explain the FMInference/FlexLLMGen implementation of arXiv Query: search_query=&id_list=2303.08518&start=0&max_results=10.",
    "output": "Running large language models on a single GPU for throughput-oriented scenarios."
  },
  {
    "input": "Summarize the Sequence to Sequence Learning with Neural Networks paper.",
    "output": "Sequence to Sequence Learning with Neural Networks is a 2014 paper in Computation and Language (Natural Language Processing). The implementation is available at farizrahman4u/seq2seq which sequence to sequence learning with keras It is written in Python. The repository has 3,173 stars, indicating high community interest."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2406.09403&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2406.09403&start=0&max_results=10 is available at https://github.com/lamini-ai/Lamini-Memory-Tuning (lamini-ai/Lamini-Memory-Tuning)."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2503.22678&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2503.22678&start=0&max_results=10 is available at https://github.com/MAXNORM8650/MedAgentSim (MAXNORM8650/MedAgentSim)."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2302.09778&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2302.09778&start=0&max_results=10 is available at https://github.com/ali-vilab/composer (ali-vilab/composer)."
  },
  {
    "input": "What is Dream to Control: Learning Behaviors by Latent Imagination about?",
    "output": "Dream to Control: Learning Behaviors by Latent Imagination is a 2019 paper in Machine Learning. The implementation is available at danijar/dreamer which dream to control: learning behaviors by latent imagination It is written in Python. The repository has 565 stars, indicating high community interest."
  },
  {
    "input": "Explain the Neural Message Passing for Quantum Chemistry paper.",
    "output": "Neural Message Passing for Quantum Chemistry is a 2017 paper in Machine Learning. The implementation is available at priba/nmp_qc which our own implementation of neural message passing for computer vision paper It is written in Python. The repository has 343 stars, indicating high community interest."
  },
  {
    "input": "What is the ArXiv category of Prefix-Tuning: Optimizing Continuous Prompts for Generation?",
    "output": "Prefix-Tuning: Optimizing Continuous Prompts for Generation is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What does the ali-vilab/composer repository do?",
    "output": "Official implementation of \"Composer: Creative and Controllable Image Synthesis with Composable Conditions\""
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2109.04908?",
    "output": "XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale"
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2007.16122&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2007.16122&start=0&max_results=10 paper was published in 2020."
  },
  {
    "input": "Tell me the title of ArXiv paper 2108.12409.",
    "output": "Linformer: Self-Attention with Linear Complexity"
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2301.13188&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2301.13188&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at amazon-science/mm-cot which official implementation for \"multimodal chain-of-thought reasoning in language models\" (stay tuned and more will be updated) It is written in Python. The repository has 3,982 stars, indicating high community interest."
  },
  {
    "input": "What's the implementation repository for S4: Efficiently Modeling Long Sequences?",
    "output": "The code for S4: Efficiently Modeling Long Sequences is available at https://github.com/state-spaces/s4 (state-spaces/s4)."
  },
  {
    "input": "When was the MiniGPT-4: Enhancing Vision-Language Understanding paper published?",
    "output": "The MiniGPT-4: Enhancing Vision-Language Understanding paper was published in 2023."
  },
  {
    "input": "What category is the FLAN: Finetuned Language Models Are Zero-Shot Learners paper in?",
    "output": "FLAN: Finetuned Language Models Are Zero-Shot Learners is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Explain the AkariAsai/self-rag implementation of Self-RAG: Learning to Retrieve, Generate, and Critique.",
    "output": "This includes the original implementation of SELF-RAG: Learning to Retrieve, Generate and Critique through self-reflection by Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2501.19393&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2501.19393&start=0&max_results=10 is available at https://github.com/simplescaling/s1 (simplescaling/s1)."
  },
  {
    "input": "Tell me the title of ArXiv paper 2410.12896.",
    "output": "arXiv Query: search_query=&id_list=2410.12896&start=0&max_results=10"
  },
  {
    "input": "What category is the DreamFusion: Text-to-3D using 2D Diffusion paper in?",
    "output": "DreamFusion: Text-to-3D using 2D Diffusion is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "Explain the MMLU: Measuring Massive Multitask Language Understanding paper.",
    "output": "MMLU: Measuring Massive Multitask Language Understanding is a 2021 paper in cs.CY. The implementation is available at hendrycks/test which measuring massive multitask language understanding | iclr 2021 It is written in Python. The repository has 1,523 stars, indicating high community interest."
  },
  {
    "input": "Explain the mit-han-lab/smoothquant implementation of SmoothQuant: Accurate and Efficient Post-Training Quantization.",
    "output": "[ICML 2023] SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models"
  },
  {
    "input": "Summarize the PyTorch: An Imperative Style, High-Performance Deep Learning Library paper.",
    "output": "PyTorch: An Imperative Style, High-Performance Deep Learning Library is a 2019 paper in Machine Learning. The implementation is available at pytorch/pytorch which tensors and dynamic neural networks in python with strong gpu acceleration It is written in Python. The repository has 95,348 stars, indicating high community interest."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2302.14838&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2302.14838&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What paper has ArXiv ID 2306.04190?",
    "output": "ChemCrow: Augmenting large-language models with chemistry tools"
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2302.14838&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2302.14838&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at ruiqi-zhong/D5 which the github repo for goal driven discovery of distributional differences via language descriptions It is written in Python. The repository has 71 stars, indicating high community interest."
  },
  {
    "input": "Summarize the Zephyr: Direct Distillation of LM Alignment paper.",
    "output": "Zephyr: Direct Distillation of LM Alignment is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at huggingface/alignment-handbook which robust recipes to align language models with human and ai preferences It is written in Python. The repository has 5,429 stars, indicating high community interest."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2405.04324&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2405.04324&start=0&max_results=10 is a 2024 paper in Machine Learning. The implementation is available at ibm-granite/granite-code-models which granite code models: a family of open foundation models for code intelligence The repository has 1,240 stars, indicating high community interest."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2303.03378&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2303.03378&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at NVlabs/prismer which the implementation of \"prismer: a vision-language model with multi-task experts\". It is written in Python. The repository has 1,307 stars, indicating high community interest."
  },
  {
    "input": "Explain the pyg-team/pytorch_geometric implementation of PyTorch Geometric: Library for Geometric Deep Learning.",
    "output": "Graph Neural Network Library for PyTorch"
  },
  {
    "input": "Describe the jacopotagliabue/you-dont-need-a-bigger-boat repository that implements arXiv Query: search_query=&id_list=2107.07346&start=0&max_results=10.",
    "output": "An end-to-end implementation of intent prediction with Metaflow and other cool tools"
  },
  {
    "input": "What paper has ArXiv ID 2302.07842?",
    "output": "The Flan Collection: Designing Data and Methods for Effective Instruction Tuning"
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2302.05543?",
    "output": "ControlNet: Adding Conditional Control to Text-to-Image Diffusion"
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2503.22708&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2503.22708&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at allenai/codescientist which codescientist: an automated scientific discovery system for code-based experiments It is written in Python. The repository has 301 stars, indicating high community interest."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2301.11305&start=0&max_results=10 implementation?",
    "output": "The autonomousvision/stylegan-t repository has 1,196 stars and 61 forks on GitHub."
  },
  {
    "input": "What year was TensorFlow: A System for Large-Scale Machine Learning released?",
    "output": "The TensorFlow: A System for Large-Scale Machine Learning paper was published in 2016."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2502.03387&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2502.03387&start=0&max_results=10 is available at https://github.com/GAIR-NLP/LIMO (GAIR-NLP/LIMO)."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2301.02828&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2301.02828&start=0&max_results=10 is available at https://github.com/frankxu2004/knnlm-why (frankxu2004/knnlm-why)."
  },
  {
    "input": "Tell me the title of ArXiv paper 2308.12950.",
    "output": "Code Llama: Open Foundation Models for Code"
  },
  {
    "input": "What is Decoupled Weight Decay Regularization about?",
    "output": "Decoupled Weight Decay Regularization is a 2017 paper in Machine Learning. The implementation is available at pytorch/pytorch which tensors and dynamic neural networks in python with strong gpu acceleration It is written in Python. The repository has 95,348 stars, indicating high community interest."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2303.04671&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2303.04671&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Give me an overview of StarCoder: May the source be with you!.",
    "output": "StarCoder: May the source be with you! is a 2023 paper in Machine Learning. The implementation is available at bigcode-project/starcoder which home of starcoder: fine-tuning & inference! It is written in Python. The repository has 7,473 stars, indicating high community interest."
  },
  {
    "input": "What category is the The Power of Scale for Parameter-Efficient Prompt Tuning paper in?",
    "output": "The Power of Scale for Parameter-Efficient Prompt Tuning is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "In what year did SQuAD: 100,000+ Questions for Machine Comprehension of Text come out?",
    "output": "The SQuAD: 100,000+ Questions for Machine Comprehension of Text paper was published in 2016."
  },
  {
    "input": "How many stars does the PyTorch: An Imperative Style, High-Performance Deep Learning Library repository have?",
    "output": "The pytorch/pytorch repository has 95,348 stars and 26,003 forks on GitHub."
  },
  {
    "input": "What does the pytorch/pytorch repository do?",
    "output": "Tensors and Dynamic neural networks in Python with strong GPU acceleration"
  },
  {
    "input": "How popular is the DeBERTa: Decoding-enhanced BERT with Disentangled Attention implementation?",
    "output": "The microsoft/DeBERTa repository has 2,169 stars and 238 forks on GitHub."
  },
  {
    "input": "Explain the lm-sys/FastChat implementation of Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena.",
    "output": "An open platform for training, serving, and evaluating large language models. Release repo for Vicuna and Chatbot Arena."
  },
  {
    "input": "Describe the lm-sys/FastChat repository that implements Vicuna: An Open-Source Chatbot.",
    "output": "An open platform for training, serving, and evaluating large language models. Release repo for Vicuna and Chatbot Arena."
  },
  {
    "input": "Where can I find the code for PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel?",
    "output": "The code for PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel is available at https://github.com/pytorch/pytorch (pytorch/pytorch)."
  },
  {
    "input": "Describe the facebookresearch/TransCoder repository that implements arXiv Query: search_query=&id_list=2004.13637&start=0&max_results=10.",
    "output": "Public release of the TransCoder research project https://arxiv.org/pdf/2006.03511.pdf"
  },
  {
    "input": "Where can I find the code for MMLU: Measuring Massive Multitask Language Understanding?",
    "output": "The code for MMLU: Measuring Massive Multitask Language Understanding is available at https://github.com/hendrycks/test (hendrycks/test)."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2303.09431&start=0&max_results=10 implementation?",
    "output": "The FMInference/FlexLLMGen repository has 9,379 stars and 584 forks on GitHub."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2006.03511&start=0&max_results=10 repository have?",
    "output": "The facebookresearch/TransCoder repository has 1,722 stars and 261 forks on GitHub."
  },
  {
    "input": "What language is the autonomousvision/stylegan-t repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2301.11316&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "How popular is the SQuAD: 100,000+ Questions for Machine Comprehension of Text implementation?",
    "output": "The rajpurkar/SQuAD-explorer repository has 570 stars and 121 forks on GitHub."
  },
  {
    "input": "What does the rail-berkeley/softlearning repository do?",
    "output": "Softlearning is a reinforcement learning framework for training maximum entropy policies in continuous domains. Includes the official implementation of the Soft Actor-Critic algorithm."
  },
  {
    "input": "What does the MAXNORM8650/MedAgentSim repository do?",
    "output": "MedAgentSim: Self-Evolving Multi-Agent Simulations for Realistic Clinical Interactions, MICCAI 2025 (oral and early accepted)"
  },
  {
    "input": "In what year did Longformer: The Long-Document Transformer come out?",
    "output": "The Longformer: The Long-Document Transformer paper was published in 2020."
  },
  {
    "input": "When was the PyTorch Geometric: Library for Geometric Deep Learning paper published?",
    "output": "The PyTorch Geometric: Library for Geometric Deep Learning paper was published in 2019."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2405.03520&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2405.03520&start=0&max_results=10 is available at https://github.com/ibm-granite/granite-code-models (ibm-granite/granite-code-models)."
  },
  {
    "input": "What language is the openai/summarize-from-feedback repository written in?",
    "output": "The Learning to summarize from human feedback implementation is written in Python."
  },
  {
    "input": "Tell me the title of ArXiv paper 2301.10343.",
    "output": "arXiv Query: search_query=&id_list=2301.10343&start=0&max_results=10"
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2302.00902&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2302.00902&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "How popular is the P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning implementation?",
    "output": "The THUDM/P-tuning-v2 repository has 2,067 stars and 206 forks on GitHub."
  },
  {
    "input": "What language is the google-deepmind/tracr repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2301.04104&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2004.13637&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2004.13637&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "How many stars does the The Flan Collection: Designing Data and Methods for Effective Instruction Tuning repository have?",
    "output": "The google-research/FLAN repository has 1,554 stars and 161 forks on GitHub."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2301.01181&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2301.01181&start=0&max_results=10 implementation is written in Jupyter Notebook."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2305.06161?",
    "output": "StarCoder: May the source be with you!"
  },
  {
    "input": "Summarize the GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers paper.",
    "output": "GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers is a 2022 paper in Machine Learning. The implementation is available at IST-DASLab/gptq which code for the iclr 2023 paper \"gptq: accurate post-training quantization of generative pretrained transformers\". It is written in Python. The repository has 2,221 stars, indicating high community interest."
  },
  {
    "input": "Give me an overview of TinyBERT: Distilling BERT for Natural Language Understanding.",
    "output": "TinyBERT: Distilling BERT for Natural Language Understanding is a 2019 paper in Computation and Language (Natural Language Processing). The implementation is available at huawei-noah/Pretrained-Language-Model which pretrained language model and its related optimization techniques developed by huawei noah's ark lab. It is written in Python. The repository has 3,149 stars, indicating high community interest."
  },
  {
    "input": "How many stars does the Linformer: Self-Attention with Linear Complexity repository have?",
    "output": "The tatp22/linformer-pytorch repository has 421 stars and 37 forks on GitHub."
  },
  {
    "input": "Explain the haitongli/knowledge-distillation-pytorch implementation of Distilling the Knowledge in a Neural Network.",
    "output": "A PyTorch implementation for exploring deep and shallow knowledge distillation (KD) experiments with flexibility"
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2301.10343&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2301.10343&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2504.01848&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2504.01848&start=0&max_results=10 is available at https://github.com/openai/frontier-evals (openai/frontier-evals)."
  },
  {
    "input": "Where can I find the code for Universal Language Model Fine-tuning for Text Classification?",
    "output": "The code for Universal Language Model Fine-tuning for Text Classification is available at https://github.com/fastai/fastai (fastai/fastai)."
  },
  {
    "input": "What are the key topics in the rail-berkeley/softlearning repository?",
    "output": "The rail-berkeley/softlearning repository covers: reinforcement-learning, soft-actor-critic, deep-learning, deep-reinforcement-learning, deep-neural-networks, machine-learning."
  },
  {
    "input": "What is the ArXiv category of BLIP-2: Bootstrapping Language-Image Pre-training?",
    "output": "BLIP-2: Bootstrapping Language-Image Pre-training is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "What paper has ArXiv ID 1609.02907?",
    "output": "Semi-Supervised Classification with Graph Convolutional Networks"
  },
  {
    "input": "How popular is the RAG: Retrieval-Augmented Generation implementation?",
    "output": "The facebookresearch/DPR repository has 1,847 stars and 315 forks on GitHub."
  },
  {
    "input": "What's the implementation repository for Very Deep Convolutional Networks for Large-Scale Image Recognition?",
    "output": "The code for Very Deep Convolutional Networks for Large-Scale Image Recognition is available at https://github.com/machrisaa/tensorflow-vgg (machrisaa/tensorflow-vgg)."
  },
  {
    "input": "Which field does Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena belong to?",
    "output": "Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2301.00303&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2301.00303&start=0&max_results=10 is available at https://github.com/facebookresearch/ConvNeXt-V2 (facebookresearch/ConvNeXt-V2)."
  },
  {
    "input": "What topics does the Mistral 7B implementation cover?",
    "output": "The mistralai/mistral-inference repository covers: llm, llm-inference, mistralai."
  },
  {
    "input": "Which field does P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning belong to?",
    "output": "P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "How popular is the HuggingGPT: Solving AI Tasks with ChatGPT implementation?",
    "output": "The microsoft/JARVIS repository has 24,460 stars and 2,051 forks on GitHub."
  },
  {
    "input": "What paper has ArXiv ID 2302.10866?",
    "output": "arXiv Query: search_query=&id_list=2302.10866&start=0&max_results=10"
  },
  {
    "input": "What's the implementation repository for SmoothQuant: Accurate and Efficient Post-Training Quantization?",
    "output": "The code for SmoothQuant: Accurate and Efficient Post-Training Quantization is available at https://github.com/mit-han-lab/smoothquant (mit-han-lab/smoothquant)."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2302.03917&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2302.03917&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at mit-han-lab/offsite-tuning which offsite-tuning: transfer learning without full model It is written in Python. The repository has 382 stars, indicating high community interest."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2301.13188&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2301.13188&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "How popular is the Densely Connected Convolutional Networks implementation?",
    "output": "The liuzhuang13/DenseNet repository has 4,843 stars and 1,072 forks on GitHub."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2303.08518&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2303.08518&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is the GitHub repository for RAG: Retrieval-Augmented Generation?",
    "output": "The code for RAG: Retrieval-Augmented Generation is available at https://github.com/facebookresearch/DPR (facebookresearch/DPR)."
  },
  {
    "input": "Summarize the Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning paper.",
    "output": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning is a 2018 paper in Machine Learning. The implementation is available at rail-berkeley/softlearning which softlearning is a reinforcement learning framework for training maximum entropy policies in continuous domains. includes the official implementation of the soft actor-critic algorithm. It is written in Python. The repository has 1,370 stars, indicating high community interest."
  },
  {
    "input": "Summarize the BEiT: BERT Pre-Training of Image Transformers paper.",
    "output": "BEiT: BERT Pre-Training of Image Transformers is a 2021 paper in Computer Vision and Pattern Recognition. The implementation is available at microsoft/unilm which large-scale self-supervised pre-training across tasks, languages, and modalities It is written in Python. The repository has 21,848 stars, indicating high community interest."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2405.03548&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2405.03548&start=0&max_results=10 is a 2024 paper in Machine Learning. The implementation is available at ibm-granite/granite-code-models which granite code models: a family of open foundation models for code intelligence The repository has 1,240 stars, indicating high community interest."
  },
  {
    "input": "What does the allenai/codescientist repository do?",
    "output": "CodeScientist: An automated scientific discovery system for code-based experiments"
  },
  {
    "input": "What paper has ArXiv ID 2310.06825?",
    "output": "Mistral 7B"
  },
  {
    "input": "How popular is the Zephyr: Direct Distillation of LM Alignment implementation?",
    "output": "The huggingface/alignment-handbook repository has 5,429 stars and 464 forks on GitHub."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2302.07459&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2302.07459&start=0&max_results=10 is available at https://github.com/haoliuhl/language-quantized-autoencoders (haoliuhl/language-quantized-autoencoders)."
  },
  {
    "input": "Where can I find the code for EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification?",
    "output": "The code for EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification is available at https://github.com/jasonwei20/eda_nlp (jasonwei20/eda_nlp)."
  },
  {
    "input": "Describe the mit-han-lab/llm-awq repository that implements AWQ: Activation-aware Weight Quantization for LLM Compression.",
    "output": "[MLSys 2024 Best Paper Award] AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration"
  },
  {
    "input": "What language is the zetaalphavector/InPars repository written in?",
    "output": "The InPars: Data Augmentation for Information Retrieval implementation is written in Python."
  },
  {
    "input": "What topics does the Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning implementation cover?",
    "output": "The rail-berkeley/softlearning repository covers: reinforcement-learning, soft-actor-critic, deep-learning, deep-reinforcement-learning, deep-neural-networks, machine-learning."
  },
  {
    "input": "What is Contriever: Unsupervised Dense Information Retrieval about?",
    "output": "Contriever: Unsupervised Dense Information Retrieval is a 2022 paper in Information Retrieval. The implementation is available at facebookresearch/contriever which contriever: unsupervised dense information retrieval with contrastive learning It is written in Python. The repository has 763 stars, indicating high community interest."
  },
  {
    "input": "What does the salesforce/LAVIS repository do?",
    "output": "LAVIS - A One-stop Library for Language-Vision Intelligence"
  },
  {
    "input": "What programming language is Text Embeddings by Weakly-Supervised Contrastive Pre-training implemented in?",
    "output": "The Text Embeddings by Weakly-Supervised Contrastive Pre-training implementation is written in Python."
  },
  {
    "input": "What paper has ArXiv ID 2309.10305?",
    "output": "WizardCoder: Empowering Code LLMs with Evol-Instruct"
  },
  {
    "input": "Explain the FlashAttention: Fast and Memory-Efficient Exact Attention paper.",
    "output": "FlashAttention: Fast and Memory-Efficient Exact Attention is a 2022 paper in Machine Learning. The implementation is available at Dao-AILab/flash-attention which fast and memory-efficient exact attention It is written in Python. The repository has 20,719 stars, indicating high community interest."
  },
  {
    "input": "What is the GitHub repository for Universal Language Model Fine-tuning for Text Classification?",
    "output": "The code for Universal Language Model Fine-tuning for Text Classification is available at https://github.com/fastai/fastai (fastai/fastai)."
  },
  {
    "input": "What year was Analyzing the Structure of Attention in a Transformer Language Model released?",
    "output": "The Analyzing the Structure of Attention in a Transformer Language Model paper was published in 2020."
  },
  {
    "input": "What year was XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale released?",
    "output": "The XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale paper was published in 2021."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2104.00355&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2104.00355&start=0&max_results=10 is available at https://github.com/facebookresearch/fairseq (facebookresearch/fairseq)."
  },
  {
    "input": "How many stars does the FlashAttention-2: Faster Attention with Better Parallelism repository have?",
    "output": "The Dao-AILab/flash-attention repository has 20,719 stars and 2,160 forks on GitHub."
  },
  {
    "input": "When was the RoBERTa: A Robustly Optimized BERT Pretraining Approach paper published?",
    "output": "The RoBERTa: A Robustly Optimized BERT Pretraining Approach paper was published in 2019."
  },
  {
    "input": "What's the implementation repository for Universal Language Model Fine-tuning for Text Classification?",
    "output": "The code for Universal Language Model Fine-tuning for Text Classification is available at https://github.com/fastai/fastai (fastai/fastai)."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2302.14233&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2302.14233&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2504.16084&start=0&max_results=10 repository have?",
    "output": "The Crista23/goal_directedness_llms repository has 12 stars and 2 forks on GitHub."
  },
  {
    "input": "What language is the hendrycks/test repository written in?",
    "output": "The MMLU: Measuring Massive Multitask Language Understanding implementation is written in Python."
  },
  {
    "input": "When was the HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction paper published?",
    "output": "The HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction paper was published in 2021."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2312.02120?",
    "output": "arXiv Query: search_query=&id_list=2312.02120&start=0&max_results=10"
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2109.03264&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2109.03264&start=0&max_results=10 is available at https://github.com/facebookresearch/fairseq (facebookresearch/fairseq)."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2301.05062&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2301.05062&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at google-deepmind/tracr It is written in Python. The repository has 548 stars, indicating high community interest."
  },
  {
    "input": "When was the SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities paper published?",
    "output": "The SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities paper was published in 2023."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2303.03378&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2303.03378&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "When was the Reformer: The Efficient Transformer paper published?",
    "output": "The Reformer: The Efficient Transformer paper was published in 2020."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2302.12246&start=0&max_results=10 repository have?",
    "output": "The shizhediao/active-prompt repository has 247 stars and 30 forks on GitHub."
  },
  {
    "input": "What does the plurai-ai/intellagent repository do?",
    "output": "A framework for comprehensive diagnosis and optimization of agents using simulated, realistic synthetic interactions"
  },
  {
    "input": "How many stars does the SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding repository have?",
    "output": "The nyu-mll/jiant repository has 1,672 stars and 297 forks on GitHub."
  },
  {
    "input": "What category is the MMLU: Measuring Massive Multitask Language Understanding paper in?",
    "output": "MMLU: Measuring Massive Multitask Language Understanding is in the cs.CY (cs.CY) category."
  },
  {
    "input": "Explain the facebookresearch/contriever implementation of Contriever: Unsupervised Dense Information Retrieval.",
    "output": "Contriever: Unsupervised Dense Information Retrieval with Contrastive Learning"
  },
  {
    "input": "In what year did DeBERTa: Decoding-enhanced BERT with Disentangled Attention come out?",
    "output": "The DeBERTa: Decoding-enhanced BERT with Disentangled Attention paper was published in 2020."
  },
  {
    "input": "What topics does the DeBERTa: Decoding-enhanced BERT with Disentangled Attention implementation cover?",
    "output": "The microsoft/DeBERTa repository covers: bert, deeplearning, representation-learning, roberta, language-model, natural-language-understanding, self-attention, transformer-encoder."
  },
  {
    "input": "How popular is the HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction implementation?",
    "output": "The facebookresearch/fairseq repository has 31,985 stars and 6,630 forks on GitHub."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2302.14233&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2302.14233&start=0&max_results=10 is available at https://github.com/ruiqi-zhong/D5 (ruiqi-zhong/D5)."
  },
  {
    "input": "Explain the Beir: A Heterogeneous Benchmark for IR paper.",
    "output": "Beir: A Heterogeneous Benchmark for IR is a 2022 paper in Information Retrieval. The implementation is available at beir-cellar/beir which a heterogeneous benchmark for information retrieval. easy to use, evaluate your models across 15+ diverse ir datasets. It is written in Python. The repository has 2,007 stars, indicating high community interest."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2312.02120&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2312.02120&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2301.01751&start=0&max_results=10 repository have?",
    "output": "The oughtinc/ice repository has 563 stars and 70 forks on GitHub."
  },
  {
    "input": "What programming language is BLIP-2: Bootstrapping Language-Image Pre-training implemented in?",
    "output": "The BLIP-2: Bootstrapping Language-Image Pre-training implementation is written in Jupyter Notebook."
  },
  {
    "input": "Describe the ashawkey/stable-dreamfusion repository that implements DreamFusion: Text-to-3D using 2D Diffusion.",
    "output": "Text-to-3D & Image-to-3D & Mesh Exportation with NeRF + Diffusion."
  },
  {
    "input": "Explain the openai/evals implementation of GPT-4 Technical Report.",
    "output": "Evals is a framework for evaluating LLMs and LLM systems, and an open-source registry of benchmarks."
  },
  {
    "input": "What year was PyTorch Geometric: Library for Geometric Deep Learning released?",
    "output": "The PyTorch Geometric: Library for Geometric Deep Learning paper was published in 2019."
  },
  {
    "input": "What's the implementation repository for Generating Wikipedia by Summarizing Long Sequences?",
    "output": "The code for Generating Wikipedia by Summarizing Long Sequences is available at https://github.com/tensorflow/tensor2tensor (tensorflow/tensor2tensor)."
  },
  {
    "input": "What is Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context about?",
    "output": "Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context is a 2019 paper in Computation and Language (Natural Language Processing). The implementation is available at kimiyoung/transformer-xl It is written in Python. The repository has 3,679 stars, indicating high community interest."
  },
  {
    "input": "What does the Vision-CAIR/MiniGPT-4 repository do?",
    "output": "Open-sourced codes for MiniGPT-4 and MiniGPT-v2 (https://minigpt-4.github.io, https://minigpt-v2.github.io/)"
  },
  {
    "input": "Describe the autonomousvision/stylegan-t repository that implements arXiv Query: search_query=&id_list=2301.11316&start=0&max_results=10.",
    "output": "[ICML'23] StyleGAN-T: Unlocking the Power of GANs for Fast Large-Scale Text-to-Image Synthesis"
  }
]