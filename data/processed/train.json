[
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2007.16122&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2007.16122&start=0&max_results=10 is a 2020 paper in Machine Learning. The implementation is available at linkedin/gdmix which a deep ranking personalization framework It is written in Python. The repository has 133 stars, indicating high community interest."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2303.08518&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2303.08518&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at FMInference/FlexLLMGen which running large language models on a single gpu for throughput-oriented scenarios. It is written in Python. The repository has 9,379 stars, indicating high community interest."
  },
  {
    "input": "Describe the salesforce/LAVIS repository that implements BLIP-2: Bootstrapping Language-Image Pre-training.",
    "output": "LAVIS - A One-stop Library for Language-Vision Intelligence"
  },
  {
    "input": "What are the key topics in the google-research/google-research repository?",
    "output": "The google-research/google-research repository covers: machine-learning, ai, research."
  },
  {
    "input": "Tell me the title of ArXiv paper 1708.05031.",
    "output": "Neural Collaborative Filtering"
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2301.00303&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2301.00303&start=0&max_results=10 is a 2022 paper in Machine Learning. The implementation is available at facebookresearch/ConvNeXt-V2 which code release for convnext v2 model It is written in Python. The repository has 1,890 stars, indicating high community interest."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2303.04129&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2303.04129&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at HazyResearch/safari which convolutions for sequence modeling It is written in Assembly. The repository has 904 stars, indicating high community interest."
  },
  {
    "input": "What programming language is Denoising Diffusion Implicit Models implemented in?",
    "output": "The Denoising Diffusion Implicit Models implementation is written in Python."
  },
  {
    "input": "What language is the clarkkev/attention-analysis repository written in?",
    "output": "The Analyzing the Structure of Attention in a Transformer Language Model implementation is written in Jupyter Notebook."
  },
  {
    "input": "What paper has ArXiv ID 2010.11929?",
    "output": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"
  },
  {
    "input": "Which field does Llama 2: Open Foundation and Fine-Tuned Chat Models belong to?",
    "output": "Llama 2: Open Foundation and Fine-Tuned Chat Models is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What year was Wav2Vec 2.0: A Framework for Self-Supervised Learning of Speech released?",
    "output": "The Wav2Vec 2.0: A Framework for Self-Supervised Learning of Speech paper was published in 2021."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2406.12824&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2406.12824&start=0&max_results=10 is a 2024 paper in Machine Learning. The implementation is available at hpcaitech/Open-Sora which open-sora: democratizing efficient video production for all It is written in Python. The repository has 27,981 stars, indicating high community interest."
  },
  {
    "input": "What is the GitHub repository for HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction?",
    "output": "The code for HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction is available at https://github.com/facebookresearch/fairseq (facebookresearch/fairseq)."
  },
  {
    "input": "What does the nyu-mll/jiant repository do?",
    "output": "jiant is an nlp toolkit"
  },
  {
    "input": "Tell me the title of ArXiv paper 2102.02779.",
    "output": "ViT: An Image is Worth 16x16 Words"
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2302.07459&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2302.07459&start=0&max_results=10 is available at https://github.com/haoliuhl/language-quantized-autoencoders (haoliuhl/language-quantized-autoencoders)."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2006.03511?",
    "output": "arXiv Query: search_query=&id_list=2006.03511&start=0&max_results=10"
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2502.14815&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2502.14815&start=0&max_results=10 is available at https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero (Open-Reasoner-Zero/Open-Reasoner-Zero)."
  },
  {
    "input": "What language is the huggingface/transformers repository written in?",
    "output": "The DistilBERT: a distilled version of BERT implementation is written in Python."
  },
  {
    "input": "Tell me the title of ArXiv paper 2404.03592.",
    "output": "arXiv Query: search_query=&id_list=2404.03592&start=0&max_results=10"
  },
  {
    "input": "What paper has ArXiv ID 2505.22954?",
    "output": "arXiv Query: search_query=&id_list=2505.22954&start=0&max_results=10"
  },
  {
    "input": "Which field does Efficient Estimation of Word Representations in Vector Space belong to?",
    "output": "Efficient Estimation of Word Representations in Vector Space is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What does the ibm-granite/granite-code-models repository do?",
    "output": "Granite Code Models: A Family of Open Foundation Models for Code Intelligence"
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2301.11305&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2301.11305&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "In what year did Prefix-Tuning: Optimizing Continuous Prompts for Generation come out?",
    "output": "The Prefix-Tuning: Optimizing Continuous Prompts for Generation paper was published in 2021."
  },
  {
    "input": "Explain the HuggingGPT: Solving AI Tasks with ChatGPT paper.",
    "output": "HuggingGPT: Solving AI Tasks with ChatGPT is a 2023 paper in Artificial Intelligence. The implementation is available at microsoft/JARVIS which jarvis, a system to connect llms with ml community. paper: https://arxiv.org/pdf/2303.17580.pdf It is written in Python. The repository has 24,460 stars, indicating high community interest."
  },
  {
    "input": "What language is the HazyResearch/safari repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2302.10866&start=0&max_results=10 implementation is written in Assembly."
  },
  {
    "input": "What language is the lucidrains/flamingo-pytorch repository written in?",
    "output": "The Flamingo: A Visual Language Model for Few-Shot Learning implementation is written in Python."
  },
  {
    "input": "What topics does the Wav2Vec 2.0: A Framework for Self-Supervised Learning of Speech implementation cover?",
    "output": "The facebookresearch/fairseq repository covers: python, pytorch, artificial-intelligence."
  },
  {
    "input": "When was the Denoising Diffusion Implicit Models paper published?",
    "output": "The Denoising Diffusion Implicit Models paper was published in 2020."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2501.13824&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2501.13824&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What year was Playing Atari with Deep Reinforcement Learning released?",
    "output": "The Playing Atari with Deep Reinforcement Learning paper was published in 2013."
  },
  {
    "input": "When was the Competition-Level Code Generation with AlphaCode paper published?",
    "output": "The Competition-Level Code Generation with AlphaCode paper was published in 2022."
  },
  {
    "input": "What language is the ruixin31/Spurious_Rewards repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2505.22954&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2503.00735&start=0&max_results=10 repository have?",
    "output": "The LTH14/fractalgen repository has 1,196 stars and 65 forks on GitHub."
  },
  {
    "input": "When was the AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning paper published?",
    "output": "The AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning paper was published in 2022."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2503.20201&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2503.20201&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at sentient-agi/OpenDeepSearch which sota search powered llm It is written in Python. The repository has 3,723 stars, indicating high community interest."
  },
  {
    "input": "Summarize the Mamba: Linear-Time Sequence Modeling with Selective State Spaces paper.",
    "output": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces is a 2023 paper in Machine Learning. The implementation is available at state-spaces/mamba which mamba ssm architecture It is written in Python. The repository has 16,525 stars, indicating high community interest."
  },
  {
    "input": "What category is the ALBERT: A Lite BERT paper in?",
    "output": "ALBERT: A Lite BERT is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What does the openai/whisper repository do?",
    "output": "Robust Speech Recognition via Large-Scale Weak Supervision"
  },
  {
    "input": "Describe the Vision-CAIR/MiniGPT-4 repository that implements MiniGPT-4: Enhancing Vision-Language Understanding.",
    "output": "Open-sourced codes for MiniGPT-4 and MiniGPT-v2 (https://minigpt-4.github.io, https://minigpt-v2.github.io/)"
  },
  {
    "input": "Where can I find the code for Decoupled Weight Decay Regularization?",
    "output": "The code for Decoupled Weight Decay Regularization is available at https://github.com/pytorch/pytorch (pytorch/pytorch)."
  },
  {
    "input": "What's the implementation repository for QLoRA: Efficient Finetuning of Quantized LLMs?",
    "output": "The code for QLoRA: Efficient Finetuning of Quantized LLMs is available at https://github.com/artidoro/qlora (artidoro/qlora)."
  },
  {
    "input": "Which field does Multi-Task Deep Neural Networks for Natural Language Understanding belong to?",
    "output": "Multi-Task Deep Neural Networks for Natural Language Understanding is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2302.04870&start=0&max_results=10 repository have?",
    "output": "The mit-han-lab/offsite-tuning repository has 382 stars and 39 forks on GitHub."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2503.20201&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2503.20201&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "What are the key topics in the microsoft/torchscale repository?",
    "output": "The microsoft/torchscale repository covers: computer-vision, machine-learning, multimodal, natural-language-processing, pretrained-language-model, speech-processing, transformer, translation."
  },
  {
    "input": "How popular is the Diffusion Models Beat GANs on Image Synthesis implementation?",
    "output": "The openai/guided-diffusion repository has 7,161 stars and 886 forks on GitHub."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2303.01469&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2303.01469&start=0&max_results=10 is available at https://github.com/ruiqi-zhong/D5 (ruiqi-zhong/D5)."
  },
  {
    "input": "Which field does HuggingGPT: Solving AI Tasks with ChatGPT belong to?",
    "output": "HuggingGPT: Solving AI Tasks with ChatGPT is in the Artificial Intelligence (cs.AI) category."
  },
  {
    "input": "What are the key topics in the hexiangnan/neural_collaborative_filtering repository?",
    "output": "The hexiangnan/neural_collaborative_filtering repository covers: deep-learning, recommender-system, collaborative-filtering."
  },
  {
    "input": "Where can I find the code for Denoising Diffusion Probabilistic Models?",
    "output": "The code for Denoising Diffusion Probabilistic Models is available at https://github.com/hojonathanho/diffusion (hojonathanho/diffusion)."
  },
  {
    "input": "How many stars does the BLIP: Bootstrapping Language-Image Pre-training repository have?",
    "output": "The salesforce/BLIP repository has 5,583 stars and 758 forks on GitHub."
  },
  {
    "input": "Explain the jax-ml/jax implementation of JAX: Composable transformations of Python+NumPy programs.",
    "output": "Composable transformations of Python+NumPy programs: differentiate, vectorize, JIT to GPU/TPU, and more"
  },
  {
    "input": "What language is the autonomousvision/stylegan-t repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2301.09515&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Which field does XLNet: Generalized Autoregressive Pretraining belong to?",
    "output": "XLNet: Generalized Autoregressive Pretraining is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What are the key topics in the beir-cellar/beir repository?",
    "output": "The beir-cellar/beir repository covers: nlp, information-retrieval, bert, benchmark, sentence-transformers, question-generation, retrieval, passage-retrieval, elasticsearch, dpr, sbert, retrieval-models, dataset, colbert, zero-shot-retrieval, deep-learning, pytorch, llm, rag."
  },
  {
    "input": "What programming language is Semi-Supervised Classification with Graph Convolutional Networks implemented in?",
    "output": "The Semi-Supervised Classification with Graph Convolutional Networks implementation is written in Python."
  },
  {
    "input": "In what year did DistilBERT: a distilled version of BERT come out?",
    "output": "The DistilBERT: a distilled version of BERT paper was published in 2019."
  },
  {
    "input": "Which field does EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification belong to?",
    "output": "EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "In what year did Denoising Diffusion Probabilistic Models come out?",
    "output": "The Denoising Diffusion Probabilistic Models paper was published in 2020."
  },
  {
    "input": "Tell me the title of ArXiv paper 2006.11239.",
    "output": "Denoising Diffusion Probabilistic Models"
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2502.01061&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2502.01061&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What language is the rajpurkar/SQuAD-explorer repository written in?",
    "output": "The Know What You Don't Know: Unanswerable Questions for SQuAD implementation is written in JavaScript."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2503.22708&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2503.22708&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at allenai/codescientist which codescientist: an automated scientific discovery system for code-based experiments It is written in Python. The repository has 301 stars, indicating high community interest."
  },
  {
    "input": "Explain the LTH14/fractalgen implementation of arXiv Query: search_query=&id_list=2502.19328&start=0&max_results=10.",
    "output": "PyTorch implementation of FractalGen https://arxiv.org/abs/2502.17437"
  },
  {
    "input": "What category is the Generating Wikipedia by Summarizing Long Sequences paper in?",
    "output": "Generating Wikipedia by Summarizing Long Sequences is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Describe the rajpurkar/SQuAD-explorer repository that implements SQuAD: 100,000+ Questions for Machine Comprehension of Text.",
    "output": "Visually Explore the Stanford Question Answering Dataset"
  },
  {
    "input": "Summarize the ReAct: Synergizing Reasoning and Acting in Language Models paper.",
    "output": "ReAct: Synergizing Reasoning and Acting in Language Models is a 2022 paper in Computation and Language (Natural Language Processing). The implementation is available at ysymyth/ReAct which [iclr 2023] react: synergizing reasoning and acting in language models It is written in Jupyter Notebook. The repository has 3,225 stars, indicating high community interest."
  },
  {
    "input": "What language is the OpenGVLab/LLaMA-Adapter repository written in?",
    "output": "The LLaMA-Adapter: Efficient Fine-tuning of Language Models implementation is written in Python."
  },
  {
    "input": "When was the PaLM: Scaling Language Modeling with Pathways paper published?",
    "output": "The PaLM: Scaling Language Modeling with Pathways paper was published in 2022."
  },
  {
    "input": "What is QLoRA: Efficient Finetuning of Quantized LLMs about?",
    "output": "QLoRA: Efficient Finetuning of Quantized LLMs is a 2023 paper in Machine Learning. The implementation is available at artidoro/qlora which qlora: efficient finetuning of quantized llms It is written in Jupyter Notebook. The repository has 10,764 stars, indicating high community interest."
  },
  {
    "input": "Give me an overview of Zephyr: Direct Distillation of LM Alignment.",
    "output": "Zephyr: Direct Distillation of LM Alignment is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at huggingface/alignment-handbook which robust recipes to align language models with human and ai preferences It is written in Python. The repository has 5,429 stars, indicating high community interest."
  },
  {
    "input": "What programming language is Prefix-Tuning: Optimizing Continuous Prompts for Generation implemented in?",
    "output": "The Prefix-Tuning: Optimizing Continuous Prompts for Generation implementation is written in Python."
  },
  {
    "input": "What is Wav2Vec 2.0: A Framework for Self-Supervised Learning of Speech about?",
    "output": "Wav2Vec 2.0: A Framework for Self-Supervised Learning of Speech is a 2021 paper in Computation and Language (Natural Language Processing). The implementation is available at facebookresearch/fairseq which facebook ai research sequence-to-sequence toolkit written in python. It is written in Python. The repository has 31,985 stars, indicating high community interest."
  },
  {
    "input": "What category is the Neural Message Passing for Quantum Chemistry paper in?",
    "output": "Neural Message Passing for Quantum Chemistry is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "How many stars does the WizardCoder: Empowering Code LLMs with Evol-Instruct repository have?",
    "output": "The nlpxucan/WizardLM repository has 9,460 stars and 761 forks on GitHub."
  },
  {
    "input": "Describe the facebookresearch/ConvNeXt-V2 repository that implements arXiv Query: search_query=&id_list=2301.00774&start=0&max_results=10.",
    "output": "Code release for ConvNeXt V2 model"
  },
  {
    "input": "What does the huggingface/transformers repository do?",
    "output": "\ud83e\udd17 Transformers: the model-definition framework for state-of-the-art machine learning models in text, vision, audio, and multimodal models, for both inference and training. "
  },
  {
    "input": "What does the AkariAsai/self-rag repository do?",
    "output": "This includes the original implementation of SELF-RAG: Learning to Retrieve, Generate and Critique through self-reflection by Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi."
  },
  {
    "input": "What's the implementation repository for BERT Rediscovers the Classical NLP Pipeline?",
    "output": "The code for BERT Rediscovers the Classical NLP Pipeline is available at https://github.com/clarkkev/attention-analysis (clarkkev/attention-analysis)."
  },
  {
    "input": "What paper has ArXiv ID 1312.5602?",
    "output": "Playing Atari with Deep Reinforcement Learning"
  },
  {
    "input": "What is the GitHub repository for T5: Exploring the Limits of Transfer Learning?",
    "output": "The code for T5: Exploring the Limits of Transfer Learning is available at https://github.com/google-research/text-to-text-transfer-transformer (google-research/text-to-text-transfer-transformer)."
  },
  {
    "input": "What is the GitHub repository for Masked Autoencoders Are Scalable Vision Learners?",
    "output": "The code for Masked Autoencoders Are Scalable Vision Learners is available at https://github.com/facebookresearch/mae (facebookresearch/mae)."
  },
  {
    "input": "Describe the tensorflow/models repository that implements Wide & Deep Learning for Recommender Systems.",
    "output": "Models and examples built with TensorFlow"
  },
  {
    "input": "Explain the Megvii-BaseDetection/YOLOX implementation of YOLOX: Exceeding YOLO Series in 2021.",
    "output": "YOLOX is a high-performance anchor-free YOLO, exceeding yolov3~v5 with MegEngine, ONNX, TensorRT, ncnn, and OpenVINO supported. Documentation: https://yolox.readthedocs.io/"
  },
  {
    "input": "What category is the PaLM: Scaling Language Modeling with Pathways paper in?",
    "output": "PaLM: Scaling Language Modeling with Pathways is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Describe the HazyResearch/safari repository that implements arXiv Query: search_query=&id_list=2303.03846&start=0&max_results=10.",
    "output": "Convolutions for Sequence Modeling"
  },
  {
    "input": "What is the ArXiv category of AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning?",
    "output": "AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2102.01192&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2102.01192&start=0&max_results=10 is a 2021 paper in Machine Learning. The implementation is available at facebookresearch/fairseq which facebook ai research sequence-to-sequence toolkit written in python. It is written in Python. The repository has 31,988 stars, indicating high community interest."
  },
  {
    "input": "Where can I find the code for PyTorch: An Imperative Style, High-Performance Deep Learning Library?",
    "output": "The code for PyTorch: An Imperative Style, High-Performance Deep Learning Library is available at https://github.com/pytorch/pytorch (pytorch/pytorch)."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2403.17887&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2403.17887&start=0&max_results=10 is a 2024 paper in Machine Learning. The implementation is available at OpenBMB/Eurus It is written in Python. The repository has 320 stars, indicating high community interest."
  },
  {
    "input": "Which field does Graph Attention Networks belong to?",
    "output": "Graph Attention Networks is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Summarize the Vicuna: An Open-Source Chatbot paper.",
    "output": "Vicuna: An Open-Source Chatbot is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at lm-sys/FastChat which an open platform for training, serving, and evaluating large language models. release repo for vicuna and chatbot arena. It is written in Python. The repository has 39,271 stars, indicating high community interest."
  },
  {
    "input": "What does the ruiqi-zhong/D5 repository do?",
    "output": "The GitHub repo for Goal Driven Discovery of Distributional Differences via Language Descriptions"
  },
  {
    "input": "What is the GitHub repository for Deep Residual Learning for Image Recognition?",
    "output": "The code for Deep Residual Learning for Image Recognition is available at https://github.com/KaimingHe/deep-residual-networks (KaimingHe/deep-residual-networks)."
  },
  {
    "input": "Tell me the title of ArXiv paper 2106.05237.",
    "output": "Wav2Vec 2.0: A Framework for Self-Supervised Learning of Speech"
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2204.14198?",
    "output": "Flamingo: A Visual Language Model for Few-Shot Learning"
  },
  {
    "input": "What are the key topics in the tensorflow/tensorflow repository?",
    "output": "The tensorflow/tensorflow repository covers: tensorflow, machine-learning, python, deep-learning, deep-neural-networks, neural-network, ml, distributed."
  },
  {
    "input": "Summarize the DeBERTa: Decoding-enhanced BERT with Disentangled Attention paper.",
    "output": "DeBERTa: Decoding-enhanced BERT with Disentangled Attention is a 2020 paper in Computation and Language (Natural Language Processing). The implementation is available at microsoft/DeBERTa which the implementation of deberta It is written in Python. The repository has 2,169 stars, indicating high community interest."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2502.01061&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2502.01061&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at GAIR-NLP/LIMO which [colm 2025] limo: less is more for reasoning It is written in Python. The repository has 1,052 stars, indicating high community interest."
  },
  {
    "input": "Explain the Training language models to follow instructions with human feedback paper.",
    "output": "Training language models to follow instructions with human feedback is a 2022 paper in Computation and Language (Natural Language Processing). The implementation is available at openai/following-instructions-human-feedback The repository has 1,249 stars, indicating high community interest."
  },
  {
    "input": "Describe the tensorflow/tensor2tensor repository that implements Attention Is All You Need.",
    "output": "Library of deep learning models and datasets designed to make deep learning more accessible and accelerate ML research."
  },
  {
    "input": "What programming language is Question and Answer Test-Train Overlap in Open-Domain QA Datasets implemented in?",
    "output": "The Question and Answer Test-Train Overlap in Open-Domain QA Datasets implementation is written in Python."
  },
  {
    "input": "What is HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction about?",
    "output": "HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction is a 2021 paper in Computation and Language (Natural Language Processing). The implementation is available at facebookresearch/fairseq which facebook ai research sequence-to-sequence toolkit written in python. It is written in Python. The repository has 31,985 stars, indicating high community interest."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2301.13188?",
    "output": "arXiv Query: search_query=&id_list=2301.13188&start=0&max_results=10"
  },
  {
    "input": "What year was Generating Wikipedia by Summarizing Long Sequences released?",
    "output": "The Generating Wikipedia by Summarizing Long Sequences paper was published in 2018."
  },
  {
    "input": "Which field does MetaGPT: Meta Programming for Multi-Agent Systems belong to?",
    "output": "MetaGPT: Meta Programming for Multi-Agent Systems is in the Artificial Intelligence (cs.AI) category."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2302.05206&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2302.05206&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2502.01061&start=0&max_results=10 repository have?",
    "output": "The GAIR-NLP/LIMO repository has 1,052 stars and 52 forks on GitHub."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2504.02732&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2504.02732&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What topics does the TinyBERT: Distilling BERT for Natural Language Understanding implementation cover?",
    "output": "The huawei-noah/Pretrained-Language-Model repository covers: knowledge-distillation, model-compression, quantization, pretrained-models, large-scale-distributed."
  },
  {
    "input": "Explain the facebookresearch/DPR implementation of RAG: Retrieval-Augmented Generation.",
    "output": "Dense Passage Retriever - is a set of tools and models for open domain Q&A task."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2303.06865&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2303.06865&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at FMInference/FlexLLMGen which running large language models on a single gpu for throughput-oriented scenarios. It is written in Python. The repository has 9,379 stars, indicating high community interest."
  },
  {
    "input": "What year was LIMA: Less Is More for Alignment released?",
    "output": "The LIMA: Less Is More for Alignment paper was published in 2023."
  },
  {
    "input": "What paper has ArXiv ID 2302.13007?",
    "output": "Synthetic Data Generation with Large Language Models"
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2301.04104&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2301.04104&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at google-deepmind/tracr It is written in Python. The repository has 548 stars, indicating high community interest."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2504.20734&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2504.20734&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at MoonshotAI/Kimi-Audio which kimi-audio, an open-source audio foundation model excelling in audio understanding, generation, and conversation It is written in Python. The repository has 4,367 stars, indicating high community interest."
  },
  {
    "input": "What programming language is Sequence to Sequence Learning with Neural Networks implemented in?",
    "output": "The Sequence to Sequence Learning with Neural Networks implementation is written in Python."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2006.16779&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2006.16779&start=0&max_results=10 paper was published in 2020."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2301.01181&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2301.01181&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2301.01751&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2301.01751&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What language is the priba/nmp_qc repository written in?",
    "output": "The Neural Message Passing for Quantum Chemistry implementation is written in Python."
  },
  {
    "input": "What does the meta-llama/codellama repository do?",
    "output": "Inference code for CodeLlama models"
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2101.04909&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2101.04909&start=0&max_results=10 is available at https://github.com/facebookresearch/CovidPrognosis (facebookresearch/CovidPrognosis)."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2301.12652&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2301.12652&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2302.04870&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2302.04870&start=0&max_results=10 is available at https://github.com/mit-han-lab/offsite-tuning (mit-han-lab/offsite-tuning)."
  },
  {
    "input": "What category is the Grounding DINO: Marrying DINO with Grounded Pre-Training paper in?",
    "output": "Grounding DINO: Marrying DINO with Grounded Pre-Training is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "What is the GitHub repository for AlpacaEval: An Automatic Evaluator for Instruction-following?",
    "output": "The code for AlpacaEval: An Automatic Evaluator for Instruction-following is available at https://github.com/tatsu-lab/alpaca_eval (tatsu-lab/alpaca_eval)."
  },
  {
    "input": "Explain the Gorilla: Large Language Model Connected with Massive APIs paper.",
    "output": "Gorilla: Large Language Model Connected with Massive APIs is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at ShishirPatil/gorilla which gorilla: training and evaluating llms for function calls (tool calls) It is written in Python. The repository has 12,574 stars, indicating high community interest."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2502.03387&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2502.03387&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "Describe the tatp22/linformer-pytorch repository that implements Linformer: Self-Attention with Linear Complexity.",
    "output": "My take on a practical implementation of Linformer for Pytorch."
  },
  {
    "input": "What paper has ArXiv ID 2301.01947?",
    "output": "arXiv Query: search_query=&id_list=2301.01947&start=0&max_results=10"
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2302.05206&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2302.05206&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at tianjunz/HIR It is written in Python. The repository has 159 stars, indicating high community interest."
  },
  {
    "input": "Describe the liuzhuang13/DenseNet repository that implements Densely Connected Convolutional Networks.",
    "output": "Densely Connected Convolutional Networks, In CVPR 2017 (Best Paper Award)."
  },
  {
    "input": "What paper has ArXiv ID 2301.00303?",
    "output": "arXiv Query: search_query=&id_list=2301.00303&start=0&max_results=10"
  },
  {
    "input": "What does the google-research/text-to-text-transfer-transformer repository do?",
    "output": "Code for the paper \"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\""
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2502.09992&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2502.09992&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at Open-Reasoner-Zero/Open-Reasoner-Zero which official repo for open-reasoner-zero It is written in Python. The repository has 2,068 stars, indicating high community interest."
  },
  {
    "input": "How popular is the Efficient Estimation of Word Representations in Vector Space implementation?",
    "output": "The tmikolov/word2vec repository has 1,569 stars and 546 forks on GitHub."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2302.04870&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2302.04870&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at mit-han-lab/offsite-tuning which offsite-tuning: transfer learning without full model It is written in Python. The repository has 382 stars, indicating high community interest."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2301.01751&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2301.01751&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "Explain the google/automl implementation of EfficientDet: Scalable and Efficient Object Detection.",
    "output": "Google Brain AutoML"
  },
  {
    "input": "What language is the langchain-ai/langchain repository written in?",
    "output": "The MRKL Systems: A modular approach to AGI implementation is written in Python."
  },
  {
    "input": "In what year did T5: Exploring the Limits of Transfer Learning come out?",
    "output": "The T5: Exploring the Limits of Transfer Learning paper was published in 2019."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2501.13824&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2501.13824&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at plurai-ai/intellagent which a framework for comprehensive diagnosis and optimization of agents using simulated, realistic synthetic interactions It is written in Python. The repository has 1,148 stars, indicating high community interest."
  },
  {
    "input": "How popular is the Neural Collaborative Filtering implementation?",
    "output": "The hexiangnan/neural_collaborative_filtering repository has 1,867 stars and 666 forks on GitHub."
  },
  {
    "input": "What are the key topics in the tatp22/linformer-pytorch repository?",
    "output": "The tatp22/linformer-pytorch repository covers: artificial-intelligence, deep-learning, attention-mechanism, pytorch, machine-learning, linformer, paper."
  },
  {
    "input": "Where can I find the code for HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction?",
    "output": "The code for HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction is available at https://github.com/facebookresearch/fairseq (facebookresearch/fairseq)."
  },
  {
    "input": "What is the ArXiv category of Training language models to follow instructions with human feedback?",
    "output": "Training language models to follow instructions with human feedback is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What year was AudioPaLM: A Large Language Model for Speech Understanding released?",
    "output": "The AudioPaLM: A Large Language Model for Speech Understanding paper was published in 2023."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2406.12824&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2406.12824&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is Beir: A Heterogeneous Benchmark for IR about?",
    "output": "Beir: A Heterogeneous Benchmark for IR is a 2022 paper in Information Retrieval. The implementation is available at beir-cellar/beir which a heterogeneous benchmark for information retrieval. easy to use, evaluate your models across 15+ diverse ir datasets. It is written in Python. The repository has 2,007 stars, indicating high community interest."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2101.00190?",
    "output": "The Power of Scale for Parameter-Efficient Prompt Tuning"
  },
  {
    "input": "Tell me the title of ArXiv paper 2307.03109.",
    "output": "AlpacaEval: An Automatic Evaluator for Instruction-following"
  },
  {
    "input": "Tell me the title of ArXiv paper 2403.17887.",
    "output": "arXiv Query: search_query=&id_list=2403.17887&start=0&max_results=10"
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2305.18290?",
    "output": "Direct Preference Optimization: Your Language Model is a Reward Model"
  },
  {
    "input": "What paper has ArXiv ID 2101.00190?",
    "output": "The Power of Scale for Parameter-Efficient Prompt Tuning"
  },
  {
    "input": "Tell me the title of ArXiv paper 2303.02913.",
    "output": "arXiv Query: search_query=&id_list=2303.02913&start=0&max_results=10"
  },
  {
    "input": "What is the ArXiv category of Finetuned Language Models Are Zero-Shot Learners?",
    "output": "Finetuned Language Models Are Zero-Shot Learners is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Explain the DeBERTa: Decoding-enhanced BERT with Disentangled Attention paper.",
    "output": "DeBERTa: Decoding-enhanced BERT with Disentangled Attention is a 2020 paper in Computation and Language (Natural Language Processing). The implementation is available at microsoft/DeBERTa which the implementation of deberta It is written in Python. The repository has 2,169 stars, indicating high community interest."
  },
  {
    "input": "Explain the Segment Anything paper.",
    "output": "Segment Anything is a 2023 paper in Computer Vision and Pattern Recognition. The implementation is available at facebookresearch/segment-anything which the repository provides code for running inference with the segmentanything model (sam), links for downloading the trained model checkpoints, and example notebooks that show how to use the model. It is written in Jupyter Notebook. The repository has 52,657 stars, indicating high community interest."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2504.21801&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2504.21801&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2302.12246&start=0&max_results=10 implementation?",
    "output": "The shizhediao/active-prompt repository has 247 stars and 30 forks on GitHub."
  },
  {
    "input": "Explain the lamini-ai/Lamini-Memory-Tuning implementation of arXiv Query: search_query=&id_list=2406.09308&start=0&max_results=10.",
    "output": "Banishing LLM Hallucinations Requires Rethinking Generalization"
  },
  {
    "input": "What language is the google-research/FLAN repository written in?",
    "output": "The Finetuned Language Models Are Zero-Shot Learners implementation is written in Python."
  },
  {
    "input": "Give me an overview of MetaGPT: Meta Programming for Multi-Agent Systems.",
    "output": "MetaGPT: Meta Programming for Multi-Agent Systems is a 2023 paper in Artificial Intelligence. The implementation is available at FoundationAgents/MetaGPT which \ud83c\udf1f the multi-agent framework: first ai software company, towards natural language programming It is written in Python. The repository has 59,669 stars, indicating high community interest."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2503.20201&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2503.20201&start=0&max_results=10 is available at https://github.com/sentient-agi/OpenDeepSearch (sentient-agi/OpenDeepSearch)."
  },
  {
    "input": "What are the key topics in the oughtinc/ice repository?",
    "output": "The oughtinc/ice repository covers: debugging, gpt-3, python, language-model."
  },
  {
    "input": "What language is the AkariAsai/self-rag repository written in?",
    "output": "The Self-RAG: Learning to Retrieve, Generate, and Critique implementation is written in Python."
  },
  {
    "input": "Explain the haoliuhl/language-quantized-autoencoders implementation of arXiv Query: search_query=&id_list=2302.08242&start=0&max_results=10.",
    "output": "Language Quantized AutoEncoders"
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2406.12824&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2406.12824&start=0&max_results=10 paper was published in 2024."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2410.12896&start=0&max_results=10 implementation?",
    "output": "The ibm-granite/granite-3.0-language-models repository has 267 stars and 28 forks on GitHub."
  },
  {
    "input": "In what year did BERT: Pre-training of Deep Bidirectional Transformers come out?",
    "output": "The BERT: Pre-training of Deep Bidirectional Transformers paper was published in 2018."
  },
  {
    "input": "Describe the facebookresearch/CovidPrognosis repository that implements arXiv Query: search_query=&id_list=2012.06009&start=0&max_results=10.",
    "output": "COVID deterioration prediction based on chest X-ray radiographs via MoCo-trained image representations"
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2301.10343&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2301.10343&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at autonomousvision/stylegan-t which [icml'23] stylegan-t: unlocking the power of gans for fast large-scale text-to-image synthesis It is written in Python. The repository has 1,196 stars, indicating high community interest."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2503.01141&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2503.01141&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "How popular is the BLIP: Bootstrapping Language-Image Pre-training implementation?",
    "output": "The salesforce/BLIP repository has 5,583 stars and 758 forks on GitHub."
  },
  {
    "input": "What's the implementation repository for BLIP: Bootstrapping Language-Image Pre-training?",
    "output": "The code for BLIP: Bootstrapping Language-Image Pre-training is available at https://github.com/salesforce/BLIP (salesforce/BLIP)."
  },
  {
    "input": "What is Mistral 7B about?",
    "output": "Mistral 7B is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at mistralai/mistral-inference which official inference library for mistral models It is written in Jupyter Notebook. The repository has 10,550 stars, indicating high community interest."
  },
  {
    "input": "What programming language is Gorilla: Large Language Model Connected with Massive APIs implemented in?",
    "output": "The Gorilla: Large Language Model Connected with Massive APIs implementation is written in Python."
  },
  {
    "input": "How popular is the DreamBooth: Fine Tuning Text-to-Image Diffusion Models implementation?",
    "output": "The google/dreambooth repository has 998 stars and 91 forks on GitHub."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2006.03511&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2006.03511&start=0&max_results=10 is a 2020 paper in Machine Learning. The implementation is available at facebookresearch/TransCoder which public release of the transcoder research project https://arxiv.org/pdf/2006.03511.pdf It is written in Python. The repository has 1,722 stars, indicating high community interest."
  },
  {
    "input": "Explain the RoBERTa: A Robustly Optimized BERT Pretraining Approach paper.",
    "output": "RoBERTa: A Robustly Optimized BERT Pretraining Approach is a 2019 paper in Computation and Language (Natural Language Processing). The implementation is available at facebookresearch/fairseq which facebook ai research sequence-to-sequence toolkit written in python. It is written in Python. The repository has 31,985 stars, indicating high community interest."
  },
  {
    "input": "What language is the rbgirshick/py-faster-rcnn repository written in?",
    "output": "The Faster R-CNN: Towards Real-Time Object Detection implementation is written in Python."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2101.04909&start=0&max_results=10 implementation?",
    "output": "The facebookresearch/CovidPrognosis repository has 158 stars and 38 forks on GitHub."
  },
  {
    "input": "What is PyTorch: An Imperative Style, High-Performance Deep Learning Library about?",
    "output": "PyTorch: An Imperative Style, High-Performance Deep Learning Library is a 2019 paper in Machine Learning. The implementation is available at pytorch/pytorch which tensors and dynamic neural networks in python with strong gpu acceleration It is written in Python. The repository has 95,348 stars, indicating high community interest."
  },
  {
    "input": "What is LLaVA: Large Language and Vision Assistant about?",
    "output": "LLaVA: Large Language and Vision Assistant is a 2023 paper in Computer Vision and Pattern Recognition. The implementation is available at haotian-liu/LLaVA which [neurips'23 oral] visual instruction tuning (llava) built towards gpt-4v level capabilities and beyond. It is written in Python. The repository has 24,034 stars, indicating high community interest."
  },
  {
    "input": "What category is the Masked Autoencoders Are Scalable Vision Learners paper in?",
    "output": "Masked Autoencoders Are Scalable Vision Learners is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "What paper has ArXiv ID 2309.03409?",
    "output": "AudioPaLM: A Large Language Model for Speech Understanding"
  },
  {
    "input": "Which field does Horovod: fast and easy distributed deep learning in TensorFlow belong to?",
    "output": "Horovod: fast and easy distributed deep learning in TensorFlow is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2312.02783&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2312.02783&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at ContextualAI/HALOs which a library with extensible implementations of dpo, kto, ppo, orpo, and other human-aware loss functions (halos). It is written in Python. The repository has 893 stars, indicating high community interest."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2504.00698&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2504.00698&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at allenai/codescientist which codescientist: an automated scientific discovery system for code-based experiments It is written in Python. The repository has 301 stars, indicating high community interest."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2301.04856&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2301.04856&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "In what year did Playing Atari with Deep Reinforcement Learning come out?",
    "output": "The Playing Atari with Deep Reinforcement Learning paper was published in 2013."
  },
  {
    "input": "What paper has ArXiv ID 2302.03917?",
    "output": "arXiv Query: search_query=&id_list=2302.03917&start=0&max_results=10"
  },
  {
    "input": "What is LLaMA-Adapter: Efficient Fine-tuning of Language Models about?",
    "output": "LLaMA-Adapter: Efficient Fine-tuning of Language Models is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at OpenGVLab/LLaMA-Adapter which [iclr 2024] fine-tuning llama to follow instructions within 1 hour and 1.2m parameters It is written in Python. The repository has 5,923 stars, indicating high community interest."
  },
  {
    "input": "How popular is the Horovod: fast and easy distributed deep learning in TensorFlow implementation?",
    "output": "The horovod/horovod repository has 14,631 stars and 2,255 forks on GitHub."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2303.03378&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2303.03378&start=0&max_results=10 is available at https://github.com/NVlabs/prismer (NVlabs/prismer)."
  },
  {
    "input": "What is the GitHub repository for FlashAttention: Fast and Memory-Efficient Exact Attention?",
    "output": "The code for FlashAttention: Fast and Memory-Efficient Exact Attention is available at https://github.com/Dao-AILab/flash-attention (Dao-AILab/flash-attention)."
  },
  {
    "input": "What paper has ArXiv ID 2212.04356?",
    "output": "Whisper: Robust Speech Recognition via Large-Scale Weak Supervision"
  },
  {
    "input": "Where can I find the code for XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale?",
    "output": "The code for XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale is available at https://github.com/facebookresearch/fairseq (facebookresearch/fairseq)."
  },
  {
    "input": "What does the autonomousvision/stylegan-t repository do?",
    "output": "[ICML'23] StyleGAN-T: Unlocking the Power of GANs for Fast Large-Scale Text-to-Image Synthesis"
  },
  {
    "input": "Describe the amazon-science/mm-cot repository that implements arXiv Query: search_query=&id_list=2301.13188&start=0&max_results=10.",
    "output": "Official implementation for \"Multimodal Chain-of-Thought Reasoning in Language Models\" (stay tuned and more will be updated)"
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2302.00923&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2302.00923&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at amazon-science/mm-cot which official implementation for \"multimodal chain-of-thought reasoning in language models\" (stay tuned and more will be updated) It is written in Python. The repository has 3,982 stars, indicating high community interest."
  },
  {
    "input": "What does the haotian-liu/LLaVA repository do?",
    "output": "[NeurIPS'23 Oral] Visual Instruction Tuning (LLaVA) built towards GPT-4V level capabilities and beyond."
  },
  {
    "input": "What programming language is Generating Wikipedia by Summarizing Long Sequences implemented in?",
    "output": "The Generating Wikipedia by Summarizing Long Sequences implementation is written in Python."
  },
  {
    "input": "How popular is the TensorFlow: A System for Large-Scale Machine Learning implementation?",
    "output": "The tensorflow/tensorflow repository has 192,543 stars and 74,998 forks on GitHub."
  },
  {
    "input": "How many stars does the DeBERTa: Decoding-enhanced BERT with Disentangled Attention repository have?",
    "output": "The microsoft/DeBERTa repository has 2,169 stars and 238 forks on GitHub."
  },
  {
    "input": "What category is the TinyBERT: Distilling BERT for Natural Language Understanding paper in?",
    "output": "TinyBERT: Distilling BERT for Natural Language Understanding is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What topics does the SQuAD: 100,000+ Questions for Machine Comprehension of Text implementation cover?",
    "output": "The rajpurkar/SQuAD-explorer repository covers: dataset, visual-analysis, leaderboard."
  },
  {
    "input": "What's the implementation repository for JAX: Composable transformations of Python+NumPy programs?",
    "output": "The code for JAX: Composable transformations of Python+NumPy programs is available at https://github.com/jax-ml/jax (jax-ml/jax)."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2301.05062&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2301.05062&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Give me an overview of Horovod: fast and easy distributed deep learning in TensorFlow.",
    "output": "Horovod: fast and easy distributed deep learning in TensorFlow is a 2017 paper in Machine Learning. The implementation is available at horovod/horovod which distributed training framework for tensorflow, keras, pytorch, and apache mxnet. It is written in Python. The repository has 14,631 stars, indicating high community interest."
  },
  {
    "input": "Explain the Time-LLM: Time Series Forecasting by Reprogramming Large Language Models paper.",
    "output": "Time-LLM: Time Series Forecasting by Reprogramming Large Language Models is a 2023 paper in Machine Learning. The implementation is available at KimMeen/Time-LLM which [iclr 2024] official implementation of \" \ud83e\udd99 time-llm: time series forecasting by reprogramming large language models\" It is written in Python. The repository has 2,377 stars, indicating high community interest."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2004.13637&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2004.13637&start=0&max_results=10 is a 2020 paper in Machine Learning. The implementation is available at facebookresearch/TransCoder which public release of the transcoder research project https://arxiv.org/pdf/2006.03511.pdf It is written in Python. The repository has 1,722 stars, indicating high community interest."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2303.03846&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2303.03846&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What paper has ArXiv ID 1904.12848?",
    "output": "EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification"
  },
  {
    "input": "What topics does the SimCSE: Simple Contrastive Learning of Sentence Embeddings implementation cover?",
    "output": "The princeton-nlp/SimCSE repository covers: nlp, sentence-embeddings."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2004.13637&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2004.13637&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Describe the zihangdai/xlnet repository that implements XLNet: Generalized Autoregressive Pretraining.",
    "output": "XLNet: Generalized Autoregressive Pretraining for Language Understanding"
  },
  {
    "input": "What does the lamini-ai/Lamini-Memory-Tuning repository do?",
    "output": "Banishing LLM Hallucinations Requires Rethinking Generalization"
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2302.14838&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2302.14838&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at ruiqi-zhong/D5 which the github repo for goal driven discovery of distributional differences via language descriptions It is written in Python. The repository has 71 stars, indicating high community interest."
  },
  {
    "input": "Explain the Lookahead Optimizer: k steps forward, 1 step back paper.",
    "output": "Lookahead Optimizer: k steps forward, 1 step back is a 2020 paper in Machine Learning. The implementation is available at alphadl/lookahead.pytorch which lookahead optimizer (lookahead optimizer: k steps forward, 1 step back) for pytorch  It is written in Python. The repository has 337 stars, indicating high community interest."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2009.06367&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2009.06367&start=0&max_results=10 is a 2020 paper in Machine Learning. The implementation is available at salesforce/GeDi which gedi: generative discriminator guided sequence generation It is written in Python. The repository has 209 stars, indicating high community interest."
  },
  {
    "input": "How many stars does the Reformer: The Efficient Transformer repository have?",
    "output": "The google/trax repository has 8,293 stars and 828 forks on GitHub."
  },
  {
    "input": "Tell me the title of ArXiv paper 2205.05198.",
    "output": "PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel"
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2405.03548&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2405.03548&start=0&max_results=10 is a 2024 paper in Machine Learning. The implementation is available at ibm-granite/granite-code-models which granite code models: a family of open foundation models for code intelligence The repository has 1,240 stars, indicating high community interest."
  },
  {
    "input": "What paper has ArXiv ID 2303.04671?",
    "output": "arXiv Query: search_query=&id_list=2303.04671&start=0&max_results=10"
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2302.05206&start=0&max_results=10 implementation?",
    "output": "The tianjunz/HIR repository has 159 stars and 11 forks on GitHub."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2302.03011&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2302.03011&start=0&max_results=10 is available at https://github.com/mit-han-lab/offsite-tuning (mit-han-lab/offsite-tuning)."
  },
  {
    "input": "What is the ArXiv category of Zephyr: Direct Distillation of LM Alignment?",
    "output": "Zephyr: Direct Distillation of LM Alignment is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What language is the FoundationAgents/MetaGPT repository written in?",
    "output": "The MetaGPT: Meta Programming for Multi-Agent Systems implementation is written in Python."
  },
  {
    "input": "What paper has ArXiv ID 2010.02502?",
    "output": "Denoising Diffusion Implicit Models"
  },
  {
    "input": "Where can I find the code for ReAct: Synergizing Reasoning and Acting in Language Models?",
    "output": "The code for ReAct: Synergizing Reasoning and Acting in Language Models is available at https://github.com/ysymyth/ReAct (ysymyth/ReAct)."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2301.02828&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2301.02828&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is Gorilla: Large Language Model Connected with Massive APIs about?",
    "output": "Gorilla: Large Language Model Connected with Massive APIs is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at ShishirPatil/gorilla which gorilla: training and evaluating llms for function calls (tool calls) It is written in Python. The repository has 12,574 stars, indicating high community interest."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2304.15004?",
    "output": "Vicuna: An Open-Source Chatbot"
  },
  {
    "input": "What is the title of the paper with ArXiv ID 1910.10683?",
    "output": "T5: Exploring the Limits of Transfer Learning"
  },
  {
    "input": "Explain the ibm-granite/granite-code-models implementation of arXiv Query: search_query=&id_list=2405.04324&start=0&max_results=10.",
    "output": "Granite Code Models: A Family of Open Foundation Models for Code Intelligence"
  },
  {
    "input": "In what year did RetNet: Retentive Network: A Successor to Transformer come out?",
    "output": "The RetNet: Retentive Network: A Successor to Transformer paper was published in 2022."
  },
  {
    "input": "What paper has ArXiv ID 2109.04908?",
    "output": "XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale"
  },
  {
    "input": "Describe the hpcaitech/Open-Sora repository that implements arXiv Query: search_query=&id_list=2406.10209&start=0&max_results=10.",
    "output": "Open-Sora: Democratizing Efficient Video Production for All"
  },
  {
    "input": "What does the facebookresearch/ConvNeXt-V2 repository do?",
    "output": "Code release for ConvNeXt V2 model"
  },
  {
    "input": "What year was BEiT: BERT Pre-Training of Image Transformers released?",
    "output": "The BEiT: BERT Pre-Training of Image Transformers paper was published in 2021."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2302.08242&start=0&max_results=10 implementation?",
    "output": "The haoliuhl/language-quantized-autoencoders repository has 111 stars and 5 forks on GitHub."
  },
  {
    "input": "What topics does the AudioPaLM: A Large Language Model for Speech Understanding implementation cover?",
    "output": "The lucidrains/audiolm-pytorch repository covers: artificial-intelligence, attention-mechanisms, audio-synthesis, deep-learning, transformers."
  },
  {
    "input": "When was the Decoupled Weight Decay Regularization paper published?",
    "output": "The Decoupled Weight Decay Regularization paper was published in 2017."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2406.06326&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2406.06326&start=0&max_results=10 is available at https://github.com/lamini-ai/Lamini-Memory-Tuning (lamini-ai/Lamini-Memory-Tuning)."
  },
  {
    "input": "What paper has ArXiv ID 2209.00796?",
    "output": "DreamFusion: Text-to-3D using 2D Diffusion"
  },
  {
    "input": "Describe the pytorch/pytorch repository that implements Adam: A Method for Stochastic Optimization.",
    "output": "Tensors and Dynamic neural networks in Python with strong GPU acceleration"
  },
  {
    "input": "What paper has ArXiv ID 2112.11446?",
    "output": "FLAN: Finetuned Language Models Are Zero-Shot Learners"
  },
  {
    "input": "In what year did DreamFusion: Text-to-3D using 2D Diffusion come out?",
    "output": "The DreamFusion: Text-to-3D using 2D Diffusion paper was published in 2022."
  },
  {
    "input": "Where can I find the code for Segment Anything?",
    "output": "The code for Segment Anything is available at https://github.com/facebookresearch/segment-anything (facebookresearch/segment-anything)."
  },
  {
    "input": "Explain the microsoft/Swin-Transformer implementation of Swin Transformer: Hierarchical Vision Transformer using Shifted Windows.",
    "output": "This is an official implementation for \"Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\"."
  },
  {
    "input": "Explain the Billion-scale similarity search with GPUs paper.",
    "output": "Billion-scale similarity search with GPUs is a 2019 paper in Computer Vision and Pattern Recognition. The implementation is available at facebookresearch/faiss which a library for efficient similarity search and clustering of dense vectors. It is written in C++. The repository has 38,143 stars, indicating high community interest."
  },
  {
    "input": "Which field does MMLU: Measuring Massive Multitask Language Understanding belong to?",
    "output": "MMLU: Measuring Massive Multitask Language Understanding is in the cs.CY (cs.CY) category."
  },
  {
    "input": "What is the GitHub repository for YOLOX: Exceeding YOLO Series in 2021?",
    "output": "The code for YOLOX: Exceeding YOLO Series in 2021 is available at https://github.com/Megvii-BaseDetection/YOLOX (Megvii-BaseDetection/YOLOX)."
  },
  {
    "input": "How popular is the Mistral 7B implementation?",
    "output": "The mistralai/mistral-inference repository has 10,550 stars and 990 forks on GitHub."
  },
  {
    "input": "Which field does An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale belong to?",
    "output": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "What's the implementation repository for Finetuned Language Models Are Zero-Shot Learners?",
    "output": "The code for Finetuned Language Models Are Zero-Shot Learners is available at https://github.com/google-research/FLAN (google-research/FLAN)."
  },
  {
    "input": "In what year did Direct Preference Optimization: Your Language Model is a Reward Model come out?",
    "output": "The Direct Preference Optimization: Your Language Model is a Reward Model paper was published in 2023."
  },
  {
    "input": "Where can I find the code for Beir: A Heterogeneous Benchmark for IR?",
    "output": "The code for Beir: A Heterogeneous Benchmark for IR is available at https://github.com/beir-cellar/beir (beir-cellar/beir)."
  },
  {
    "input": "What is Llama 2: Open Foundation and Fine-Tuned Chat Models about?",
    "output": "Llama 2: Open Foundation and Fine-Tuned Chat Models is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at meta-llama/llama which inference code for llama models It is written in Python. The repository has 58,946 stars, indicating high community interest."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2301.12652&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2301.12652&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at amazon-science/mm-cot which official implementation for \"multimodal chain-of-thought reasoning in language models\" (stay tuned and more will be updated) It is written in Python. The repository has 3,982 stars, indicating high community interest."
  },
  {
    "input": "Tell me the title of ArXiv paper 2212.10496.",
    "output": "Self-RAG: Learning to Retrieve, Generate, and Critique"
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2004.13637&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2004.13637&start=0&max_results=10 is a 2020 paper in Machine Learning. The implementation is available at facebookresearch/TransCoder which public release of the transcoder research project https://arxiv.org/pdf/2006.03511.pdf It is written in Python. The repository has 1,722 stars, indicating high community interest."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2504.00698&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2504.00698&start=0&max_results=10 is available at https://github.com/allenai/codescientist (allenai/codescientist)."
  },
  {
    "input": "Explain the Question and Answer Test-Train Overlap in Open-Domain QA Datasets paper.",
    "output": "Question and Answer Test-Train Overlap in Open-Domain QA Datasets is a 2020 paper in Computation and Language (Natural Language Processing). The implementation is available at google-research-datasets/natural-questions which natural questions (nq) contains real user questions issued to google search, and answers found from wikipedia by annotators. nq is designed for the training and evaluation of automatic question answering systems. It is written in Python. The repository has 1,074 stars, indicating high community interest."
  },
  {
    "input": "Explain the amazon-science/mm-cot implementation of arXiv Query: search_query=&id_list=2301.12652&start=0&max_results=10.",
    "output": "Official implementation for \"Multimodal Chain-of-Thought Reasoning in Language Models\" (stay tuned and more will be updated)"
  },
  {
    "input": "How many stars does the Inductive Representation Learning on Large Graphs repository have?",
    "output": "The williamleif/GraphSAGE repository has 3,630 stars and 849 forks on GitHub."
  },
  {
    "input": "What's the implementation repository for Grounding DINO: Marrying DINO with Grounded Pre-Training?",
    "output": "The code for Grounding DINO: Marrying DINO with Grounded Pre-Training is available at https://github.com/IDEA-Research/GroundingDINO (IDEA-Research/GroundingDINO)."
  },
  {
    "input": "Explain the frankxu2004/knnlm-why implementation of arXiv Query: search_query=&id_list=2301.02828&start=0&max_results=10.",
    "output": "Repo for ICML23 \"Why do Nearest Neighbor Language Models Work?\""
  },
  {
    "input": "What programming language is MRKL Systems: A modular approach to AGI implemented in?",
    "output": "The MRKL Systems: A modular approach to AGI implementation is written in Python."
  },
  {
    "input": "When was the Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning paper published?",
    "output": "The Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning paper was published in 2018."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2301.04104&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2301.04104&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2306.05685?",
    "output": "Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena"
  },
  {
    "input": "What paper has ArXiv ID 2504.19413?",
    "output": "arXiv Query: search_query=&id_list=2504.19413&start=0&max_results=10"
  },
  {
    "input": "How popular is the The Flan Collection: Designing Data and Methods for Effective Instruction Tuning implementation?",
    "output": "The google-research/FLAN repository has 1,554 stars and 161 forks on GitHub."
  },
  {
    "input": "What is Codex: Evaluating Large Language Models Trained on Code about?",
    "output": "Codex: Evaluating Large Language Models Trained on Code is a 2021 paper in Machine Learning. The implementation is available at openai/human-eval which code for the paper \"evaluating large language models trained on code\" It is written in Python. The repository has 3,030 stars, indicating high community interest."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2303.03846&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2303.03846&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at HazyResearch/safari which convolutions for sequence modeling It is written in Assembly. The repository has 904 stars, indicating high community interest."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2504.00698&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2504.00698&start=0&max_results=10 is available at https://github.com/allenai/codescientist (allenai/codescientist)."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2301.11305&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2301.11305&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Tell me the title of ArXiv paper 2305.18290.",
    "output": "Direct Preference Optimization: Your Language Model is a Reward Model"
  },
  {
    "input": "What programming language is Playing Atari with Deep Reinforcement Learning implemented in?",
    "output": "The Playing Atari with Deep Reinforcement Learning implementation is written in Lua."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2303.01469&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2303.01469&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2301.00774&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2301.00774&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2007.16122&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2007.16122&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Explain the ALBERT: A Lite BERT paper.",
    "output": "ALBERT: A Lite BERT is a 2019 paper in Computation and Language (Natural Language Processing). The implementation is available at google-research/albert which albert: a lite bert for self-supervised learning of language representations It is written in Python. The repository has 3,274 stars, indicating high community interest."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2303.06865&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2303.06865&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What language is the LTH14/fractalgen repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2503.00735&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What programming language is High-Resolution Image Synthesis with Latent Diffusion Models implemented in?",
    "output": "The High-Resolution Image Synthesis with Latent Diffusion Models implementation is written in Jupyter Notebook."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2501.19393&start=0&max_results=10 repository have?",
    "output": "The simplescaling/s1 repository has 6,605 stars and 763 forks on GitHub."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2004.08492&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2004.08492&start=0&max_results=10 is a 2020 paper in Machine Learning. The implementation is available at uber/orbit which a python package for bayesian forecasting with object-oriented design and probabilistic models under the hood. It is written in Python. The repository has 2,019 stars, indicating high community interest."
  },
  {
    "input": "What paper has ArXiv ID 2302.03011?",
    "output": "arXiv Query: search_query=&id_list=2302.03011&start=0&max_results=10"
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2502.14815&start=0&max_results=10 repository have?",
    "output": "The Open-Reasoner-Zero/Open-Reasoner-Zero repository has 2,068 stars and 119 forks on GitHub."
  },
  {
    "input": "What year was Neural Machine Translation by Jointly Learning to Align and Translate released?",
    "output": "The Neural Machine Translation by Jointly Learning to Align and Translate paper was published in 2015."
  },
  {
    "input": "Which field does PyTorch Geometric: Library for Geometric Deep Learning belong to?",
    "output": "PyTorch Geometric: Library for Geometric Deep Learning is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Explain the stanford-futuredata/ColBERT implementation of ColBERT: Efficient Passage Search via Contextualized Late Interaction.",
    "output": "ColBERT: state-of-the-art neural search (SIGIR'20, TACL'21, NeurIPS'21, NAACL'22, CIKM'22, ACL'23, EMNLP'23)"
  },
  {
    "input": "What does the liuzhuang13/DenseNet repository do?",
    "output": "Densely Connected Convolutional Networks, In CVPR 2017 (Best Paper Award)."
  },
  {
    "input": "What year was MiniGPT-4: Enhancing Vision-Language Understanding released?",
    "output": "The MiniGPT-4: Enhancing Vision-Language Understanding paper was published in 2023."
  },
  {
    "input": "Explain the facebookresearch/ConvNeXt-V2 implementation of arXiv Query: search_query=&id_list=2301.00303&start=0&max_results=10.",
    "output": "Code release for ConvNeXt V2 model"
  },
  {
    "input": "What is the title of the paper with ArXiv ID 1906.08237?",
    "output": "XLNet: Generalized Autoregressive Pretraining"
  },
  {
    "input": "What is the GitHub repository for LoRA: Low-Rank Adaptation of Large Language Models?",
    "output": "The code for LoRA: Low-Rank Adaptation of Large Language Models is available at https://github.com/microsoft/LoRA (microsoft/LoRA)."
  },
  {
    "input": "Describe the facebookresearch/fairseq repository that implements HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction.",
    "output": "Facebook AI Research Sequence-to-Sequence Toolkit written in Python."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2301.13188&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2301.13188&start=0&max_results=10 is available at https://github.com/amazon-science/mm-cot (amazon-science/mm-cot)."
  },
  {
    "input": "Tell me the title of ArXiv paper 2406.09403.",
    "output": "arXiv Query: search_query=&id_list=2406.09403&start=0&max_results=10"
  },
  {
    "input": "What category is the BLIP: Bootstrapping Language-Image Pre-training paper in?",
    "output": "BLIP: Bootstrapping Language-Image Pre-training is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2303.03846&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2303.03846&start=0&max_results=10 is available at https://github.com/HazyResearch/safari (HazyResearch/safari)."
  },
  {
    "input": "What's the implementation repository for ELECTRA: Pre-training Text Encoders as Discriminators?",
    "output": "The code for ELECTRA: Pre-training Text Encoders as Discriminators is available at https://github.com/google-research/electra (google-research/electra)."
  },
  {
    "input": "Describe the huggingface/alignment-handbook repository that implements Zephyr: Direct Distillation of LM Alignment.",
    "output": "Robust recipes to align language models with human and AI preferences"
  },
  {
    "input": "What does the rbgirshick/py-faster-rcnn repository do?",
    "output": "Faster R-CNN (Python implementation) -- see https://github.com/ShaoqingRen/faster_rcnn for the official MATLAB version"
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2301.11305&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2301.11305&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at autonomousvision/stylegan-t which [icml'23] stylegan-t: unlocking the power of gans for fast large-scale text-to-image synthesis It is written in Python. The repository has 1,196 stars, indicating high community interest."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2301.13688&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2301.13688&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at amazon-science/mm-cot which official implementation for \"multimodal chain-of-thought reasoning in language models\" (stay tuned and more will be updated) It is written in Python. The repository has 3,982 stars, indicating high community interest."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2303.01469&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2303.01469&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2501.13545&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2501.13545&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at plurai-ai/intellagent which a framework for comprehensive diagnosis and optimization of agents using simulated, realistic synthetic interactions It is written in Python. The repository has 1,148 stars, indicating high community interest."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2301.01751?",
    "output": "arXiv Query: search_query=&id_list=2301.01751&start=0&max_results=10"
  },
  {
    "input": "When was the Self-RAG: Learning to Retrieve, Generate, and Critique paper published?",
    "output": "The Self-RAG: Learning to Retrieve, Generate, and Critique paper was published in 2022."
  },
  {
    "input": "Summarize the BLIP: Bootstrapping Language-Image Pre-training paper.",
    "output": "BLIP: Bootstrapping Language-Image Pre-training is a 2022 paper in Computer Vision and Pattern Recognition. The implementation is available at salesforce/BLIP which pytorch code for blip: bootstrapping language-image pre-training for unified vision-language understanding and generation   It is written in Jupyter Notebook. The repository has 5,583 stars, indicating high community interest."
  },
  {
    "input": "What language is the CompVis/latent-diffusion repository written in?",
    "output": "The High-Resolution Image Synthesis with Latent Diffusion Models implementation is written in Jupyter Notebook."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2502.14815&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2502.14815&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "Tell me the title of ArXiv paper 2303.04129.",
    "output": "arXiv Query: search_query=&id_list=2303.04129&start=0&max_results=10"
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2503.22708&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2503.22708&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "What does the facebookresearch/TransCoder repository do?",
    "output": "Public release of the TransCoder research project https://arxiv.org/pdf/2006.03511.pdf"
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2501.19393&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2501.19393&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "Where can I find the code for Swin Transformer: Hierarchical Vision Transformer using Shifted Windows?",
    "output": "The code for Swin Transformer: Hierarchical Vision Transformer using Shifted Windows is available at https://github.com/microsoft/Swin-Transformer (microsoft/Swin-Transformer)."
  },
  {
    "input": "Give me an overview of Inductive Representation Learning on Large Graphs.",
    "output": "Inductive Representation Learning on Large Graphs is a 2017 paper in cs.SI. The implementation is available at williamleif/GraphSAGE which representation learning on large graphs using stochastic graph convolutions. It is written in Python. The repository has 3,630 stars, indicating high community interest."
  },
  {
    "input": "What paper has ArXiv ID 2301.12652?",
    "output": "arXiv Query: search_query=&id_list=2301.12652&start=0&max_results=10"
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2502.14815?",
    "output": "arXiv Query: search_query=&id_list=2502.14815&start=0&max_results=10"
  },
  {
    "input": "What year was MRKL Systems: A modular approach to AGI released?",
    "output": "The MRKL Systems: A modular approach to AGI paper was published in 2022."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2004.13637&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2004.13637&start=0&max_results=10 is a 2020 paper in Machine Learning. The implementation is available at facebookresearch/TransCoder which public release of the transcoder research project https://arxiv.org/pdf/2006.03511.pdf It is written in Python. The repository has 1,722 stars, indicating high community interest."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2303.06865&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2303.06865&start=0&max_results=10 is available at https://github.com/FMInference/FlexLLMGen (FMInference/FlexLLMGen)."
  },
  {
    "input": "Describe the google-research/google-research repository that implements Depthwise Separable Convolutions for Neural Machine Translation.",
    "output": "Google Research"
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2303.02913&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2303.02913&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2305.10425?",
    "output": "Time-LLM: Time Series Forecasting by Reprogramming Large Language Models"
  },
  {
    "input": "In what year did RoFormer: Enhanced Transformer with Rotary Position Embedding come out?",
    "output": "The RoFormer: Enhanced Transformer with Rotary Position Embedding paper was published in 2021."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2012.06009&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2012.06009&start=0&max_results=10 is a 2020 paper in Machine Learning. The implementation is available at facebookresearch/CovidPrognosis which covid deterioration prediction based on chest x-ray radiographs via moco-trained image representations It is written in Python. The repository has 158 stars, indicating high community interest."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2301.12652&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2301.12652&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at amazon-science/mm-cot which official implementation for \"multimodal chain-of-thought reasoning in language models\" (stay tuned and more will be updated) It is written in Python. The repository has 3,982 stars, indicating high community interest."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2501.13545&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2501.13545&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2301.04856?",
    "output": "arXiv Query: search_query=&id_list=2301.04856&start=0&max_results=10"
  },
  {
    "input": "What language is the huggingface/sentence-transformers repository written in?",
    "output": "The Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks implementation is written in Python."
  },
  {
    "input": "What does the meta-llama/llama repository do?",
    "output": "Inference code for Llama models"
  },
  {
    "input": "What paper has ArXiv ID 2005.12872?",
    "output": "EfficientDet: Scalable and Efficient Object Detection"
  },
  {
    "input": "Which field does Billion-scale similarity search with GPUs belong to?",
    "output": "Billion-scale similarity search with GPUs is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "What does the Shark-NLP/OpenICL repository do?",
    "output": "OpenICL is an open-source framework to facilitate research, development, and prototyping of in-context learning."
  },
  {
    "input": "Describe the namisan/mt-dnn repository that implements Multi-Task Deep Neural Networks for Natural Language Understanding.",
    "output": "Multi-Task Deep Neural Networks for Natural Language Understanding"
  },
  {
    "input": "Where can I find the code for BEiT: BERT Pre-Training of Image Transformers?",
    "output": "The code for BEiT: BERT Pre-Training of Image Transformers is available at https://github.com/microsoft/unilm (microsoft/unilm)."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2302.00902&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2302.00902&start=0&max_results=10 is available at https://github.com/haoliuhl/language-quantized-autoencoders (haoliuhl/language-quantized-autoencoders)."
  },
  {
    "input": "Where can I find the code for RetNet: Retentive Network: A Successor to Transformer?",
    "output": "The code for RetNet: Retentive Network: A Successor to Transformer is available at https://github.com/microsoft/torchscale (microsoft/torchscale)."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2301.00704&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2301.00704&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2302.04023&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2302.04023&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at mit-han-lab/offsite-tuning which offsite-tuning: transfer learning without full model It is written in Python. The repository has 382 stars, indicating high community interest."
  },
  {
    "input": "What is SmoothQuant: Accurate and Efficient Post-Training Quantization about?",
    "output": "SmoothQuant: Accurate and Efficient Post-Training Quantization is a 2022 paper in Machine Learning. The implementation is available at mit-han-lab/smoothquant which [icml 2023] smoothquant: accurate and efficient post-training quantization for large language models It is written in Python. The repository has 1,561 stars, indicating high community interest."
  },
  {
    "input": "What year was Whisper: Robust Speech Recognition via Large-Scale Weak Supervision released?",
    "output": "The Whisper: Robust Speech Recognition via Large-Scale Weak Supervision paper was published in 2022."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2303.03378&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2303.03378&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Explain the Playing Atari with Deep Reinforcement Learning paper.",
    "output": "Playing Atari with Deep Reinforcement Learning is a 2013 paper in Machine Learning. The implementation is available at google-deepmind/dqn which lua/torch implementation of dqn (nature, 2015) It is written in Lua. The repository has 618 stars, indicating high community interest."
  },
  {
    "input": "What does the oughtinc/ice repository do?",
    "output": "Interactive Composition Explorer: a debugger for compositional language model programs"
  },
  {
    "input": "What does the FMInference/FlexLLMGen repository do?",
    "output": "Running large language models on a single GPU for throughput-oriented scenarios."
  },
  {
    "input": "What is the ArXiv category of Flamingo: A Visual Language Model for Few-Shot Learning?",
    "output": "Flamingo: A Visual Language Model for Few-Shot Learning is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "What language is the ashawkey/stable-dreamfusion repository written in?",
    "output": "The DreamFusion: Text-to-3D using 2D Diffusion implementation is written in Python."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2504.16084?",
    "output": "arXiv Query: search_query=&id_list=2504.16084&start=0&max_results=10"
  },
  {
    "input": "How many stars does the Med-PaLM 2: Towards Expert-Level Medical Question Answering repository have?",
    "output": "The google-research/google-research repository has 36,800 stars and 8,255 forks on GitHub."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2104.00355&start=0&max_results=10 implementation?",
    "output": "The facebookresearch/fairseq repository has 31,988 stars and 6,630 forks on GitHub."
  },
  {
    "input": "What year was BLIP-2: Bootstrapping Language-Image Pre-training released?",
    "output": "The BLIP-2: Bootstrapping Language-Image Pre-training paper was published in 2023."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2012.06009&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2012.06009&start=0&max_results=10 is a 2020 paper in Machine Learning. The implementation is available at facebookresearch/CovidPrognosis which covid deterioration prediction based on chest x-ray radiographs via moco-trained image representations It is written in Python. The repository has 158 stars, indicating high community interest."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2301.13188&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2301.13188&start=0&max_results=10 is available at https://github.com/amazon-science/mm-cot (amazon-science/mm-cot)."
  },
  {
    "input": "Which field does Question and Answer Test-Train Overlap in Open-Domain QA Datasets belong to?",
    "output": "Question and Answer Test-Train Overlap in Open-Domain QA Datasets is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "In what year did Neural Collaborative Filtering come out?",
    "output": "The Neural Collaborative Filtering paper was published in 2017."
  },
  {
    "input": "What programming language is Neural Collaborative Filtering implemented in?",
    "output": "The Neural Collaborative Filtering implementation is written in Python."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2302.04870&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2302.04870&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What does the jax-ml/jax repository do?",
    "output": "Composable transformations of Python+NumPy programs: differentiate, vectorize, JIT to GPU/TPU, and more"
  },
  {
    "input": "Tell me the title of ArXiv paper 1603.04467.",
    "output": "TensorFlow: A System for Large-Scale Machine Learning"
  },
  {
    "input": "Which field does Segment Anything in Medical Images belong to?",
    "output": "Segment Anything in Medical Images is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "Tell me the title of ArXiv paper 2301.11305.",
    "output": "arXiv Query: search_query=&id_list=2301.11305&start=0&max_results=10"
  },
  {
    "input": "In what year did An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale come out?",
    "output": "The An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale paper was published in 2020."
  },
  {
    "input": "Describe the autonomousvision/stylegan-t repository that implements arXiv Query: search_query=&id_list=2301.10343&start=0&max_results=10.",
    "output": "[ICML'23] StyleGAN-T: Unlocking the Power of GANs for Fast Large-Scale Text-to-Image Synthesis"
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2301.00303&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2301.00303&start=0&max_results=10 paper was published in 2022."
  },
  {
    "input": "What is Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena about?",
    "output": "Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at lm-sys/FastChat which an open platform for training, serving, and evaluating large language models. release repo for vicuna and chatbot arena. It is written in Python. The repository has 39,271 stars, indicating high community interest."
  },
  {
    "input": "How popular is the Know What You Don't Know: Unanswerable Questions for SQuAD implementation?",
    "output": "The rajpurkar/SQuAD-explorer repository has 570 stars and 121 forks on GitHub."
  },
  {
    "input": "What is GPT-4 Technical Report about?",
    "output": "GPT-4 Technical Report is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at openai/evals which evals is a framework for evaluating llms and llm systems, and an open-source registry of benchmarks. It is written in Python. The repository has 17,349 stars, indicating high community interest."
  },
  {
    "input": "How popular is the ALBERT: A Lite BERT implementation?",
    "output": "The google-research/albert repository has 3,274 stars and 574 forks on GitHub."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2302.04870&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2302.04870&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2301.00704&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2301.00704&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Describe the haitongli/knowledge-distillation-pytorch repository that implements Distilling the Knowledge in a Neural Network.",
    "output": "A PyTorch implementation for exploring deep and shallow knowledge distillation (KD) experiments with flexibility"
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2006.16779&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2006.16779&start=0&max_results=10 paper was published in 2020."
  },
  {
    "input": "What paper has ArXiv ID 2205.05638?",
    "output": "AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning"
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2504.01848&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2504.01848&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at openai/frontier-evals which openai frontier evals It is written in Python. The repository has 949 stars, indicating high community interest."
  },
  {
    "input": "In what year did PyTorch Geometric: Library for Geometric Deep Learning come out?",
    "output": "The PyTorch Geometric: Library for Geometric Deep Learning paper was published in 2019."
  },
  {
    "input": "What category is the RAG: Retrieval-Augmented Generation paper in?",
    "output": "RAG: Retrieval-Augmented Generation is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Summarize the Horovod: fast and easy distributed deep learning in TensorFlow paper.",
    "output": "Horovod: fast and easy distributed deep learning in TensorFlow is a 2017 paper in Machine Learning. The implementation is available at horovod/horovod which distributed training framework for tensorflow, keras, pytorch, and apache mxnet. It is written in Python. The repository has 14,631 stars, indicating high community interest."
  },
  {
    "input": "When was the MRKL Systems: A modular approach to AGI paper published?",
    "output": "The MRKL Systems: A modular approach to AGI paper was published in 2022."
  },
  {
    "input": "Explain the facebookresearch/ConvNeXt-V2 implementation of arXiv Query: search_query=&id_list=2301.00808&start=0&max_results=10.",
    "output": "Code release for ConvNeXt V2 model"
  },
  {
    "input": "Where can I find the code for Know What You Don't Know: Unanswerable Questions for SQuAD?",
    "output": "The code for Know What You Don't Know: Unanswerable Questions for SQuAD is available at https://github.com/rajpurkar/SQuAD-explorer (rajpurkar/SQuAD-explorer)."
  },
  {
    "input": "Explain the mit-han-lab/offsite-tuning implementation of arXiv Query: search_query=&id_list=2302.03011&start=0&max_results=10.",
    "output": "Offsite-Tuning: Transfer Learning without Full Model"
  },
  {
    "input": "Which field does SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding belong to?",
    "output": "SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What category is the Enriching Word Vectors with Subword Information paper in?",
    "output": "Enriching Word Vectors with Subword Information is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2112.11446?",
    "output": "FLAN: Finetuned Language Models Are Zero-Shot Learners"
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2303.08518&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2303.08518&start=0&max_results=10 is available at https://github.com/FMInference/FlexLLMGen (FMInference/FlexLLMGen)."
  },
  {
    "input": "Describe the danijar/dreamer repository that implements Dream to Control: Learning Behaviors by Latent Imagination.",
    "output": "Dream to Control: Learning Behaviors by Latent Imagination"
  },
  {
    "input": "Explain the DreamFusion: Text-to-3D using 2D Diffusion paper.",
    "output": "DreamFusion: Text-to-3D using 2D Diffusion is a 2022 paper in Computer Vision and Pattern Recognition. The implementation is available at ashawkey/stable-dreamfusion which text-to-3d & image-to-3d & mesh exportation with nerf + diffusion. It is written in Python. The repository has 8,773 stars, indicating high community interest."
  },
  {
    "input": "Where can I find the code for FLAN: Finetuned Language Models Are Zero-Shot Learners?",
    "output": "The code for FLAN: Finetuned Language Models Are Zero-Shot Learners is available at https://github.com/google-research/FLAN (google-research/FLAN)."
  },
  {
    "input": "What programming language is EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification implemented in?",
    "output": "The EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification implementation is written in Python."
  },
  {
    "input": "What year was LoRA: Low-Rank Adaptation of Large Language Models released?",
    "output": "The LoRA: Low-Rank Adaptation of Large Language Models paper was published in 2021."
  },
  {
    "input": "What category is the High-Resolution Image Synthesis with Latent Diffusion Models paper in?",
    "output": "High-Resolution Image Synthesis with Latent Diffusion Models is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "Tell me the title of ArXiv paper 2303.11331.",
    "output": "Grounding DINO: Marrying DINO with Grounded Pre-Training"
  },
  {
    "input": "What paper has ArXiv ID 2501.19393?",
    "output": "arXiv Query: search_query=&id_list=2501.19393&start=0&max_results=10"
  },
  {
    "input": "What is the GitHub repository for QLoRA: Efficient Finetuning of Quantized LLMs?",
    "output": "The code for QLoRA: Efficient Finetuning of Quantized LLMs is available at https://github.com/artidoro/qlora (artidoro/qlora)."
  },
  {
    "input": "Explain the ZhuiyiTechnology/roformer implementation of RoFormer: Enhanced Transformer with Rotary Position Embedding.",
    "output": "Rotary Transformer"
  },
  {
    "input": "What does the KimMeen/Time-LLM repository do?",
    "output": "[ICLR 2024] Official implementation of \" \ud83e\udd99 Time-LLM: Time Series Forecasting by Reprogramming Large Language Models\""
  },
  {
    "input": "What is the GitHub repository for Training language models to follow instructions with human feedback?",
    "output": "The code for Training language models to follow instructions with human feedback is available at https://github.com/openai/following-instructions-human-feedback (openai/following-instructions-human-feedback)."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2303.02913&start=0&max_results=10 repository have?",
    "output": "The Shark-NLP/OpenICL repository has 579 stars and 30 forks on GitHub."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2504.02732&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2504.02732&start=0&max_results=10 is available at https://github.com/MAXNORM8650/MedAgentSim (MAXNORM8650/MedAgentSim)."
  },
  {
    "input": "What programming language is Wide & Deep Learning for Recommender Systems implemented in?",
    "output": "The Wide & Deep Learning for Recommender Systems implementation is written in Python."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2504.11844&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2504.11844&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at Crista23/goal_directedness_llms It is written in Python. The repository has 12 stars, indicating high community interest."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2302.10866?",
    "output": "arXiv Query: search_query=&id_list=2302.10866&start=0&max_results=10"
  },
  {
    "input": "When was the Med-PaLM 2: Towards Expert-Level Medical Question Answering paper published?",
    "output": "The Med-PaLM 2: Towards Expert-Level Medical Question Answering paper was published in 2023."
  },
  {
    "input": "What year was Proximal Policy Optimization Algorithms released?",
    "output": "The Proximal Policy Optimization Algorithms paper was published in 2017."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2302.07459&start=0&max_results=10 implementation?",
    "output": "The haoliuhl/language-quantized-autoencoders repository has 111 stars and 5 forks on GitHub."
  },
  {
    "input": "What year was EfficientDet: Scalable and Efficient Object Detection released?",
    "output": "The EfficientDet: Scalable and Efficient Object Detection paper was published in 2020."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2301.01947&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2301.01947&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "How popular is the SmoothQuant: Accurate and Efficient Post-Training Quantization implementation?",
    "output": "The mit-han-lab/smoothquant repository has 1,561 stars and 190 forks on GitHub."
  },
  {
    "input": "What is PyTorch Geometric: Library for Geometric Deep Learning about?",
    "output": "PyTorch Geometric: Library for Geometric Deep Learning is a 2019 paper in Machine Learning. The implementation is available at pyg-team/pytorch_geometric which graph neural network library for pytorch It is written in Python. The repository has 23,186 stars, indicating high community interest."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2301.01181&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2301.01181&start=0&max_results=10 is available at https://github.com/JohnNay/llm-lobbyist (JohnNay/llm-lobbyist)."
  },
  {
    "input": "Tell me the title of ArXiv paper 2301.05062.",
    "output": "arXiv Query: search_query=&id_list=2301.05062&start=0&max_results=10"
  },
  {
    "input": "Which field does Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks belong to?",
    "output": "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2503.22708&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2503.22708&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at allenai/codescientist which codescientist: an automated scientific discovery system for code-based experiments It is written in Python. The repository has 301 stars, indicating high community interest."
  },
  {
    "input": "What are the key topics in the nyu-mll/jiant repository?",
    "output": "The nyu-mll/jiant repository covers: nlp, sentence-representation, bert, multitask-learning, transformers, transfer-learning."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2303.03846&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2303.03846&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What paper has ArXiv ID 2308.12950?",
    "output": "Code Llama: Open Foundation Models for Code"
  },
  {
    "input": "Explain the ibm-granite/granite-code-models implementation of arXiv Query: search_query=&id_list=2405.02803&start=0&max_results=10.",
    "output": "Granite Code Models: A Family of Open Foundation Models for Code Intelligence"
  },
  {
    "input": "What's the implementation repository for ALBERT: A Lite BERT?",
    "output": "The code for ALBERT: A Lite BERT is available at https://github.com/google-research/albert (google-research/albert)."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2301.05062?",
    "output": "arXiv Query: search_query=&id_list=2301.05062&start=0&max_results=10"
  },
  {
    "input": "What language is the deepspeedai/DeepSpeed repository written in?",
    "output": "The ZeRO: Memory Optimizations Toward Training Trillion Parameter Models implementation is written in Python."
  },
  {
    "input": "How many stars does the GPT-4 Technical Report repository have?",
    "output": "The openai/evals repository has 17,349 stars and 2,848 forks on GitHub."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2405.04324&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2405.04324&start=0&max_results=10 is a 2024 paper in Machine Learning. The implementation is available at ibm-granite/granite-code-models which granite code models: a family of open foundation models for code intelligence The repository has 1,240 stars, indicating high community interest."
  },
  {
    "input": "Explain the Denoising Diffusion Probabilistic Models paper.",
    "output": "Denoising Diffusion Probabilistic Models is a 2020 paper in Machine Learning. The implementation is available at hojonathanho/diffusion which denoising diffusion probabilistic models It is written in Python. The repository has 4,860 stars, indicating high community interest."
  },
  {
    "input": "What programming language is LLaMA: Open and Efficient Foundation Language Models implemented in?",
    "output": "The LLaMA: Open and Efficient Foundation Language Models implementation is written in Python."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2503.23513&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2503.23513&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "What is the GitHub repository for Horovod: fast and easy distributed deep learning in TensorFlow?",
    "output": "The code for Horovod: fast and easy distributed deep learning in TensorFlow is available at https://github.com/horovod/horovod (horovod/horovod)."
  },
  {
    "input": "Summarize the YOLOX: Exceeding YOLO Series in 2021 paper.",
    "output": "YOLOX: Exceeding YOLO Series in 2021 is a 2018 paper in Computer Vision and Pattern Recognition. The implementation is available at Megvii-BaseDetection/YOLOX which yolox is a high-performance anchor-free yolo, exceeding yolov3~v5 with megengine, onnx, tensorrt, ncnn, and openvino supported. documentation: https://yolox.readthedocs.io/ It is written in Python. The repository has 10,185 stars, indicating high community interest."
  },
  {
    "input": "What is Denoising Diffusion Probabilistic Models about?",
    "output": "Denoising Diffusion Probabilistic Models is a 2020 paper in Machine Learning. The implementation is available at hojonathanho/diffusion which denoising diffusion probabilistic models It is written in Python. The repository has 4,860 stars, indicating high community interest."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2410.12896&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2410.12896&start=0&max_results=10 paper was published in 2024."
  },
  {
    "input": "How popular is the AlpacaEval: An Automatic Evaluator for Instruction-following implementation?",
    "output": "The tatsu-lab/alpaca_eval repository has 1,907 stars and 290 forks on GitHub."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2502.17437&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2502.17437&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2301.02828&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2301.02828&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at frankxu2004/knnlm-why which repo for icml23 \"why do nearest neighbor language models work?\" It is written in Python. The repository has 59 stars, indicating high community interest."
  },
  {
    "input": "How many stars does the Generating Wikipedia by Summarizing Long Sequences repository have?",
    "output": "The tensorflow/tensor2tensor repository has 16,760 stars and 3,702 forks on GitHub."
  },
  {
    "input": "What is BERT: Pre-training of Deep Bidirectional Transformers about?",
    "output": "BERT: Pre-training of Deep Bidirectional Transformers is a 2018 paper in Computation and Language (Natural Language Processing). The implementation is available at google-research/bert which tensorflow code and pre-trained models for bert It is written in Python. The repository has 39,687 stars, indicating high community interest."
  },
  {
    "input": "How many stars does the HuggingGPT: Solving AI Tasks with ChatGPT repository have?",
    "output": "The microsoft/JARVIS repository has 24,460 stars and 2,051 forks on GitHub."
  },
  {
    "input": "What topics does the Beir: A Heterogeneous Benchmark for IR implementation cover?",
    "output": "The beir-cellar/beir repository covers: nlp, information-retrieval, bert, benchmark, sentence-transformers, question-generation, retrieval, passage-retrieval, elasticsearch, dpr, sbert, retrieval-models, dataset, colbert, zero-shot-retrieval, deep-learning, pytorch, llm, rag."
  },
  {
    "input": "Explain the google/trax implementation of Reformer: The Efficient Transformer.",
    "output": "Trax \u2014 Deep Learning with Clear Code and Speed"
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2504.00698?",
    "output": "arXiv Query: search_query=&id_list=2504.00698&start=0&max_results=10"
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2109.03264?",
    "output": "arXiv Query: search_query=&id_list=2109.03264&start=0&max_results=10"
  },
  {
    "input": "Explain the ControlNet: Adding Conditional Control to Text-to-Image Diffusion paper.",
    "output": "ControlNet: Adding Conditional Control to Text-to-Image Diffusion is a 2023 paper in Computer Vision and Pattern Recognition. The implementation is available at lllyasviel/ControlNet which let us control diffusion models! It is written in Python. The repository has 33,344 stars, indicating high community interest."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2503.22678&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2503.22678&start=0&max_results=10 is available at https://github.com/MAXNORM8650/MedAgentSim (MAXNORM8650/MedAgentSim)."
  },
  {
    "input": "Explain the Swin Transformer: Hierarchical Vision Transformer using Shifted Windows paper.",
    "output": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows is a 2021 paper in Computer Vision and Pattern Recognition. The implementation is available at microsoft/Swin-Transformer which this is an official implementation for \"swin transformer: hierarchical vision transformer using shifted windows\". It is written in Python. The repository has 15,466 stars, indicating high community interest."
  },
  {
    "input": "What paper has ArXiv ID 2303.03846?",
    "output": "arXiv Query: search_query=&id_list=2303.03846&start=0&max_results=10"
  },
  {
    "input": "Describe the Dao-AILab/flash-attention repository that implements FlashAttention: Fast and Memory-Efficient Exact Attention.",
    "output": "Fast and memory-efficient exact attention"
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2109.03264&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2109.03264&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What are the key topics in the Shark-NLP/OpenICL repository?",
    "output": "The Shark-NLP/OpenICL repository covers: in-context-learning, nlp, language-model."
  },
  {
    "input": "What is the GitHub repository for FLAN: Finetuned Language Models Are Zero-Shot Learners?",
    "output": "The code for FLAN: Finetuned Language Models Are Zero-Shot Learners is available at https://github.com/google-research/FLAN (google-research/FLAN)."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2009.06367&start=0&max_results=10 repository have?",
    "output": "The salesforce/GeDi repository has 209 stars and 46 forks on GitHub."
  },
  {
    "input": "What programming language is LLM.int8(): 8-bit Matrix Multiplication for Transformers implemented in?",
    "output": "The LLM.int8(): 8-bit Matrix Multiplication for Transformers implementation is written in Python."
  },
  {
    "input": "What paper has ArXiv ID 2005.08100?",
    "output": "Question and Answer Test-Train Overlap in Open-Domain QA Datasets"
  },
  {
    "input": "What year was Linformer: Self-Attention with Linear Complexity released?",
    "output": "The Linformer: Self-Attention with Linear Complexity paper was published in 2021."
  },
  {
    "input": "What topics does the Neural Collaborative Filtering implementation cover?",
    "output": "The hexiangnan/neural_collaborative_filtering repository covers: deep-learning, recommender-system, collaborative-filtering."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2406.09308&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2406.09308&start=0&max_results=10 is available at https://github.com/lamini-ai/Lamini-Memory-Tuning (lamini-ai/Lamini-Memory-Tuning)."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2302.10866&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2302.10866&start=0&max_results=10 implementation is written in Assembly."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2302.07459&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2302.07459&start=0&max_results=10 is available at https://github.com/haoliuhl/language-quantized-autoencoders (haoliuhl/language-quantized-autoencoders)."
  },
  {
    "input": "What year was InPars: Data Augmentation for Information Retrieval released?",
    "output": "The InPars: Data Augmentation for Information Retrieval paper was published in 2022."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2302.12766&start=0&max_results=10 implementation?",
    "output": "The siddk/voltron-robotics repository has 227 stars and 23 forks on GitHub."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2302.14233&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2302.14233&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2406.10209&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2406.10209&start=0&max_results=10 is a 2024 paper in Machine Learning. The implementation is available at hpcaitech/Open-Sora which open-sora: democratizing efficient video production for all It is written in Python. The repository has 27,981 stars, indicating high community interest."
  },
  {
    "input": "Tell me the title of ArXiv paper 2307.09288.",
    "output": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
  },
  {
    "input": "What language is the mit-han-lab/offsite-tuning repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2302.03011&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Explain the autonomousvision/stylegan-t implementation of arXiv Query: search_query=&id_list=2301.11305&start=0&max_results=10.",
    "output": "[ICML'23] StyleGAN-T: Unlocking the Power of GANs for Fast Large-Scale Text-to-Image Synthesis"
  },
  {
    "input": "Explain the ShishirPatil/gorilla implementation of Gorilla: Large Language Model Connected with Massive APIs.",
    "output": "Gorilla: Training and Evaluating LLMs for Function Calls (Tool Calls)"
  },
  {
    "input": "What does the openai/summarize-from-feedback repository do?",
    "output": "Code for \"Learning to summarize from human feedback\""
  },
  {
    "input": "Summarize the Wav2Vec 2.0: A Framework for Self-Supervised Learning of Speech paper.",
    "output": "Wav2Vec 2.0: A Framework for Self-Supervised Learning of Speech is a 2021 paper in Computation and Language (Natural Language Processing). The implementation is available at facebookresearch/fairseq which facebook ai research sequence-to-sequence toolkit written in python. It is written in Python. The repository has 31,985 stars, indicating high community interest."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2503.22678&start=0&max_results=10 repository have?",
    "output": "The MAXNORM8650/MedAgentSim repository has 94 stars and 11 forks on GitHub."
  },
  {
    "input": "Which field does Swin Transformer: Hierarchical Vision Transformer using Shifted Windows belong to?",
    "output": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2009.06367&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2009.06367&start=0&max_results=10 is a 2020 paper in Machine Learning. The implementation is available at salesforce/GeDi which gedi: generative discriminator guided sequence generation It is written in Python. The repository has 209 stars, indicating high community interest."
  },
  {
    "input": "What paper has ArXiv ID 1412.6980?",
    "output": "Adam: A Method for Stochastic Optimization"
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2012.06009&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2012.06009&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2406.06326&start=0&max_results=10 implementation?",
    "output": "The lamini-ai/Lamini-Memory-Tuning repository has 275 stars and 12 forks on GitHub."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2504.16084&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2504.16084&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at Crista23/goal_directedness_llms It is written in Python. The repository has 12 stars, indicating high community interest."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2301.05062&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2301.05062&start=0&max_results=10 is available at https://github.com/google-deepmind/tracr (google-deepmind/tracr)."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2301.00303&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2301.00303&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Give me an overview of Playing Atari with Deep Reinforcement Learning.",
    "output": "Playing Atari with Deep Reinforcement Learning is a 2013 paper in Machine Learning. The implementation is available at google-deepmind/dqn which lua/torch implementation of dqn (nature, 2015) It is written in Lua. The repository has 618 stars, indicating high community interest."
  },
  {
    "input": "When was the Whisper: Robust Speech Recognition via Large-Scale Weak Supervision paper published?",
    "output": "The Whisper: Robust Speech Recognition via Large-Scale Weak Supervision paper was published in 2022."
  },
  {
    "input": "Explain the Dense Passage Retrieval for Open-Domain Question Answering paper.",
    "output": "Dense Passage Retrieval for Open-Domain Question Answering is a 2020 paper in Computation and Language (Natural Language Processing). The implementation is available at facebookresearch/DPR which dense passage retriever - is a set of tools and models for open domain q&a task. It is written in Python. The repository has 1,847 stars, indicating high community interest."
  },
  {
    "input": "When was the Segment Anything paper published?",
    "output": "The Segment Anything paper was published in 2023."
  },
  {
    "input": "What is ALBERT: A Lite BERT about?",
    "output": "ALBERT: A Lite BERT is a 2019 paper in Computation and Language (Natural Language Processing). The implementation is available at google-research/albert which albert: a lite bert for self-supervised learning of language representations It is written in Python. The repository has 3,274 stars, indicating high community interest."
  },
  {
    "input": "What category is the MRKL Systems: A modular approach to AGI paper in?",
    "output": "MRKL Systems: A modular approach to AGI is in the Artificial Intelligence (cs.AI) category."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2503.20201&start=0&max_results=10 implementation?",
    "output": "The sentient-agi/OpenDeepSearch repository has 3,723 stars and 343 forks on GitHub."
  },
  {
    "input": "When was the Depthwise Separable Convolutions for Neural Machine Translation paper published?",
    "output": "The Depthwise Separable Convolutions for Neural Machine Translation paper was published in 2017."
  },
  {
    "input": "Explain the frankxu2004/knnlm-why implementation of arXiv Query: search_query=&id_list=2301.04856&start=0&max_results=10.",
    "output": "Repo for ICML23 \"Why do Nearest Neighbor Language Models Work?\""
  },
  {
    "input": "What language is the tensorflow/tensor2tensor repository written in?",
    "output": "The Attention Is All You Need implementation is written in Python."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2502.01061&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2502.01061&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at GAIR-NLP/LIMO which [colm 2025] limo: less is more for reasoning It is written in Python. The repository has 1,052 stars, indicating high community interest."
  },
  {
    "input": "How popular is the Multi-Task Deep Neural Networks for Natural Language Understanding implementation?",
    "output": "The namisan/mt-dnn repository has 2,258 stars and 414 forks on GitHub."
  },
  {
    "input": "What does the ruiqi-zhong/D5 repository do?",
    "output": "The GitHub repo for Goal Driven Discovery of Distributional Differences via Language Descriptions"
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2403.17887&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2403.17887&start=0&max_results=10 paper was published in 2024."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2404.03592&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2404.03592&start=0&max_results=10 is a 2024 paper in Machine Learning. The implementation is available at OpenBMB/Eurus It is written in Python. The repository has 320 stars, indicating high community interest."
  },
  {
    "input": "Describe the haoliuhl/language-quantized-autoencoders repository that implements arXiv Query: search_query=&id_list=2302.07459&start=0&max_results=10.",
    "output": "Language Quantized AutoEncoders"
  },
  {
    "input": "What's the implementation repository for RoBERTa: A Robustly Optimized BERT Pretraining Approach?",
    "output": "The code for RoBERTa: A Robustly Optimized BERT Pretraining Approach is available at https://github.com/facebookresearch/fairseq (facebookresearch/fairseq)."
  },
  {
    "input": "What are the key topics in the tensorflow/tensor2tensor repository?",
    "output": "The tensorflow/tensor2tensor repository covers: machine-learning, machine-translation, deep-learning, reinforcement-learning, tpu."
  },
  {
    "input": "Describe the LTH14/fractalgen repository that implements arXiv Query: search_query=&id_list=2503.01141&start=0&max_results=10.",
    "output": "PyTorch implementation of FractalGen https://arxiv.org/abs/2502.17437"
  },
  {
    "input": "What does the siddk/voltron-robotics repository do?",
    "output": "Voltron: Language-Driven Representation Learning for Robotics"
  },
  {
    "input": "What year was SQuAD: 100,000+ Questions for Machine Comprehension of Text released?",
    "output": "The SQuAD: 100,000+ Questions for Machine Comprehension of Text paper was published in 2016."
  },
  {
    "input": "Tell me the title of ArXiv paper 2109.03264.",
    "output": "arXiv Query: search_query=&id_list=2109.03264&start=0&max_results=10"
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2302.04870&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2302.04870&start=0&max_results=10 is available at https://github.com/mit-han-lab/offsite-tuning (mit-han-lab/offsite-tuning)."
  },
  {
    "input": "What is the ArXiv category of ALBERT: A Lite BERT?",
    "output": "ALBERT: A Lite BERT is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2503.22708&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2503.22708&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2303.04671&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2303.04671&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at chenfei-wu/TaskMatrix It is written in Python. The repository has 34,352 stars, indicating high community interest."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2405.03520&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2405.03520&start=0&max_results=10 is available at https://github.com/ibm-granite/granite-code-models (ibm-granite/granite-code-models)."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2305.14314?",
    "output": "QLoRA: Efficient Finetuning of Quantized LLMs"
  },
  {
    "input": "What is the ArXiv category of Deep Residual Learning for Image Recognition?",
    "output": "Deep Residual Learning for Image Recognition is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "How popular is the BLIP-2: Bootstrapping Language-Image Pre-training implementation?",
    "output": "The salesforce/LAVIS repository has 11,040 stars and 1,086 forks on GitHub."
  },
  {
    "input": "When was the DistilBERT: a distilled version of BERT paper published?",
    "output": "The DistilBERT: a distilled version of BERT paper was published in 2019."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2203.02155?",
    "output": "Training language models to follow instructions with human feedback"
  },
  {
    "input": "What category is the Depthwise Separable Convolutions for Neural Machine Translation paper in?",
    "output": "Depthwise Separable Convolutions for Neural Machine Translation is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2006.16779&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2006.16779&start=0&max_results=10 is available at https://github.com/google-research/pegasus (google-research/pegasus)."
  },
  {
    "input": "What is Learning to summarize from human feedback about?",
    "output": "Learning to summarize from human feedback is a 2020 paper in Computation and Language (Natural Language Processing). The implementation is available at openai/summarize-from-feedback which code for \"learning to summarize from human feedback\" It is written in Python. The repository has 1,055 stars, indicating high community interest."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2303.03846&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2303.03846&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Describe the LTH14/fractalgen repository that implements arXiv Query: search_query=&id_list=2503.00735&start=0&max_results=10.",
    "output": "PyTorch implementation of FractalGen https://arxiv.org/abs/2502.17437"
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2102.01192&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2102.01192&start=0&max_results=10 paper was published in 2021."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2501.19393&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2501.19393&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Tell me the title of ArXiv paper 1902.05188.",
    "output": "JAX: Composable transformations of Python+NumPy programs"
  },
  {
    "input": "What paper has ArXiv ID 1506.02640?",
    "output": "Faster R-CNN: Towards Real-Time Object Detection"
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2302.03917&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2302.03917&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What language is the state-spaces/mamba repository written in?",
    "output": "The Mamba: Linear-Time Sequence Modeling with Selective State Spaces implementation is written in Python."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2504.19413&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2504.19413&start=0&max_results=10 is available at https://github.com/MoonshotAI/Kimi-Audio (MoonshotAI/Kimi-Audio)."
  },
  {
    "input": "Explain the Open-Reasoner-Zero/Open-Reasoner-Zero implementation of arXiv Query: search_query=&id_list=2502.12115&start=0&max_results=10.",
    "output": "Official Repo for Open-Reasoner-Zero"
  },
  {
    "input": "When was the LLaVA-1.5: Improved Baselines with Visual Instruction Tuning paper published?",
    "output": "The LLaVA-1.5: Improved Baselines with Visual Instruction Tuning paper was published in 2023."
  },
  {
    "input": "Describe the jasonwei20/eda_nlp repository that implements EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification.",
    "output": "Data augmentation for NLP, presented at EMNLP 2019"
  },
  {
    "input": "What does the ermongroup/ddim repository do?",
    "output": "Denoising Diffusion Implicit Models"
  },
  {
    "input": "What year was ReAct: Synergizing Reasoning and Acting in Language Models released?",
    "output": "The ReAct: Synergizing Reasoning and Acting in Language Models paper was published in 2022."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2301.02828&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2301.02828&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at frankxu2004/knnlm-why which repo for icml23 \"why do nearest neighbor language models work?\" It is written in Python. The repository has 59 stars, indicating high community interest."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2301.00303&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2301.00303&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Describe the lucidrains/PaLM-pytorch repository that implements PaLM: Scaling Language Modeling with Pathways.",
    "output": "Implementation of the specific Transformer architecture from PaLM - Scaling Language Modeling with Pathways"
  },
  {
    "input": "Give me an overview of SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities.",
    "output": "SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at 0nutation/SpeechGPT which speechgpt series: speech large language models It is written in Python. The repository has 1,396 stars, indicating high community interest."
  },
  {
    "input": "What category is the ChemCrow: Augmenting large-language models with chemistry tools paper in?",
    "output": "ChemCrow: Augmenting large-language models with chemistry tools is in the Artificial Intelligence (cs.AI) category."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2303.02506&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2303.02506&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at NVlabs/prismer which the implementation of \"prismer: a vision-language model with multi-task experts\". It is written in Python. The repository has 1,307 stars, indicating high community interest."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2406.07394&start=0&max_results=10 implementation?",
    "output": "The hpcaitech/Open-Sora repository has 27,981 stars and 2,781 forks on GitHub."
  },
  {
    "input": "When was the CLIP: Learning Transferable Visual Models From Natural Language paper published?",
    "output": "The CLIP: Learning Transferable Visual Models From Natural Language paper was published in 2021."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2004.08492&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2004.08492&start=0&max_results=10 is available at https://github.com/uber/orbit (uber/orbit)."
  },
  {
    "input": "What category is the MiniGPT-4: Enhancing Vision-Language Understanding paper in?",
    "output": "MiniGPT-4: Enhancing Vision-Language Understanding is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "Explain the EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification paper.",
    "output": "EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification is a 2019 paper in Computation and Language (Natural Language Processing). The implementation is available at jasonwei20/eda_nlp which data augmentation for nlp, presented at emnlp 2019 It is written in Python. The repository has 1,649 stars, indicating high community interest."
  },
  {
    "input": "What does the Dao-AILab/flash-attention repository do?",
    "output": "Fast and memory-efficient exact attention"
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2302.14233&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2302.14233&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at ruiqi-zhong/D5 which the github repo for goal driven discovery of distributional differences via language descriptions It is written in Python. The repository has 71 stars, indicating high community interest."
  },
  {
    "input": "How popular is the Dream to Control: Learning Behaviors by Latent Imagination implementation?",
    "output": "The danijar/dreamer repository has 565 stars and 113 forks on GitHub."
  },
  {
    "input": "Tell me the title of ArXiv paper 2111.00396.",
    "output": "S4: Efficiently Modeling Long Sequences"
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2503.22678&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2503.22678&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What is the ArXiv category of ELECTRA: Pre-training Text Encoders as Discriminators?",
    "output": "ELECTRA: Pre-training Text Encoders as Discriminators is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2302.13971?",
    "output": "LLaMA: Open and Efficient Foundation Language Models"
  },
  {
    "input": "Which field does Depthwise Separable Convolutions for Neural Machine Translation belong to?",
    "output": "Depthwise Separable Convolutions for Neural Machine Translation is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What are the key topics in the google-research/bert repository?",
    "output": "The google-research/bert repository covers: nlp, google, natural-language-processing, natural-language-understanding, tensorflow."
  },
  {
    "input": "What is the GitHub repository for The Flan Collection: Designing Data and Methods for Effective Instruction Tuning?",
    "output": "The code for The Flan Collection: Designing Data and Methods for Effective Instruction Tuning is available at https://github.com/google-research/FLAN (google-research/FLAN)."
  },
  {
    "input": "What language is the ContextualAI/HALOs repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2312.02783&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2503.20201&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2503.20201&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Summarize the BLIP-2: Bootstrapping Language-Image Pre-training paper.",
    "output": "BLIP-2: Bootstrapping Language-Image Pre-training is a 2023 paper in Computer Vision and Pattern Recognition. The implementation is available at salesforce/LAVIS which lavis - a one-stop library for language-vision intelligence It is written in Jupyter Notebook. The repository has 11,040 stars, indicating high community interest."
  },
  {
    "input": "What does the XiangLi1999/PrefixTuning repository do?",
    "output": "Prefix-Tuning: Optimizing Continuous Prompts for Generation"
  },
  {
    "input": "Tell me the title of ArXiv paper 1801.01290.",
    "output": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning"
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2406.06326&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2406.06326&start=0&max_results=10 is a 2024 paper in Machine Learning. The implementation is available at lamini-ai/Lamini-Memory-Tuning which banishing llm hallucinations requires rethinking generalization The repository has 275 stars, indicating high community interest."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2006.04768?",
    "output": "DeBERTa: Decoding-enhanced BERT with Disentangled Attention"
  },
  {
    "input": "Explain the Competition-Level Code Generation with AlphaCode paper.",
    "output": "Competition-Level Code Generation with AlphaCode is a 2022 paper in Machine Learning. The implementation is available at google-deepmind/code_contests It is written in C++. The repository has 2,177 stars, indicating high community interest."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2301.05062&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2301.05062&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What category is the Playing Atari with Deep Reinforcement Learning paper in?",
    "output": "Playing Atari with Deep Reinforcement Learning is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2502.17437&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2502.17437&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What topics does the Med-PaLM 2: Towards Expert-Level Medical Question Answering implementation cover?",
    "output": "The google-research/google-research repository covers: machine-learning, ai, research."
  },
  {
    "input": "What is Reformer: The Efficient Transformer about?",
    "output": "Reformer: The Efficient Transformer is a 2020 paper in Machine Learning. The implementation is available at google/trax which trax \u2014 deep learning with clear code and speed It is written in Python. The repository has 8,293 stars, indicating high community interest."
  },
  {
    "input": "Give me an overview of Flamingo: A Visual Language Model for Few-Shot Learning.",
    "output": "Flamingo: A Visual Language Model for Few-Shot Learning is a 2022 paper in Computer Vision and Pattern Recognition. The implementation is available at lucidrains/flamingo-pytorch which implementation of \ud83e\udda9 flamingo, state-of-the-art few-shot visual question answering attention net out of deepmind, in pytorch It is written in Python. The repository has 1,270 stars, indicating high community interest."
  },
  {
    "input": "What does the pytorch/pytorch repository do?",
    "output": "Tensors and Dynamic neural networks in Python with strong GPU acceleration"
  },
  {
    "input": "What's the implementation repository for Inductive Representation Learning on Large Graphs?",
    "output": "The code for Inductive Representation Learning on Large Graphs is available at https://github.com/williamleif/GraphSAGE (williamleif/GraphSAGE)."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2303.01469&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2303.01469&start=0&max_results=10 is available at https://github.com/ruiqi-zhong/D5 (ruiqi-zhong/D5)."
  },
  {
    "input": "What topics does the arXiv Query: search_query=&id_list=2312.02783&start=0&max_results=10 implementation cover?",
    "output": "The ContextualAI/HALOs repository covers: alignment, dpo, halos, kto, ppo, rlhf."
  },
  {
    "input": "Describe the openai/whisper repository that implements Whisper: Robust Speech Recognition via Large-Scale Weak Supervision.",
    "output": "Robust Speech Recognition via Large-Scale Weak Supervision"
  },
  {
    "input": "What paper has ArXiv ID 1707.06347?",
    "output": "Proximal Policy Optimization Algorithms"
  },
  {
    "input": "What does the facebookresearch/CovidPrognosis repository do?",
    "output": "COVID deterioration prediction based on chest X-ray radiographs via MoCo-trained image representations"
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2211.10438?",
    "output": "SmoothQuant: Accurate and Efficient Post-Training Quantization"
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2102.01192&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2102.01192&start=0&max_results=10 paper was published in 2021."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2107.07346&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2107.07346&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2104.00355&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2104.00355&start=0&max_results=10 paper was published in 2021."
  },
  {
    "input": "How many stars does the Beir: A Heterogeneous Benchmark for IR repository have?",
    "output": "The beir-cellar/beir repository has 2,007 stars and 225 forks on GitHub."
  },
  {
    "input": "Tell me the title of ArXiv paper 2306.05685.",
    "output": "Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena"
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2504.19413&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2504.19413&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2302.14838?",
    "output": "arXiv Query: search_query=&id_list=2302.14838&start=0&max_results=10"
  },
  {
    "input": "Summarize the Prefix-Tuning: Optimizing Continuous Prompts for Generation paper.",
    "output": "Prefix-Tuning: Optimizing Continuous Prompts for Generation is a 2021 paper in Computation and Language (Natural Language Processing). The implementation is available at XiangLi1999/PrefixTuning which prefix-tuning: optimizing continuous prompts for generation It is written in Python. The repository has 954 stars, indicating high community interest."
  },
  {
    "input": "What language is the amazon-science/mm-cot repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2301.12652&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2303.09431&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2303.09431&start=0&max_results=10 is available at https://github.com/FMInference/FlexLLMGen (FMInference/FlexLLMGen)."
  },
  {
    "input": "What year was Horovod: fast and easy distributed deep learning in TensorFlow released?",
    "output": "The Horovod: fast and easy distributed deep learning in TensorFlow paper was published in 2017."
  },
  {
    "input": "What is Whisper: Robust Speech Recognition via Large-Scale Weak Supervision about?",
    "output": "Whisper: Robust Speech Recognition via Large-Scale Weak Supervision is a 2022 paper in Computation and Language (Natural Language Processing). The implementation is available at openai/whisper which robust speech recognition via large-scale weak supervision It is written in Python. The repository has 91,236 stars, indicating high community interest."
  },
  {
    "input": "Where can I find the code for Reformer: The Efficient Transformer?",
    "output": "The code for Reformer: The Efficient Transformer is available at https://github.com/google/trax (google/trax)."
  },
  {
    "input": "Which field does Proximal Policy Optimization Algorithms belong to?",
    "output": "Proximal Policy Optimization Algorithms is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What paper has ArXiv ID 2301.13188?",
    "output": "arXiv Query: search_query=&id_list=2301.13188&start=0&max_results=10"
  },
  {
    "input": "What language is the IDEA-Research/GroundingDINO repository written in?",
    "output": "The Grounding DINO: Marrying DINO with Grounded Pre-Training implementation is written in Python."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2302.04023&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2302.04023&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "When was the SimCSE: Simple Contrastive Learning of Sentence Embeddings paper published?",
    "output": "The SimCSE: Simple Contrastive Learning of Sentence Embeddings paper was published in 2021."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2303.04671&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2303.04671&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "Tell me the title of ArXiv paper 1906.04341.",
    "output": "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks"
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2301.09515&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2301.09515&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at autonomousvision/stylegan-t which [icml'23] stylegan-t: unlocking the power of gans for fast large-scale text-to-image synthesis It is written in Python. The repository has 1,196 stars, indicating high community interest."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2301.05062&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2301.05062&start=0&max_results=10 is available at https://github.com/google-deepmind/tracr (google-deepmind/tracr)."
  },
  {
    "input": "What does the huggingface/alignment-handbook repository do?",
    "output": "Robust recipes to align language models with human and AI preferences"
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2301.04856&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2301.04856&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at frankxu2004/knnlm-why which repo for icml23 \"why do nearest neighbor language models work?\" It is written in Python. The repository has 59 stars, indicating high community interest."
  },
  {
    "input": "What category is the AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning paper in?",
    "output": "AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2301.13188&start=0&max_results=10 implementation?",
    "output": "The amazon-science/mm-cot repository has 3,982 stars and 333 forks on GitHub."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2502.19328&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2502.19328&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "What does the facebookresearch/fairseq repository do?",
    "output": "Facebook AI Research Sequence-to-Sequence Toolkit written in Python."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2004.08492&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2004.08492&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is the ArXiv category of HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction?",
    "output": "HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "When was the ALBERT: A Lite BERT paper published?",
    "output": "The ALBERT: A Lite BERT paper was published in 2019."
  },
  {
    "input": "What paper has ArXiv ID 2301.09515?",
    "output": "arXiv Query: search_query=&id_list=2301.09515&start=0&max_results=10"
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2303.04129&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2303.04129&start=0&max_results=10 is available at https://github.com/HazyResearch/safari (HazyResearch/safari)."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2101.04909&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2101.04909&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What language is the google-research/text-to-text-transfer-transformer repository written in?",
    "output": "The T5: Exploring the Limits of Transfer Learning implementation is written in Python."
  },
  {
    "input": "Where can I find the code for High-Resolution Image Synthesis with Latent Diffusion Models?",
    "output": "The code for High-Resolution Image Synthesis with Latent Diffusion Models is available at https://github.com/CompVis/latent-diffusion (CompVis/latent-diffusion)."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2302.04023&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2302.04023&start=0&max_results=10 is available at https://github.com/mit-han-lab/offsite-tuning (mit-han-lab/offsite-tuning)."
  },
  {
    "input": "Describe the tensorflow/tensorflow repository that implements TensorFlow: A System for Large-Scale Machine Learning.",
    "output": "An Open Source Machine Learning Framework for Everyone"
  },
  {
    "input": "In what year did Diffusion Models Beat GANs on Image Synthesis come out?",
    "output": "The Diffusion Models Beat GANs on Image Synthesis paper was published in 2021."
  },
  {
    "input": "What's the implementation repository for Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning?",
    "output": "The code for Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning is available at https://github.com/rail-berkeley/softlearning (rail-berkeley/softlearning)."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2503.22678&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2503.22678&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "Tell me the title of ArXiv paper 2504.02732.",
    "output": "arXiv Query: search_query=&id_list=2504.02732&start=0&max_results=10"
  },
  {
    "input": "Tell me the title of ArXiv paper 2203.07814.",
    "output": "Competition-Level Code Generation with AlphaCode"
  },
  {
    "input": "What paper has ArXiv ID 2006.11239?",
    "output": "Denoising Diffusion Probabilistic Models"
  },
  {
    "input": "Summarize the Animate Anyone: Consistent and Controllable Image-to-Video Synthesis paper.",
    "output": "Animate Anyone: Consistent and Controllable Image-to-Video Synthesis is a 2023 paper in Computer Vision and Pattern Recognition. The implementation is available at HumanAIGC/AnimateAnyone which animate anyone: consistent and controllable image-to-video synthesis for character animation The repository has 14,797 stars, indicating high community interest."
  },
  {
    "input": "Explain the AWQ: Activation-aware Weight Quantization for LLM Compression paper.",
    "output": "AWQ: Activation-aware Weight Quantization for LLM Compression is a 2023 paper in Machine Learning. The implementation is available at mit-han-lab/llm-awq which [mlsys 2024 best paper award] awq: activation-aware weight quantization for llm compression and acceleration It is written in Python. The repository has 3,357 stars, indicating high community interest."
  },
  {
    "input": "Summarize the Contriever: Unsupervised Dense Information Retrieval paper.",
    "output": "Contriever: Unsupervised Dense Information Retrieval is a 2022 paper in Information Retrieval. The implementation is available at facebookresearch/contriever which contriever: unsupervised dense information retrieval with contrastive learning It is written in Python. The repository has 763 stars, indicating high community interest."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2009.06367&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2009.06367&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What's the implementation repository for PyTorch Geometric: Library for Geometric Deep Learning?",
    "output": "The code for PyTorch Geometric: Library for Geometric Deep Learning is available at https://github.com/pyg-team/pytorch_geometric (pyg-team/pytorch_geometric)."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2205.01917?",
    "output": "Masked Autoencoders Are Scalable Vision Learners"
  },
  {
    "input": "What category is the ReAct: Synergizing Reasoning and Acting in Language Models paper in?",
    "output": "ReAct: Synergizing Reasoning and Acting in Language Models is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What language is the facebookresearch/DPR repository written in?",
    "output": "The Dense Passage Retrieval for Open-Domain Question Answering implementation is written in Python."
  },
  {
    "input": "What programming language is DistilBERT: a distilled version of BERT implemented in?",
    "output": "The DistilBERT: a distilled version of BERT implementation is written in Python."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2504.20734&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2504.20734&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at MoonshotAI/Kimi-Audio which kimi-audio, an open-source audio foundation model excelling in audio understanding, generation, and conversation It is written in Python. The repository has 4,367 stars, indicating high community interest."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2303.09431&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2303.09431&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Tell me the title of ArXiv paper 2303.08518.",
    "output": "arXiv Query: search_query=&id_list=2303.08518&start=0&max_results=10"
  },
  {
    "input": "What's the implementation repository for Segment Anything in Medical Images?",
    "output": "The code for Segment Anything in Medical Images is available at https://github.com/bowang-lab/MedSAM (bowang-lab/MedSAM)."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2302.10866&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2302.10866&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2504.01848&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2504.01848&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "Where can I find the code for Dense Passage Retrieval for Open-Domain Question Answering?",
    "output": "The code for Dense Passage Retrieval for Open-Domain Question Answering is available at https://github.com/facebookresearch/DPR (facebookresearch/DPR)."
  },
  {
    "input": "What are the key topics in the MAXNORM8650/MedAgentSim repository?",
    "output": "The MAXNORM8650/MedAgentSim repository covers: multi-agent-simulations."
  },
  {
    "input": "What is XLNet: Generalized Autoregressive Pretraining about?",
    "output": "XLNet: Generalized Autoregressive Pretraining is a 2019 paper in Computation and Language (Natural Language Processing). The implementation is available at zihangdai/xlnet which xlnet: generalized autoregressive pretraining for language understanding It is written in Python. The repository has 6,179 stars, indicating high community interest."
  },
  {
    "input": "What language is the bigcode-project/starcoder repository written in?",
    "output": "The StarCoder: May the source be with you! implementation is written in Python."
  },
  {
    "input": "Summarize the WizardCoder: Empowering Code LLMs with Evol-Instruct paper.",
    "output": "WizardCoder: Empowering Code LLMs with Evol-Instruct is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at nlpxucan/WizardLM which llms build upon evol insturct: wizardlm, wizardcoder, wizardmath It is written in Python. The repository has 9,460 stars, indicating high community interest."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2302.12766&start=0&max_results=10 repository have?",
    "output": "The siddk/voltron-robotics repository has 227 stars and 23 forks on GitHub."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2301.09515&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2301.09515&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at autonomousvision/stylegan-t which [icml'23] stylegan-t: unlocking the power of gans for fast large-scale text-to-image synthesis It is written in Python. The repository has 1,196 stars, indicating high community interest."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2301.01181&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2301.01181&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What category is the Efficient Estimation of Word Representations in Vector Space paper in?",
    "output": "Efficient Estimation of Word Representations in Vector Space is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2502.17437&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2502.17437&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "Tell me the title of ArXiv paper 2212.03533.",
    "output": "Text Embeddings by Weakly-Supervised Contrastive Pre-training"
  },
  {
    "input": "How many stars does the HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction repository have?",
    "output": "The facebookresearch/fairseq repository has 31,985 stars and 6,630 forks on GitHub."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2012.06009&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2012.06009&start=0&max_results=10 is a 2020 paper in Machine Learning. The implementation is available at facebookresearch/CovidPrognosis which covid deterioration prediction based on chest x-ray radiographs via moco-trained image representations It is written in Python. The repository has 158 stars, indicating high community interest."
  },
  {
    "input": "When was the Inductive Representation Learning on Large Graphs paper published?",
    "output": "The Inductive Representation Learning on Large Graphs paper was published in 2017."
  },
  {
    "input": "Tell me the title of ArXiv paper 2101.04909.",
    "output": "arXiv Query: search_query=&id_list=2101.04909&start=0&max_results=10"
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2303.04671&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2303.04671&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at chenfei-wu/TaskMatrix It is written in Python. The repository has 34,352 stars, indicating high community interest."
  },
  {
    "input": "What year was Wide & Deep Learning for Recommender Systems released?",
    "output": "The Wide & Deep Learning for Recommender Systems paper was published in 2016."
  },
  {
    "input": "What is TinyBERT: Distilling BERT for Natural Language Understanding about?",
    "output": "TinyBERT: Distilling BERT for Natural Language Understanding is a 2019 paper in Computation and Language (Natural Language Processing). The implementation is available at huawei-noah/Pretrained-Language-Model which pretrained language model and its related optimization techniques developed by huawei noah's ark lab. It is written in Python. The repository has 3,149 stars, indicating high community interest."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2009.06367&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2009.06367&start=0&max_results=10 paper was published in 2020."
  },
  {
    "input": "What does the salesforce/BLIP repository do?",
    "output": "PyTorch code for BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation  "
  },
  {
    "input": "What paper has ArXiv ID 2301.01181?",
    "output": "arXiv Query: search_query=&id_list=2301.01181&start=0&max_results=10"
  },
  {
    "input": "Which field does Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context belong to?",
    "output": "Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2302.07459&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2302.07459&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "How popular is the Deep Residual Learning for Image Recognition implementation?",
    "output": "The KaimingHe/deep-residual-networks repository has 6,664 stars and 2,231 forks on GitHub."
  },
  {
    "input": "Where can I find the code for LIMA: Less Is More for Alignment?",
    "output": "The code for LIMA: Less Is More for Alignment is available at https://github.com/meta-llama/llama (meta-llama/llama)."
  },
  {
    "input": "How many stars does the Diffusion Models Beat GANs on Image Synthesis repository have?",
    "output": "The openai/guided-diffusion repository has 7,161 stars and 886 forks on GitHub."
  },
  {
    "input": "Tell me the title of ArXiv paper 2101.00027.",
    "output": "SimCSE: Simple Contrastive Learning of Sentence Embeddings"
  },
  {
    "input": "What is AudioPaLM: A Large Language Model for Speech Understanding about?",
    "output": "AudioPaLM: A Large Language Model for Speech Understanding is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at lucidrains/audiolm-pytorch which implementation of audiolm, a sota language modeling approach to audio generation out of google research, in pytorch It is written in Python. The repository has 2,607 stars, indicating high community interest."
  },
  {
    "input": "Explain the nyu-mll/jiant implementation of SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding.",
    "output": "jiant is an nlp toolkit"
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2303.04671&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2303.04671&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is the GitHub repository for GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers?",
    "output": "The code for GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers is available at https://github.com/IST-DASLab/gptq (IST-DASLab/gptq)."
  },
  {
    "input": "What is the ArXiv category of Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning?",
    "output": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Give me an overview of PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel.",
    "output": "PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel is a 2022 paper in Distributed, Parallel, and Cluster Computing. The implementation is available at pytorch/pytorch which tensors and dynamic neural networks in python with strong gpu acceleration It is written in Python. The repository has 95,348 stars, indicating high community interest."
  },
  {
    "input": "What language is the MoonshotAI/Kimi-Audio repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2504.21801&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "In what year did Distilling the Knowledge in a Neural Network come out?",
    "output": "The Distilling the Knowledge in a Neural Network paper was published in 2015."
  },
  {
    "input": "Where can I find the code for SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding?",
    "output": "The code for SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding is available at https://github.com/nyu-mll/jiant (nyu-mll/jiant)."
  },
  {
    "input": "What category is the Billion-scale similarity search with GPUs paper in?",
    "output": "Billion-scale similarity search with GPUs is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2502.09992&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2502.09992&start=0&max_results=10 is available at https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero (Open-Reasoner-Zero/Open-Reasoner-Zero)."
  },
  {
    "input": "What topics does the arXiv Query: search_query=&id_list=2302.04870&start=0&max_results=10 implementation cover?",
    "output": "The mit-han-lab/offsite-tuning repository covers: deep-learning, transfer-learning."
  },
  {
    "input": "Where can I find the code for EfficientDet: Scalable and Efficient Object Detection?",
    "output": "The code for EfficientDet: Scalable and Efficient Object Detection is available at https://github.com/google/automl (google/automl)."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2302.12246&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2302.12246&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2303.04129&start=0&max_results=10 repository have?",
    "output": "The HazyResearch/safari repository has 904 stars and 70 forks on GitHub."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2007.16122&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2007.16122&start=0&max_results=10 is available at https://github.com/linkedin/gdmix (linkedin/gdmix)."
  },
  {
    "input": "What are the key topics in the ContextualAI/HALOs repository?",
    "output": "The ContextualAI/HALOs repository covers: alignment, dpo, halos, kto, ppo, rlhf."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2012.06009&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2012.06009&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "In what year did Know What You Don't Know: Unanswerable Questions for SQuAD come out?",
    "output": "The Know What You Don't Know: Unanswerable Questions for SQuAD paper was published in 2018."
  },
  {
    "input": "How many stars does the Adam: A Method for Stochastic Optimization repository have?",
    "output": "The pytorch/pytorch repository has 95,348 stars and 26,003 forks on GitHub."
  },
  {
    "input": "In what year did DreamBooth: Fine Tuning Text-to-Image Diffusion Models come out?",
    "output": "The DreamBooth: Fine Tuning Text-to-Image Diffusion Models paper was published in 2022."
  },
  {
    "input": "What year was Llama 2: Open Foundation and Fine-Tuned Chat Models released?",
    "output": "The Llama 2: Open Foundation and Fine-Tuned Chat Models paper was published in 2023."
  },
  {
    "input": "How popular is the SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities implementation?",
    "output": "The 0nutation/SpeechGPT repository has 1,396 stars and 95 forks on GitHub."
  },
  {
    "input": "How many stars does the Universal Language Model Fine-tuning for Text Classification repository have?",
    "output": "The fastai/fastai repository has 27,611 stars and 7,663 forks on GitHub."
  },
  {
    "input": "What language is the facebookresearch/contriever repository written in?",
    "output": "The Contriever: Unsupervised Dense Information Retrieval implementation is written in Python."
  },
  {
    "input": "Explain the Efficient Estimation of Word Representations in Vector Space paper.",
    "output": "Efficient Estimation of Word Representations in Vector Space is a 2013 paper in Computation and Language (Natural Language Processing). The implementation is available at tmikolov/word2vec which automatically exported from code.google.com/p/word2vec It is written in C. The repository has 1,569 stars, indicating high community interest."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2503.23513&start=0&max_results=10 repository have?",
    "output": "The MAXNORM8650/MedAgentSim repository has 94 stars and 11 forks on GitHub."
  },
  {
    "input": "What is Proximal Policy Optimization Algorithms about?",
    "output": "Proximal Policy Optimization Algorithms is a 2017 paper in Machine Learning. The implementation is available at openai/baselines which openai baselines: high-quality implementations of reinforcement learning algorithms It is written in Python. The repository has 16,566 stars, indicating high community interest."
  },
  {
    "input": "What language is the lllyasviel/ControlNet repository written in?",
    "output": "The ControlNet: Adding Conditional Control to Text-to-Image Diffusion implementation is written in Python."
  },
  {
    "input": "What does the google-research/bert repository do?",
    "output": "TensorFlow code and pre-trained models for BERT"
  },
  {
    "input": "What paper has ArXiv ID 2302.00902?",
    "output": "arXiv Query: search_query=&id_list=2302.00902&start=0&max_results=10"
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2302.03011&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2302.03011&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at mit-han-lab/offsite-tuning which offsite-tuning: transfer learning without full model It is written in Python. The repository has 382 stars, indicating high community interest."
  },
  {
    "input": "What topics does the HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction implementation cover?",
    "output": "The facebookresearch/fairseq repository covers: python, pytorch, artificial-intelligence."
  },
  {
    "input": "What language is the openai/whisper repository written in?",
    "output": "The Whisper: Robust Speech Recognition via Large-Scale Weak Supervision implementation is written in Python."
  },
  {
    "input": "Explain the Attention Is All You Need paper.",
    "output": "Attention Is All You Need is a 2017 paper in Computation and Language (Natural Language Processing). The implementation is available at tensorflow/tensor2tensor which library of deep learning models and datasets designed to make deep learning more accessible and accelerate ml research. It is written in Python. The repository has 16,760 stars, indicating high community interest."
  },
  {
    "input": "What is Playing Atari with Deep Reinforcement Learning about?",
    "output": "Playing Atari with Deep Reinforcement Learning is a 2013 paper in Machine Learning. The implementation is available at google-deepmind/dqn which lua/torch implementation of dqn (nature, 2015) It is written in Lua. The repository has 618 stars, indicating high community interest."
  },
  {
    "input": "In what year did AudioPaLM: A Large Language Model for Speech Understanding come out?",
    "output": "The AudioPaLM: A Large Language Model for Speech Understanding paper was published in 2023."
  },
  {
    "input": "What is DistilBERT: a distilled version of BERT about?",
    "output": "DistilBERT: a distilled version of BERT is a 2019 paper in Computation and Language (Natural Language Processing). The implementation is available at huggingface/transformers which \ud83e\udd17 transformers: the model-definition framework for state-of-the-art machine learning models in text, vision, audio, and multimodal models, for both inference and training.  It is written in Python. The repository has 152,994 stars, indicating high community interest."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2009.06367&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2009.06367&start=0&max_results=10 is available at https://github.com/salesforce/GeDi (salesforce/GeDi)."
  },
  {
    "input": "What paper has ArXiv ID 1906.08237?",
    "output": "XLNet: Generalized Autoregressive Pretraining"
  },
  {
    "input": "Give me an overview of SimCSE: Simple Contrastive Learning of Sentence Embeddings.",
    "output": "SimCSE: Simple Contrastive Learning of Sentence Embeddings is a 2021 paper in Computation and Language (Natural Language Processing). The implementation is available at princeton-nlp/SimCSE which [emnlp 2021] simcse: simple contrastive learning of sentence embeddings https://arxiv.org/abs/2104.08821 It is written in Python. The repository has 3,615 stars, indicating high community interest."
  },
  {
    "input": "Where can I find the code for LLaVA-1.5: Improved Baselines with Visual Instruction Tuning?",
    "output": "The code for LLaVA-1.5: Improved Baselines with Visual Instruction Tuning is available at https://github.com/haotian-liu/LLaVA (haotian-liu/LLaVA)."
  },
  {
    "input": "What are the key topics in the google-research/prompt-tuning repository?",
    "output": "The google-research/prompt-tuning repository covers: prompt-tuning, nlp, machine-learning, jax, flax, language-model."
  },
  {
    "input": "Where can I find the code for TensorFlow: A System for Large-Scale Machine Learning?",
    "output": "The code for TensorFlow: A System for Large-Scale Machine Learning is available at https://github.com/tensorflow/tensorflow (tensorflow/tensorflow)."
  },
  {
    "input": "Describe the google-research/text-to-text-transfer-transformer repository that implements T5: Exploring the Limits of Transfer Learning.",
    "output": "Code for the paper \"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\""
  },
  {
    "input": "Explain the facebookresearch/fastText implementation of Enriching Word Vectors with Subword Information.",
    "output": "Library for fast text representation and classification."
  },
  {
    "input": "What paper has ArXiv ID 2203.08913?",
    "output": "RetNet: Retentive Network: A Successor to Transformer"
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2504.21801&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2504.21801&start=0&max_results=10 is available at https://github.com/MoonshotAI/Kimi-Audio (MoonshotAI/Kimi-Audio)."
  },
  {
    "input": "What is Neural Collaborative Filtering about?",
    "output": "Neural Collaborative Filtering is a 2017 paper in Information Retrieval. The implementation is available at hexiangnan/neural_collaborative_filtering which neural collaborative filtering It is written in Python. The repository has 1,867 stars, indicating high community interest."
  },
  {
    "input": "What category is the BERT: Pre-training of Deep Bidirectional Transformers paper in?",
    "output": "BERT: Pre-training of Deep Bidirectional Transformers is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "How popular is the Dense Passage Retrieval for Open-Domain Question Answering implementation?",
    "output": "The facebookresearch/DPR repository has 1,847 stars and 315 forks on GitHub."
  },
  {
    "input": "In what year did PaLM: Scaling Language Modeling with Pathways come out?",
    "output": "The PaLM: Scaling Language Modeling with Pathways paper was published in 2022."
  },
  {
    "input": "Tell me the title of ArXiv paper 2302.10866.",
    "output": "arXiv Query: search_query=&id_list=2302.10866&start=0&max_results=10"
  },
  {
    "input": "What language is the LTH14/fractalgen repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2503.01141&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What does the tatsu-lab/alpaca_eval repository do?",
    "output": "An automatic evaluator for instruction-following language models. Human-validated, high-quality, cheap, and fast."
  },
  {
    "input": "What is the GitHub repository for ALBERT: A Lite BERT?",
    "output": "The code for ALBERT: A Lite BERT is available at https://github.com/google-research/albert (google-research/albert)."
  },
  {
    "input": "Give me an overview of LLaMA-Adapter: Efficient Fine-tuning of Language Models.",
    "output": "LLaMA-Adapter: Efficient Fine-tuning of Language Models is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at OpenGVLab/LLaMA-Adapter which [iclr 2024] fine-tuning llama to follow instructions within 1 hour and 1.2m parameters It is written in Python. The repository has 5,923 stars, indicating high community interest."
  },
  {
    "input": "What is the ArXiv category of DistilBERT: a distilled version of BERT?",
    "output": "DistilBERT: a distilled version of BERT is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Which field does Med-PaLM 2: Towards Expert-Level Medical Question Answering belong to?",
    "output": "Med-PaLM 2: Towards Expert-Level Medical Question Answering is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Summarize the GPT-3: Language Models are Few-Shot Learners paper.",
    "output": "GPT-3: Language Models are Few-Shot Learners is a 2020 paper in Computation and Language (Natural Language Processing). The implementation is available at openai/gpt-3 which gpt-3: language models are few-shot learners The repository has 15,784 stars, indicating high community interest."
  },
  {
    "input": "What is ELECTRA: Pre-training Text Encoders as Discriminators about?",
    "output": "ELECTRA: Pre-training Text Encoders as Discriminators is a 2019 paper in Computation and Language (Natural Language Processing). The implementation is available at google-research/electra which electra: pre-training text encoders as discriminators rather than generators It is written in Python. The repository has 2,365 stars, indicating high community interest."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2501.13824&start=0&max_results=10 implementation?",
    "output": "The plurai-ai/intellagent repository has 1,148 stars and 139 forks on GitHub."
  },
  {
    "input": "What does the state-spaces/s4 repository do?",
    "output": "Structured state space sequence models"
  },
  {
    "input": "How many stars does the YOLOX: Exceeding YOLO Series in 2021 repository have?",
    "output": "The Megvii-BaseDetection/YOLOX repository has 10,185 stars and 2,413 forks on GitHub."
  },
  {
    "input": "Tell me the title of ArXiv paper 1301.3781.",
    "output": "Efficient Estimation of Word Representations in Vector Space"
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2504.21801&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2504.21801&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is the GitHub repository for Swin Transformer: Hierarchical Vision Transformer using Shifted Windows?",
    "output": "The code for Swin Transformer: Hierarchical Vision Transformer using Shifted Windows is available at https://github.com/microsoft/Swin-Transformer (microsoft/Swin-Transformer)."
  },
  {
    "input": "Tell me the title of ArXiv paper 2505.22101.",
    "output": "arXiv Query: search_query=&id_list=2505.22101&start=0&max_results=10"
  },
  {
    "input": "What is the GitHub repository for Competition-Level Code Generation with AlphaCode?",
    "output": "The code for Competition-Level Code Generation with AlphaCode is available at https://github.com/google-deepmind/code_contests (google-deepmind/code_contests)."
  },
  {
    "input": "Tell me the title of ArXiv paper 2302.12246.",
    "output": "arXiv Query: search_query=&id_list=2302.12246&start=0&max_results=10"
  },
  {
    "input": "What category is the Attention Is All You Need paper in?",
    "output": "Attention Is All You Need is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2301.04104&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2301.04104&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at google-deepmind/tracr It is written in Python. The repository has 548 stars, indicating high community interest."
  },
  {
    "input": "What's the implementation repository for Semi-Supervised Classification with Graph Convolutional Networks?",
    "output": "The code for Semi-Supervised Classification with Graph Convolutional Networks is available at https://github.com/tkipf/gcn (tkipf/gcn)."
  },
  {
    "input": "What language is the nyu-mll/jiant repository written in?",
    "output": "The SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding implementation is written in Python."
  },
  {
    "input": "What does the mit-han-lab/llm-awq repository do?",
    "output": "[MLSys 2024 Best Paper Award] AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration"
  },
  {
    "input": "What paper has ArXiv ID 2503.22678?",
    "output": "arXiv Query: search_query=&id_list=2503.22678&start=0&max_results=10"
  },
  {
    "input": "Tell me the title of ArXiv paper 2301.00808.",
    "output": "arXiv Query: search_query=&id_list=2301.00808&start=0&max_results=10"
  },
  {
    "input": "What paper has ArXiv ID 2007.16122?",
    "output": "arXiv Query: search_query=&id_list=2007.16122&start=0&max_results=10"
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2303.06865&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2303.06865&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What programming language is Multi-Task Deep Neural Networks for Natural Language Understanding implemented in?",
    "output": "The Multi-Task Deep Neural Networks for Natural Language Understanding implementation is written in Python."
  },
  {
    "input": "What is Reflexion: Language Agents with Verbal Reinforcement Learning about?",
    "output": "Reflexion: Language Agents with Verbal Reinforcement Learning is a 2023 paper in Artificial Intelligence. The implementation is available at noahshinn/reflexion which [neurips 2023] reflexion: language agents with verbal reinforcement learning It is written in Python. The repository has 2,959 stars, indicating high community interest."
  },
  {
    "input": "How popular is the FLAN: Finetuned Language Models Are Zero-Shot Learners implementation?",
    "output": "The google-research/FLAN repository has 1,554 stars and 161 forks on GitHub."
  },
  {
    "input": "Tell me the title of ArXiv paper 2406.07394.",
    "output": "arXiv Query: search_query=&id_list=2406.07394&start=0&max_results=10"
  },
  {
    "input": "Tell me the title of ArXiv paper 2406.09308.",
    "output": "arXiv Query: search_query=&id_list=2406.09308&start=0&max_results=10"
  },
  {
    "input": "How popular is the ChemCrow: Augmenting large-language models with chemistry tools implementation?",
    "output": "The ur-whitelab/chemcrow-public repository has 838 stars and 130 forks on GitHub."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2503.22708&start=0&max_results=10 implementation?",
    "output": "The allenai/codescientist repository has 301 stars and 37 forks on GitHub."
  },
  {
    "input": "Explain the Dream to Control: Learning Behaviors by Latent Imagination paper.",
    "output": "Dream to Control: Learning Behaviors by Latent Imagination is a 2019 paper in Machine Learning. The implementation is available at danijar/dreamer which dream to control: learning behaviors by latent imagination It is written in Python. The repository has 565 stars, indicating high community interest."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2302.14838&start=0&max_results=10 implementation?",
    "output": "The ruiqi-zhong/D5 repository has 71 stars and 13 forks on GitHub."
  },
  {
    "input": "In what year did Denoising Diffusion Implicit Models come out?",
    "output": "The Denoising Diffusion Implicit Models paper was published in 2020."
  },
  {
    "input": "Where can I find the code for ViT: An Image is Worth 16x16 Words?",
    "output": "The code for ViT: An Image is Worth 16x16 Words is available at https://github.com/google-research/vision_transformer (google-research/vision_transformer)."
  },
  {
    "input": "What year was Segment Anything in Medical Images released?",
    "output": "The Segment Anything in Medical Images paper was published in 2023."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 1910.13461?",
    "output": "ALBERT: A Lite BERT"
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2302.08242&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2302.08242&start=0&max_results=10 is available at https://github.com/haoliuhl/language-quantized-autoencoders (haoliuhl/language-quantized-autoencoders)."
  },
  {
    "input": "What year was Densely Connected Convolutional Networks released?",
    "output": "The Densely Connected Convolutional Networks paper was published in 2016."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2301.01181&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2301.01181&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at JohnNay/llm-lobbyist which code for the paper: \"large language models as corporate lobbyists\" (2023). It is written in Jupyter Notebook. The repository has 172 stars, indicating high community interest."
  },
  {
    "input": "What is Lookahead Optimizer: k steps forward, 1 step back about?",
    "output": "Lookahead Optimizer: k steps forward, 1 step back is a 2020 paper in Machine Learning. The implementation is available at alphadl/lookahead.pytorch which lookahead optimizer (lookahead optimizer: k steps forward, 1 step back) for pytorch  It is written in Python. The repository has 337 stars, indicating high community interest."
  },
  {
    "input": "In what year did LoRA: Low-Rank Adaptation of Large Language Models come out?",
    "output": "The LoRA: Low-Rank Adaptation of Large Language Models paper was published in 2021."
  },
  {
    "input": "How many stars does the Attention Is All You Need repository have?",
    "output": "The tensorflow/tensor2tensor repository has 16,760 stars and 3,702 forks on GitHub."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2504.01848&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2504.01848&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at openai/frontier-evals which openai frontier evals It is written in Python. The repository has 949 stars, indicating high community interest."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2303.06865?",
    "output": "arXiv Query: search_query=&id_list=2303.06865&start=0&max_results=10"
  },
  {
    "input": "Where can I find the code for Diffusion Models Beat GANs on Image Synthesis?",
    "output": "The code for Diffusion Models Beat GANs on Image Synthesis is available at https://github.com/openai/guided-diffusion (openai/guided-diffusion)."
  },
  {
    "input": "What are the key topics in the mit-han-lab/offsite-tuning repository?",
    "output": "The mit-han-lab/offsite-tuning repository covers: deep-learning, transfer-learning."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2009.06367&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2009.06367&start=0&max_results=10 paper was published in 2020."
  },
  {
    "input": "What programming language is Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning implemented in?",
    "output": "The Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning implementation is written in Python."
  },
  {
    "input": "How popular is the MMLU: Measuring Massive Multitask Language Understanding implementation?",
    "output": "The hendrycks/test repository has 1,523 stars and 113 forks on GitHub."
  },
  {
    "input": "Describe the GAIR-NLP/LIMO repository that implements arXiv Query: search_query=&id_list=2502.03387&start=0&max_results=10.",
    "output": "[COLM 2025] LIMO: Less is More for Reasoning"
  },
  {
    "input": "What category is the Analyzing the Structure of Attention in a Transformer Language Model paper in?",
    "output": "Analyzing the Structure of Attention in a Transformer Language Model is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2302.14838&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2302.14838&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2502.17437&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2502.17437&start=0&max_results=10 is available at https://github.com/LTH14/fractalgen (LTH14/fractalgen)."
  },
  {
    "input": "Tell me the title of ArXiv paper 1910.01741.",
    "output": "Dream to Control: Learning Behaviors by Latent Imagination"
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2301.09515&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2301.09515&start=0&max_results=10 is available at https://github.com/autonomousvision/stylegan-t (autonomousvision/stylegan-t)."
  },
  {
    "input": "Tell me the title of ArXiv paper 2302.00923.",
    "output": "arXiv Query: search_query=&id_list=2302.00923&start=0&max_results=10"
  },
  {
    "input": "Where can I find the code for Deep Graph Library: A Graph-Centric, Highly-Performant Package?",
    "output": "The code for Deep Graph Library: A Graph-Centric, Highly-Performant Package is available at https://github.com/dmlc/dgl (dmlc/dgl)."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2302.09778&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2302.09778&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at ali-vilab/composer which official implementation of \"composer: creative and controllable image synthesis with composable conditions\" The repository has 1,560 stars, indicating high community interest."
  },
  {
    "input": "What topics does the arXiv Query: search_query=&id_list=2303.02506&start=0&max_results=10 implementation cover?",
    "output": "The NVlabs/prismer repository covers: image-captioning, language-model, multi-modal-learning, multi-task-learning, vision-language-model, vision-and-language, vqa."
  },
  {
    "input": "What year was Longformer: The Long-Document Transformer released?",
    "output": "The Longformer: The Long-Document Transformer paper was published in 2020."
  },
  {
    "input": "Tell me the title of ArXiv paper 2503.01141.",
    "output": "arXiv Query: search_query=&id_list=2503.01141&start=0&max_results=10"
  },
  {
    "input": "How popular is the Time-LLM: Time Series Forecasting by Reprogramming Large Language Models implementation?",
    "output": "The KimMeen/Time-LLM repository has 2,377 stars and 416 forks on GitHub."
  },
  {
    "input": "What are the key topics in the microsoft/JARVIS repository?",
    "output": "The microsoft/JARVIS repository covers: deep-learning, platform, pytorch."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2104.09864?",
    "output": "RoFormer: Enhanced Transformer with Rotary Position Embedding"
  },
  {
    "input": "Explain the WizardCoder: Empowering Code LLMs with Evol-Instruct paper.",
    "output": "WizardCoder: Empowering Code LLMs with Evol-Instruct is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at nlpxucan/WizardLM which llms build upon evol insturct: wizardlm, wizardcoder, wizardmath It is written in Python. The repository has 9,460 stars, indicating high community interest."
  },
  {
    "input": "Summarize the ALBERT: A Lite BERT paper.",
    "output": "ALBERT: A Lite BERT is a 2019 paper in Computation and Language (Natural Language Processing). The implementation is available at google-research/albert which albert: a lite bert for self-supervised learning of language representations It is written in Python. The repository has 3,274 stars, indicating high community interest."
  },
  {
    "input": "What year was Direct Preference Optimization: Your Language Model is a Reward Model released?",
    "output": "The Direct Preference Optimization: Your Language Model is a Reward Model paper was published in 2023."
  },
  {
    "input": "What paper has ArXiv ID 2205.05198?",
    "output": "PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel"
  },
  {
    "input": "When was the Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context paper published?",
    "output": "The Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context paper was published in 2019."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2107.07346?",
    "output": "arXiv Query: search_query=&id_list=2107.07346&start=0&max_results=10"
  },
  {
    "input": "Tell me the title of ArXiv paper 2504.16084.",
    "output": "arXiv Query: search_query=&id_list=2504.16084&start=0&max_results=10"
  },
  {
    "input": "What is the GitHub repository for AWQ: Activation-aware Weight Quantization for LLM Compression?",
    "output": "The code for AWQ: Activation-aware Weight Quantization for LLM Compression is available at https://github.com/mit-han-lab/llm-awq (mit-han-lab/llm-awq)."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2301.00303&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2301.00303&start=0&max_results=10 is a 2022 paper in Machine Learning. The implementation is available at facebookresearch/ConvNeXt-V2 which code release for convnext v2 model It is written in Python. The repository has 1,890 stars, indicating high community interest."
  },
  {
    "input": "What's the implementation repository for Deep Graph Library: A Graph-Centric, Highly-Performant Package?",
    "output": "The code for Deep Graph Library: A Graph-Centric, Highly-Performant Package is available at https://github.com/dmlc/dgl (dmlc/dgl)."
  },
  {
    "input": "Explain the FMInference/FlexLLMGen implementation of arXiv Query: search_query=&id_list=2303.09431&start=0&max_results=10.",
    "output": "Running large language models on a single GPU for throughput-oriented scenarios."
  },
  {
    "input": "Explain the Sequence to Sequence Learning with Neural Networks paper.",
    "output": "Sequence to Sequence Learning with Neural Networks is a 2014 paper in Computation and Language (Natural Language Processing). The implementation is available at farizrahman4u/seq2seq which sequence to sequence learning with keras It is written in Python. The repository has 3,173 stars, indicating high community interest."
  },
  {
    "input": "What year was FLAN: Finetuned Language Models Are Zero-Shot Learners released?",
    "output": "The FLAN: Finetuned Language Models Are Zero-Shot Learners paper was published in 2021."
  },
  {
    "input": "In what year did Codex: Evaluating Large Language Models Trained on Code come out?",
    "output": "The Codex: Evaluating Large Language Models Trained on Code paper was published in 2021."
  },
  {
    "input": "When was the ReAct: Synergizing Reasoning and Acting in Language Models paper published?",
    "output": "The ReAct: Synergizing Reasoning and Acting in Language Models paper was published in 2022."
  },
  {
    "input": "Which field does InstructBLIP: Towards General-purpose Vision-Language Models belong to?",
    "output": "InstructBLIP: Towards General-purpose Vision-Language Models is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "Tell me the title of ArXiv paper 2005.14165.",
    "output": "GPT-3: Language Models are Few-Shot Learners"
  },
  {
    "input": "What's the implementation repository for Self-RAG: Learning to Retrieve, Generate, and Critique?",
    "output": "The code for Self-RAG: Learning to Retrieve, Generate, and Critique is available at https://github.com/AkariAsai/self-rag (AkariAsai/self-rag)."
  },
  {
    "input": "Describe the pyg-team/pytorch_geometric repository that implements PyTorch Geometric: Library for Geometric Deep Learning.",
    "output": "Graph Neural Network Library for PyTorch"
  },
  {
    "input": "Describe the princeton-nlp/SimCSE repository that implements SimCSE: Simple Contrastive Learning of Sentence Embeddings.",
    "output": "[EMNLP 2021] SimCSE: Simple Contrastive Learning of Sentence Embeddings https://arxiv.org/abs/2104.08821"
  },
  {
    "input": "What programming language is FLAN: Finetuned Language Models Are Zero-Shot Learners implemented in?",
    "output": "The FLAN: Finetuned Language Models Are Zero-Shot Learners implementation is written in Python."
  },
  {
    "input": "Explain the eric-mitchell/direct-preference-optimization implementation of Direct Preference Optimization: Your Language Model is a Reward Model.",
    "output": "Reference implementation for DPO (Direct Preference Optimization)"
  },
  {
    "input": "Describe the microsoft/unilm repository that implements BEiT: BERT Pre-Training of Image Transformers.",
    "output": "Large-scale Self-supervised Pre-training Across Tasks, Languages, and Modalities"
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2406.10209&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2406.10209&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is the ArXiv category of Diffusion Models Beat GANs on Image Synthesis?",
    "output": "Diffusion Models Beat GANs on Image Synthesis is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2301.04104&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2301.04104&start=0&max_results=10 is available at https://github.com/google-deepmind/tracr (google-deepmind/tracr)."
  },
  {
    "input": "Explain the meta-llama/llama implementation of LIMA: Less Is More for Alignment.",
    "output": "Inference code for Llama models"
  },
  {
    "input": "Explain the machrisaa/tensorflow-vgg implementation of Very Deep Convolutional Networks for Large-Scale Image Recognition.",
    "output": "VGG19 and VGG16 on Tensorflow"
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2107.07346&start=0&max_results=10 repository have?",
    "output": "The jacopotagliabue/you-dont-need-a-bigger-boat repository has 873 stars and 66 forks on GitHub."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2004.08492&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2004.08492&start=0&max_results=10 is available at https://github.com/uber/orbit (uber/orbit)."
  },
  {
    "input": "What category is the AudioPaLM: A Large Language Model for Speech Understanding paper in?",
    "output": "AudioPaLM: A Large Language Model for Speech Understanding is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What year was Diffusion Models Beat GANs on Image Synthesis released?",
    "output": "The Diffusion Models Beat GANs on Image Synthesis paper was published in 2021."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2410.18050&start=0&max_results=10 implementation?",
    "output": "The ibm-granite/granite-3.0-language-models repository has 267 stars and 28 forks on GitHub."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2007.16122?",
    "output": "arXiv Query: search_query=&id_list=2007.16122&start=0&max_results=10"
  },
  {
    "input": "What programming language is ELECTRA: Pre-training Text Encoders as Discriminators implemented in?",
    "output": "The ELECTRA: Pre-training Text Encoders as Discriminators implementation is written in Python."
  },
  {
    "input": "What is the GitHub repository for ReAct: Synergizing Reasoning and Acting in Language Models?",
    "output": "The code for ReAct: Synergizing Reasoning and Acting in Language Models is available at https://github.com/ysymyth/ReAct (ysymyth/ReAct)."
  },
  {
    "input": "Summarize the Dense Passage Retrieval for Open-Domain Question Answering paper.",
    "output": "Dense Passage Retrieval for Open-Domain Question Answering is a 2020 paper in Computation and Language (Natural Language Processing). The implementation is available at facebookresearch/DPR which dense passage retriever - is a set of tools and models for open domain q&a task. It is written in Python. The repository has 1,847 stars, indicating high community interest."
  },
  {
    "input": "Where can I find the code for SimCSE: Simple Contrastive Learning of Sentence Embeddings?",
    "output": "The code for SimCSE: Simple Contrastive Learning of Sentence Embeddings is available at https://github.com/princeton-nlp/SimCSE (princeton-nlp/SimCSE)."
  },
  {
    "input": "What topics does the LLaVA: Large Language and Vision Assistant implementation cover?",
    "output": "The haotian-liu/LLaVA repository covers: gpt-4, chatbot, chatgpt, llama, multimodal, llava, foundation-models, instruction-tuning, multi-modality, visual-language-learning, llama-2, llama2, vision-language-model."
  },
  {
    "input": "Explain the Horovod: fast and easy distributed deep learning in TensorFlow paper.",
    "output": "Horovod: fast and easy distributed deep learning in TensorFlow is a 2017 paper in Machine Learning. The implementation is available at horovod/horovod which distributed training framework for tensorflow, keras, pytorch, and apache mxnet. It is written in Python. The repository has 14,631 stars, indicating high community interest."
  },
  {
    "input": "What category is the YOLOX: Exceeding YOLO Series in 2021 paper in?",
    "output": "YOLOX: Exceeding YOLO Series in 2021 is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "What paper has ArXiv ID 2005.11401?",
    "output": "RAG: Retrieval-Augmented Generation"
  },
  {
    "input": "Explain the GAIR-NLP/LIMO implementation of arXiv Query: search_query=&id_list=2502.03387&start=0&max_results=10.",
    "output": "[COLM 2025] LIMO: Less is More for Reasoning"
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2406.12824&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2406.12824&start=0&max_results=10 is a 2024 paper in Machine Learning. The implementation is available at hpcaitech/Open-Sora which open-sora: democratizing efficient video production for all It is written in Python. The repository has 27,981 stars, indicating high community interest."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2302.12766&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2302.12766&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at siddk/voltron-robotics which voltron: language-driven representation learning for robotics It is written in Python. The repository has 227 stars, indicating high community interest."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2303.09431&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2303.09431&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at FMInference/FlexLLMGen which running large language models on a single gpu for throughput-oriented scenarios. It is written in Python. The repository has 9,379 stars, indicating high community interest."
  },
  {
    "input": "Explain the BLIP-2: Bootstrapping Language-Image Pre-training paper.",
    "output": "BLIP-2: Bootstrapping Language-Image Pre-training is a 2023 paper in Computer Vision and Pattern Recognition. The implementation is available at salesforce/LAVIS which lavis - a one-stop library for language-vision intelligence It is written in Jupyter Notebook. The repository has 11,040 stars, indicating high community interest."
  },
  {
    "input": "Which field does LLaMA: Open and Efficient Foundation Language Models belong to?",
    "output": "LLaMA: Open and Efficient Foundation Language Models is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Tell me the title of ArXiv paper 2105.05233.",
    "output": "Diffusion Models Beat GANs on Image Synthesis"
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2410.12896&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2410.12896&start=0&max_results=10 is a 2024 paper in Machine Learning. The implementation is available at ibm-granite/granite-3.0-language-models The repository has 267 stars, indicating high community interest."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2301.13688&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2301.13688&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Summarize the RetNet: Retentive Network: A Successor to Transformer paper.",
    "output": "RetNet: Retentive Network: A Successor to Transformer is a 2022 paper in Machine Learning. The implementation is available at microsoft/torchscale which foundation architecture for (m)llms It is written in Python. The repository has 3,121 stars, indicating high community interest."
  },
  {
    "input": "How popular is the SimCSE: Simple Contrastive Learning of Sentence Embeddings implementation?",
    "output": "The princeton-nlp/SimCSE repository has 3,615 stars and 531 forks on GitHub."
  },
  {
    "input": "Give me an overview of Mamba: Linear-Time Sequence Modeling with Selective State Spaces.",
    "output": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces is a 2023 paper in Machine Learning. The implementation is available at state-spaces/mamba which mamba ssm architecture It is written in Python. The repository has 16,525 stars, indicating high community interest."
  },
  {
    "input": "What is SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding about?",
    "output": "SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding is a 2019 paper in Computation and Language (Natural Language Processing). The implementation is available at nyu-mll/jiant which jiant is an nlp toolkit It is written in Python. The repository has 1,672 stars, indicating high community interest."
  },
  {
    "input": "Explain the danijar/dreamer implementation of Dream to Control: Learning Behaviors by Latent Imagination.",
    "output": "Dream to Control: Learning Behaviors by Latent Imagination"
  },
  {
    "input": "What paper has ArXiv ID 2502.03387?",
    "output": "arXiv Query: search_query=&id_list=2502.03387&start=0&max_results=10"
  },
  {
    "input": "What programming language is XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale implemented in?",
    "output": "The XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale implementation is written in Python."
  },
  {
    "input": "Give me an overview of S4: Efficiently Modeling Long Sequences.",
    "output": "S4: Efficiently Modeling Long Sequences is a 2021 paper in Machine Learning. The implementation is available at state-spaces/s4 which structured state space sequence models It is written in Jupyter Notebook. The repository has 2,783 stars, indicating high community interest."
  },
  {
    "input": "What is CLIP: Learning Transferable Visual Models From Natural Language about?",
    "output": "CLIP: Learning Transferable Visual Models From Natural Language is a 2021 paper in Computer Vision and Pattern Recognition. The implementation is available at openai/CLIP which clip (contrastive language-image pretraining),  predict the most relevant text snippet given an image It is written in Jupyter Notebook. The repository has 31,706 stars, indicating high community interest."
  },
  {
    "input": "What programming language is RoBERTa: A Robustly Optimized BERT Pretraining Approach implemented in?",
    "output": "The RoBERTa: A Robustly Optimized BERT Pretraining Approach implementation is written in Python."
  },
  {
    "input": "What does the microsoft/torchscale repository do?",
    "output": "Foundation Architecture for (M)LLMs"
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2303.02506&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2303.02506&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What paper has ArXiv ID 2303.03378?",
    "output": "arXiv Query: search_query=&id_list=2303.03378&start=0&max_results=10"
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2406.09403&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2406.09403&start=0&max_results=10 is a 2024 paper in Machine Learning. The implementation is available at lamini-ai/Lamini-Memory-Tuning which banishing llm hallucinations requires rethinking generalization The repository has 275 stars, indicating high community interest."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2301.04856&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2301.04856&start=0&max_results=10 is available at https://github.com/frankxu2004/knnlm-why (frankxu2004/knnlm-why)."
  },
  {
    "input": "How popular is the Beir: A Heterogeneous Benchmark for IR implementation?",
    "output": "The beir-cellar/beir repository has 2,007 stars and 225 forks on GitHub."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2004.08492&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2004.08492&start=0&max_results=10 paper was published in 2020."
  },
  {
    "input": "How many stars does the RoBERTa: A Robustly Optimized BERT Pretraining Approach repository have?",
    "output": "The facebookresearch/fairseq repository has 31,985 stars and 6,630 forks on GitHub."
  },
  {
    "input": "Tell me the title of ArXiv paper 2106.07447.",
    "output": "HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction"
  },
  {
    "input": "Give me an overview of ReAct: Synergizing Reasoning and Acting in Language Models.",
    "output": "ReAct: Synergizing Reasoning and Acting in Language Models is a 2022 paper in Computation and Language (Natural Language Processing). The implementation is available at ysymyth/ReAct which [iclr 2023] react: synergizing reasoning and acting in language models It is written in Jupyter Notebook. The repository has 3,225 stars, indicating high community interest."
  },
  {
    "input": "How many stars does the Horovod: fast and easy distributed deep learning in TensorFlow repository have?",
    "output": "The horovod/horovod repository has 14,631 stars and 2,255 forks on GitHub."
  },
  {
    "input": "What's the implementation repository for Enriching Word Vectors with Subword Information?",
    "output": "The code for Enriching Word Vectors with Subword Information is available at https://github.com/facebookresearch/fastText (facebookresearch/fastText)."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2502.17437&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2502.17437&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at LTH14/fractalgen which pytorch implementation of fractalgen https://arxiv.org/abs/2502.17437 It is written in Python. The repository has 1,196 stars, indicating high community interest."
  },
  {
    "input": "Explain the LTH14/fractalgen implementation of arXiv Query: search_query=&id_list=2502.17437&start=0&max_results=10.",
    "output": "PyTorch implementation of FractalGen https://arxiv.org/abs/2502.17437"
  },
  {
    "input": "In what year did Decoupled Weight Decay Regularization come out?",
    "output": "The Decoupled Weight Decay Regularization paper was published in 2017."
  },
  {
    "input": "What paper has ArXiv ID 2002.04745?",
    "output": "Lookahead Optimizer: k steps forward, 1 step back"
  },
  {
    "input": "How popular is the ControlNet: Adding Conditional Control to Text-to-Image Diffusion implementation?",
    "output": "The lllyasviel/ControlNet repository has 33,344 stars and 2,987 forks on GitHub."
  },
  {
    "input": "When was the Codex: Evaluating Large Language Models Trained on Code paper published?",
    "output": "The Codex: Evaluating Large Language Models Trained on Code paper was published in 2021."
  },
  {
    "input": "Explain the Direct Preference Optimization: Your Language Model is a Reward Model paper.",
    "output": "Direct Preference Optimization: Your Language Model is a Reward Model is a 2023 paper in Machine Learning. The implementation is available at eric-mitchell/direct-preference-optimization which reference implementation for dpo (direct preference optimization) It is written in Python. The repository has 2,789 stars, indicating high community interest."
  },
  {
    "input": "Tell me the title of ArXiv paper 2012.06009.",
    "output": "arXiv Query: search_query=&id_list=2012.06009&start=0&max_results=10"
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2303.08518&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2303.08518&start=0&max_results=10 is available at https://github.com/FMInference/FlexLLMGen (FMInference/FlexLLMGen)."
  },
  {
    "input": "Explain the XLNet: Generalized Autoregressive Pretraining paper.",
    "output": "XLNet: Generalized Autoregressive Pretraining is a 2019 paper in Computation and Language (Natural Language Processing). The implementation is available at zihangdai/xlnet which xlnet: generalized autoregressive pretraining for language understanding It is written in Python. The repository has 6,179 stars, indicating high community interest."
  },
  {
    "input": "Summarize the TensorFlow: A System for Large-Scale Machine Learning paper.",
    "output": "TensorFlow: A System for Large-Scale Machine Learning is a 2016 paper in Distributed, Parallel, and Cluster Computing. The implementation is available at tensorflow/tensorflow which an open source machine learning framework for everyone It is written in C++. The repository has 192,543 stars, indicating high community interest."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2301.13188&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2301.13188&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at amazon-science/mm-cot which official implementation for \"multimodal chain-of-thought reasoning in language models\" (stay tuned and more will be updated) It is written in Python. The repository has 3,982 stars, indicating high community interest."
  },
  {
    "input": "When was the Efficient Estimation of Word Representations in Vector Space paper published?",
    "output": "The Efficient Estimation of Word Representations in Vector Space paper was published in 2013."
  },
  {
    "input": "Give me an overview of LoRA: Low-Rank Adaptation of Large Language Models.",
    "output": "LoRA: Low-Rank Adaptation of Large Language Models is a 2021 paper in Computation and Language (Natural Language Processing). The implementation is available at microsoft/LoRA which code for loralib, an implementation of \"lora: low-rank adaptation of large language models\" It is written in Python. The repository has 12,979 stars, indicating high community interest."
  },
  {
    "input": "What year was Code Llama: Open Foundation Models for Code released?",
    "output": "The Code Llama: Open Foundation Models for Code paper was published in 2023."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2312.00752?",
    "output": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces"
  },
  {
    "input": "Which field does Generating Wikipedia by Summarizing Long Sequences belong to?",
    "output": "Generating Wikipedia by Summarizing Long Sequences is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What topics does the BLIP: Bootstrapping Language-Image Pre-training implementation cover?",
    "output": "The salesforce/BLIP repository covers: vision-language, vision-and-language-pre-training, image-text-retrieval, image-captioning, visual-question-answering, visual-reasoning, vision-language-transformer."
  },
  {
    "input": "Give me an overview of AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning.",
    "output": "AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning is a 2022 paper in Machine Learning. The implementation is available at QingruZhang/AdaLoRA which adalora: adaptive budget allocation for parameter-efficient fine-tuning (iclr 2023).  It is written in Python. The repository has 361 stars, indicating high community interest."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2006.03511&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2006.03511&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Which field does FlashAttention-2: Faster Attention with Better Parallelism belong to?",
    "output": "FlashAttention-2: Faster Attention with Better Parallelism is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "How many stars does the Densely Connected Convolutional Networks repository have?",
    "output": "The liuzhuang13/DenseNet repository has 4,843 stars and 1,072 forks on GitHub."
  },
  {
    "input": "When was the LIMA: Less Is More for Alignment paper published?",
    "output": "The LIMA: Less Is More for Alignment paper was published in 2023."
  },
  {
    "input": "How popular is the Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks implementation?",
    "output": "The huggingface/sentence-transformers repository has 17,904 stars and 2,717 forks on GitHub."
  },
  {
    "input": "What is Know What You Don't Know: Unanswerable Questions for SQuAD about?",
    "output": "Know What You Don't Know: Unanswerable Questions for SQuAD is a 2018 paper in Computation and Language (Natural Language Processing). The implementation is available at rajpurkar/SQuAD-explorer which visually explore the stanford question answering dataset It is written in JavaScript. The repository has 570 stars, indicating high community interest."
  },
  {
    "input": "What paper has ArXiv ID 2502.19328?",
    "output": "arXiv Query: search_query=&id_list=2502.19328&start=0&max_results=10"
  },
  {
    "input": "What is the ArXiv category of FLAN: Finetuned Language Models Are Zero-Shot Learners?",
    "output": "FLAN: Finetuned Language Models Are Zero-Shot Learners is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "How many stars does the ELECTRA: Pre-training Text Encoders as Discriminators repository have?",
    "output": "The google-research/electra repository has 2,365 stars and 350 forks on GitHub."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2301.10343&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2301.10343&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "When was the Contriever: Unsupervised Dense Information Retrieval paper published?",
    "output": "The Contriever: Unsupervised Dense Information Retrieval paper was published in 2022."
  },
  {
    "input": "What category is the RoFormer: Enhanced Transformer with Rotary Position Embedding paper in?",
    "output": "RoFormer: Enhanced Transformer with Rotary Position Embedding is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What language is the ZhuiyiTechnology/roformer repository written in?",
    "output": "The RoFormer: Enhanced Transformer with Rotary Position Embedding implementation is written in Python."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2303.03378&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2303.03378&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2504.21801&start=0&max_results=10 repository have?",
    "output": "The MoonshotAI/Kimi-Audio repository has 4,367 stars and 316 forks on GitHub."
  },
  {
    "input": "Explain the ruiqi-zhong/D5 implementation of arXiv Query: search_query=&id_list=2303.01469&start=0&max_results=10.",
    "output": "The GitHub repo for Goal Driven Discovery of Distributional Differences via Language Descriptions"
  },
  {
    "input": "Describe the ibm-granite/granite-code-models repository that implements arXiv Query: search_query=&id_list=2405.04324&start=0&max_results=10.",
    "output": "Granite Code Models: A Family of Open Foundation Models for Code Intelligence"
  },
  {
    "input": "What's the implementation repository for Proximal Policy Optimization Algorithms?",
    "output": "The code for Proximal Policy Optimization Algorithms is available at https://github.com/openai/baselines (openai/baselines)."
  },
  {
    "input": "What's the implementation repository for Codex: Evaluating Large Language Models Trained on Code?",
    "output": "The code for Codex: Evaluating Large Language Models Trained on Code is available at https://github.com/openai/human-eval (openai/human-eval)."
  },
  {
    "input": "Describe the AkariAsai/self-rag repository that implements Self-RAG: Learning to Retrieve, Generate, and Critique.",
    "output": "This includes the original implementation of SELF-RAG: Learning to Retrieve, Generate and Critique through self-reflection by Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2504.01848&start=0&max_results=10 repository have?",
    "output": "The openai/frontier-evals repository has 949 stars and 108 forks on GitHub."
  },
  {
    "input": "Summarize the Deep Graph Library: A Graph-Centric, Highly-Performant Package paper.",
    "output": "Deep Graph Library: A Graph-Centric, Highly-Performant Package is a 2020 paper in Machine Learning. The implementation is available at dmlc/dgl which python package built to ease deep learning on graph, on top of existing dl frameworks. It is written in Python. The repository has 14,149 stars, indicating high community interest."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2312.02783&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2312.02783&start=0&max_results=10 is available at https://github.com/ContextualAI/HALOs (ContextualAI/HALOs)."
  },
  {
    "input": "What paper has ArXiv ID 2206.01861?",
    "output": "LLM.int8(): 8-bit Matrix Multiplication for Transformers"
  },
  {
    "input": "What topics does the arXiv Query: search_query=&id_list=2303.08518&start=0&max_results=10 implementation cover?",
    "output": "The FMInference/FlexLLMGen repository covers: deep-learning, gpt-3, high-throughput, large-language-models, machine-learning, offloading, opt."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2310.11511?",
    "output": "Zephyr: Direct Distillation of LM Alignment"
  },
  {
    "input": "What paper has ArXiv ID 2212.10375?",
    "output": "InPars: Data Augmentation for Information Retrieval"
  },
  {
    "input": "In what year did Depthwise Separable Convolutions for Neural Machine Translation come out?",
    "output": "The Depthwise Separable Convolutions for Neural Machine Translation paper was published in 2017."
  },
  {
    "input": "Explain the shizhediao/active-prompt implementation of arXiv Query: search_query=&id_list=2302.12246&start=0&max_results=10.",
    "output": "Source code for the paper \"Active Prompting with Chain-of-Thought for Large Language Models\""
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2006.16779&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2006.16779&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Describe the ibm-granite/granite-code-models repository that implements arXiv Query: search_query=&id_list=2405.02803&start=0&max_results=10.",
    "output": "Granite Code Models: A Family of Open Foundation Models for Code Intelligence"
  },
  {
    "input": "Describe the facebookresearch/DPR repository that implements RAG: Retrieval-Augmented Generation.",
    "output": "Dense Passage Retriever - is a set of tools and models for open domain Q&A task."
  },
  {
    "input": "Describe the google-research/google-research repository that implements Med-PaLM 2: Towards Expert-Level Medical Question Answering.",
    "output": "Google Research"
  },
  {
    "input": "Describe the plurai-ai/intellagent repository that implements arXiv Query: search_query=&id_list=2501.13824&start=0&max_results=10.",
    "output": "A framework for comprehensive diagnosis and optimization of agents using simulated, realistic synthetic interactions"
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2410.18050&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2410.18050&start=0&max_results=10 is available at https://github.com/ibm-granite/granite-3.0-language-models (ibm-granite/granite-3.0-language-models)."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2302.12246&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2302.12246&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at shizhediao/active-prompt which source code for the paper \"active prompting with chain-of-thought for large language models\" It is written in Python. The repository has 247 stars, indicating high community interest."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2504.01848&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2504.01848&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at openai/frontier-evals which openai frontier evals It is written in Python. The repository has 949 stars, indicating high community interest."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2301.05062&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2301.05062&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2301.10343&start=0&max_results=10 implementation?",
    "output": "The autonomousvision/stylegan-t repository has 1,196 stars and 61 forks on GitHub."
  },
  {
    "input": "What is Wide & Deep Learning for Recommender Systems about?",
    "output": "Wide & Deep Learning for Recommender Systems is a 2016 paper in Information Retrieval. The implementation is available at tensorflow/models which models and examples built with tensorflow It is written in Python. The repository has 77,677 stars, indicating high community interest."
  },
  {
    "input": "Tell me the title of ArXiv paper 2502.17437.",
    "output": "arXiv Query: search_query=&id_list=2502.17437&start=0&max_results=10"
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2004.13637&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2004.13637&start=0&max_results=10 paper was published in 2020."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2301.12652&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2301.12652&start=0&max_results=10 is available at https://github.com/amazon-science/mm-cot (amazon-science/mm-cot)."
  },
  {
    "input": "What is the GitHub repository for AudioPaLM: A Large Language Model for Speech Understanding?",
    "output": "The code for AudioPaLM: A Large Language Model for Speech Understanding is available at https://github.com/lucidrains/audiolm-pytorch (lucidrains/audiolm-pytorch)."
  },
  {
    "input": "In what year did RoBERTa: A Robustly Optimized BERT Pretraining Approach come out?",
    "output": "The RoBERTa: A Robustly Optimized BERT Pretraining Approach paper was published in 2019."
  },
  {
    "input": "Explain the Neural Machine Translation by Jointly Learning to Align and Translate paper.",
    "output": "Neural Machine Translation by Jointly Learning to Align and Translate is a 2015 paper in Computation and Language (Natural Language Processing). The implementation is available at lisa-groundhog/GroundHog which library for implementing rnns with theano It is written in Python. The repository has 613 stars, indicating high community interest."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2406.09308&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2406.09308&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is the ArXiv category of ControlNet: Adding Conditional Control to Text-to-Image Diffusion?",
    "output": "ControlNet: Adding Conditional Control to Text-to-Image Diffusion is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "What category is the Lookahead Optimizer: k steps forward, 1 step back paper in?",
    "output": "Lookahead Optimizer: k steps forward, 1 step back is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2102.01192&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2102.01192&start=0&max_results=10 is available at https://github.com/facebookresearch/fairseq (facebookresearch/fairseq)."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2504.16084&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2504.16084&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2406.10209&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2406.10209&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Tell me the title of ArXiv paper 2302.03011.",
    "output": "arXiv Query: search_query=&id_list=2302.03011&start=0&max_results=10"
  },
  {
    "input": "Tell me the title of ArXiv paper 2112.04426.",
    "output": "ColBERT: Efficient Passage Search via Contextualized Late Interaction"
  },
  {
    "input": "Where can I find the code for Adam: A Method for Stochastic Optimization?",
    "output": "The code for Adam: A Method for Stochastic Optimization is available at https://github.com/pytorch/pytorch (pytorch/pytorch)."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2406.07394&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2406.07394&start=0&max_results=10 is a 2024 paper in Machine Learning. The implementation is available at hpcaitech/Open-Sora which open-sora: democratizing efficient video production for all It is written in Python. The repository has 27,981 stars, indicating high community interest."
  },
  {
    "input": "Where can I find the code for Proximal Policy Optimization Algorithms?",
    "output": "The code for Proximal Policy Optimization Algorithms is available at https://github.com/openai/baselines (openai/baselines)."
  },
  {
    "input": "What are the key topics in the openai/CLIP repository?",
    "output": "The openai/CLIP repository covers: deep-learning, machine-learning."
  },
  {
    "input": "What does the MoonshotAI/Kimi-Audio repository do?",
    "output": "Kimi-Audio, an open-source audio foundation model excelling in audio understanding, generation, and conversation"
  },
  {
    "input": "What's the implementation repository for LLaVA-1.5: Improved Baselines with Visual Instruction Tuning?",
    "output": "The code for LLaVA-1.5: Improved Baselines with Visual Instruction Tuning is available at https://github.com/haotian-liu/LLaVA (haotian-liu/LLaVA)."
  },
  {
    "input": "Which field does Playing Atari with Deep Reinforcement Learning belong to?",
    "output": "Playing Atari with Deep Reinforcement Learning is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What language is the meta-llama/llama repository written in?",
    "output": "The LIMA: Less Is More for Alignment implementation is written in Python."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2005.12872?",
    "output": "EfficientDet: Scalable and Efficient Object Detection"
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2502.19328&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2502.19328&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at LTH14/fractalgen which pytorch implementation of fractalgen https://arxiv.org/abs/2502.17437 It is written in Python. The repository has 1,196 stars, indicating high community interest."
  },
  {
    "input": "Describe the tkipf/gcn repository that implements Semi-Supervised Classification with Graph Convolutional Networks.",
    "output": "Implementation of Graph Convolutional Networks in TensorFlow"
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2404.03592&start=0&max_results=10 implementation?",
    "output": "The OpenBMB/Eurus repository has 320 stars and 14 forks on GitHub."
  },
  {
    "input": "What programming language is GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers implemented in?",
    "output": "The GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers implementation is written in Python."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2301.12652&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2301.12652&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "How many stars does the Depthwise Separable Convolutions for Neural Machine Translation repository have?",
    "output": "The google-research/google-research repository has 36,800 stars and 8,255 forks on GitHub."
  },
  {
    "input": "Which field does Contriever: Unsupervised Dense Information Retrieval belong to?",
    "output": "Contriever: Unsupervised Dense Information Retrieval is in the Information Retrieval (cs.IR) category."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2505.22954&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2505.22954&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at ruixin31/Spurious_Rewards It is written in Python. The repository has 342 stars, indicating high community interest."
  },
  {
    "input": "Explain the ContextualAI/HALOs implementation of arXiv Query: search_query=&id_list=2312.02120&start=0&max_results=10.",
    "output": "A library with extensible implementations of DPO, KTO, PPO, ORPO, and other human-aware loss functions (HALOs)."
  },
  {
    "input": "How popular is the LLaMA-Adapter: Efficient Fine-tuning of Language Models implementation?",
    "output": "The OpenGVLab/LLaMA-Adapter repository has 5,923 stars and 383 forks on GitHub."
  },
  {
    "input": "What category is the ELECTRA: Pre-training Text Encoders as Discriminators paper in?",
    "output": "ELECTRA: Pre-training Text Encoders as Discriminators is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2301.11316&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2301.11316&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at autonomousvision/stylegan-t which [icml'23] stylegan-t: unlocking the power of gans for fast large-scale text-to-image synthesis It is written in Python. The repository has 1,196 stars, indicating high community interest."
  },
  {
    "input": "What topics does the MetaGPT: Meta Programming for Multi-Agent Systems implementation cover?",
    "output": "The FoundationAgents/MetaGPT repository covers: agent, gpt, llm, metagpt, multi-agent."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2301.10343&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2301.10343&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "When was the Densely Connected Convolutional Networks paper published?",
    "output": "The Densely Connected Convolutional Networks paper was published in 2016."
  },
  {
    "input": "When was the BERT Rediscovers the Classical NLP Pipeline paper published?",
    "output": "The BERT Rediscovers the Classical NLP Pipeline paper was published in 2019."
  },
  {
    "input": "Describe the lucidrains/audiolm-pytorch repository that implements AudioPaLM: A Large Language Model for Speech Understanding.",
    "output": "Implementation of AudioLM, a SOTA Language Modeling Approach to Audio Generation out of Google Research, in Pytorch"
  },
  {
    "input": "Where can I find the code for Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context?",
    "output": "The code for Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context is available at https://github.com/kimiyoung/transformer-xl (kimiyoung/transformer-xl)."
  },
  {
    "input": "How popular is the Graph Attention Networks implementation?",
    "output": "The PetarV-/GAT repository has 3,460 stars and 673 forks on GitHub."
  },
  {
    "input": "What are the key topics in the IDEA-Research/GroundingDINO repository?",
    "output": "The IDEA-Research/GroundingDINO repository covers: object-detection, open-world, open-world-detection, vision-language, vision-language-transformer."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2303.04671&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2303.04671&start=0&max_results=10 is available at https://github.com/chenfei-wu/TaskMatrix (chenfei-wu/TaskMatrix)."
  },
  {
    "input": "What paper has ArXiv ID 1910.02054?",
    "output": "ZeRO: Memory Optimizations Toward Training Trillion Parameter Models"
  },
  {
    "input": "Explain the Zephyr: Direct Distillation of LM Alignment paper.",
    "output": "Zephyr: Direct Distillation of LM Alignment is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at huggingface/alignment-handbook which robust recipes to align language models with human and ai preferences It is written in Python. The repository has 5,429 stars, indicating high community interest."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2502.03387&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2502.03387&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What is the GitHub repository for Lookahead Optimizer: k steps forward, 1 step back?",
    "output": "The code for Lookahead Optimizer: k steps forward, 1 step back is available at https://github.com/alphadl/lookahead.pytorch (alphadl/lookahead.pytorch)."
  },
  {
    "input": "Summarize the Graph Attention Networks paper.",
    "output": "Graph Attention Networks is a 2017 paper in Machine Learning. The implementation is available at PetarV-/GAT which graph attention networks (https://arxiv.org/abs/1710.10903) It is written in Python. The repository has 3,460 stars, indicating high community interest."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2303.02506&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2303.02506&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2504.21801&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2504.21801&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at MoonshotAI/Kimi-Audio which kimi-audio, an open-source audio foundation model excelling in audio understanding, generation, and conversation It is written in Python. The repository has 4,367 stars, indicating high community interest."
  },
  {
    "input": "What paper has ArXiv ID 2112.10752?",
    "output": "High-Resolution Image Synthesis with Latent Diffusion Models"
  },
  {
    "input": "What paper has ArXiv ID 2104.00355?",
    "output": "arXiv Query: search_query=&id_list=2104.00355&start=0&max_results=10"
  },
  {
    "input": "Where can I find the code for AudioPaLM: A Large Language Model for Speech Understanding?",
    "output": "The code for AudioPaLM: A Large Language Model for Speech Understanding is available at https://github.com/lucidrains/audiolm-pytorch (lucidrains/audiolm-pytorch)."
  },
  {
    "input": "What is the GitHub repository for Segment Anything in Medical Images?",
    "output": "The code for Segment Anything in Medical Images is available at https://github.com/bowang-lab/MedSAM (bowang-lab/MedSAM)."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2302.07459&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2302.07459&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "Which field does ColBERT: Efficient Passage Search via Contextualized Late Interaction belong to?",
    "output": "ColBERT: Efficient Passage Search via Contextualized Late Interaction is in the Information Retrieval (cs.IR) category."
  },
  {
    "input": "What year was RoBERTa: A Robustly Optimized BERT Pretraining Approach released?",
    "output": "The RoBERTa: A Robustly Optimized BERT Pretraining Approach paper was published in 2019."
  },
  {
    "input": "What category is the Finetuned Language Models Are Zero-Shot Learners paper in?",
    "output": "Finetuned Language Models Are Zero-Shot Learners is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Explain the Neural Collaborative Filtering paper.",
    "output": "Neural Collaborative Filtering is a 2017 paper in Information Retrieval. The implementation is available at hexiangnan/neural_collaborative_filtering which neural collaborative filtering It is written in Python. The repository has 1,867 stars, indicating high community interest."
  },
  {
    "input": "What year was Learning to summarize from human feedback released?",
    "output": "The Learning to summarize from human feedback paper was published in 2020."
  },
  {
    "input": "Explain the RetNet: Retentive Network: A Successor to Transformer paper.",
    "output": "RetNet: Retentive Network: A Successor to Transformer is a 2022 paper in Machine Learning. The implementation is available at microsoft/torchscale which foundation architecture for (m)llms It is written in Python. The repository has 3,121 stars, indicating high community interest."
  },
  {
    "input": "What is Sequence to Sequence Learning with Neural Networks about?",
    "output": "Sequence to Sequence Learning with Neural Networks is a 2014 paper in Computation and Language (Natural Language Processing). The implementation is available at farizrahman4u/seq2seq which sequence to sequence learning with keras It is written in Python. The repository has 3,173 stars, indicating high community interest."
  },
  {
    "input": "What language is the google-deepmind/tracr repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2301.05062&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2502.14815&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2502.14815&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2303.08518&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2303.08518&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2009.06367&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2009.06367&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2102.01192&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2102.01192&start=0&max_results=10 is available at https://github.com/facebookresearch/fairseq (facebookresearch/fairseq)."
  },
  {
    "input": "In what year did GPT-3: Language Models are Few-Shot Learners come out?",
    "output": "The GPT-3: Language Models are Few-Shot Learners paper was published in 2020."
  },
  {
    "input": "Tell me the title of ArXiv paper 2502.12115.",
    "output": "arXiv Query: search_query=&id_list=2502.12115&start=0&max_results=10"
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2501.11067&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2501.11067&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at plurai-ai/intellagent which a framework for comprehensive diagnosis and optimization of agents using simulated, realistic synthetic interactions It is written in Python. The repository has 1,148 stars, indicating high community interest."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2502.12115&start=0&max_results=10 repository have?",
    "output": "The Open-Reasoner-Zero/Open-Reasoner-Zero repository has 2,068 stars and 119 forks on GitHub."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2303.04129&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2303.04129&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Tell me the title of ArXiv paper 2007.16122.",
    "output": "arXiv Query: search_query=&id_list=2007.16122&start=0&max_results=10"
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2410.18050&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2410.18050&start=0&max_results=10 is a 2024 paper in Machine Learning. The implementation is available at ibm-granite/granite-3.0-language-models The repository has 267 stars, indicating high community interest."
  },
  {
    "input": "Explain the Deep Graph Library: A Graph-Centric, Highly-Performant Package paper.",
    "output": "Deep Graph Library: A Graph-Centric, Highly-Performant Package is a 2020 paper in Machine Learning. The implementation is available at dmlc/dgl which python package built to ease deep learning on graph, on top of existing dl frameworks. It is written in Python. The repository has 14,149 stars, indicating high community interest."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2005.08100?",
    "output": "Question and Answer Test-Train Overlap in Open-Domain QA Datasets"
  },
  {
    "input": "What are the key topics in the haotian-liu/LLaVA repository?",
    "output": "The haotian-liu/LLaVA repository covers: gpt-4, chatbot, chatgpt, llama, multimodal, llava, foundation-models, instruction-tuning, multi-modality, visual-language-learning, llama-2, llama2, vision-language-model."
  },
  {
    "input": "What paper has ArXiv ID 2502.14815?",
    "output": "arXiv Query: search_query=&id_list=2502.14815&start=0&max_results=10"
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2301.11305&start=0&max_results=10 repository have?",
    "output": "The autonomousvision/stylegan-t repository has 1,196 stars and 61 forks on GitHub."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2007.16122&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2007.16122&start=0&max_results=10 is available at https://github.com/linkedin/gdmix (linkedin/gdmix)."
  },
  {
    "input": "What category is the PyTorch Geometric: Library for Geometric Deep Learning paper in?",
    "output": "PyTorch Geometric: Library for Geometric Deep Learning is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What language is the williamleif/GraphSAGE repository written in?",
    "output": "The Inductive Representation Learning on Large Graphs implementation is written in Python."
  },
  {
    "input": "What is the ArXiv category of InPars: Data Augmentation for Information Retrieval?",
    "output": "InPars: Data Augmentation for Information Retrieval is in the Information Retrieval (cs.IR) category."
  },
  {
    "input": "What does the autonomousvision/stylegan-t repository do?",
    "output": "[ICML'23] StyleGAN-T: Unlocking the Power of GANs for Fast Large-Scale Text-to-Image Synthesis"
  },
  {
    "input": "What is the GitHub repository for Synthetic Data Generation with Large Language Models?",
    "output": "The code for Synthetic Data Generation with Large Language Models is available at https://github.com/google-research/google-research (google-research/google-research)."
  },
  {
    "input": "What category is the AWQ: Activation-aware Weight Quantization for LLM Compression paper in?",
    "output": "AWQ: Activation-aware Weight Quantization for LLM Compression is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Tell me the title of ArXiv paper 2109.07958.",
    "output": "MMLU: Measuring Massive Multitask Language Understanding"
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2303.09431&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2303.09431&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "In what year did Grounding DINO: Marrying DINO with Grounded Pre-Training come out?",
    "output": "The Grounding DINO: Marrying DINO with Grounded Pre-Training paper was published in 2023."
  },
  {
    "input": "Tell me the title of ArXiv paper 2104.08691.",
    "output": "Prefix-Tuning: Optimizing Continuous Prompts for Generation"
  },
  {
    "input": "How popular is the Animate Anyone: Consistent and Controllable Image-to-Video Synthesis implementation?",
    "output": "The HumanAIGC/AnimateAnyone repository has 14,797 stars and 1,008 forks on GitHub."
  },
  {
    "input": "What language is the tensorflow/models repository written in?",
    "output": "The Wide & Deep Learning for Recommender Systems implementation is written in Python."
  },
  {
    "input": "What language is the facebookresearch/fairseq repository written in?",
    "output": "The Wav2Vec 2.0: A Framework for Self-Supervised Learning of Speech implementation is written in Python."
  },
  {
    "input": "In what year did MRKL Systems: A modular approach to AGI come out?",
    "output": "The MRKL Systems: A modular approach to AGI paper was published in 2022."
  },
  {
    "input": "In what year did Semi-Supervised Classification with Graph Convolutional Networks come out?",
    "output": "The Semi-Supervised Classification with Graph Convolutional Networks paper was published in 2016."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2501.11067&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2501.11067&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "Give me an overview of T5: Exploring the Limits of Transfer Learning.",
    "output": "T5: Exploring the Limits of Transfer Learning is a 2019 paper in Computation and Language (Natural Language Processing). The implementation is available at google-research/text-to-text-transfer-transformer which code for the paper \"exploring the limits of transfer learning with a unified text-to-text transformer\" It is written in Python. The repository has 6,453 stars, indicating high community interest."
  },
  {
    "input": "Explain the autonomousvision/stylegan-t implementation of arXiv Query: search_query=&id_list=2301.11316&start=0&max_results=10.",
    "output": "[ICML'23] StyleGAN-T: Unlocking the Power of GANs for Fast Large-Scale Text-to-Image Synthesis"
  },
  {
    "input": "What language is the facebookresearch/TransCoder repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2006.03511&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Which field does SQuAD: 100,000+ Questions for Machine Comprehension of Text belong to?",
    "output": "SQuAD: 100,000+ Questions for Machine Comprehension of Text is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Tell me the title of ArXiv paper 2205.12755.",
    "output": "Beir: A Heterogeneous Benchmark for IR"
  },
  {
    "input": "What is GPT-3: Language Models are Few-Shot Learners about?",
    "output": "GPT-3: Language Models are Few-Shot Learners is a 2020 paper in Computation and Language (Natural Language Processing). The implementation is available at openai/gpt-3 which gpt-3: language models are few-shot learners The repository has 15,784 stars, indicating high community interest."
  },
  {
    "input": "Summarize the Neural Collaborative Filtering paper.",
    "output": "Neural Collaborative Filtering is a 2017 paper in Information Retrieval. The implementation is available at hexiangnan/neural_collaborative_filtering which neural collaborative filtering It is written in Python. The repository has 1,867 stars, indicating high community interest."
  },
  {
    "input": "What is the GitHub repository for Gorilla: Large Language Model Connected with Massive APIs?",
    "output": "The code for Gorilla: Large Language Model Connected with Massive APIs is available at https://github.com/ShishirPatil/gorilla (ShishirPatil/gorilla)."
  },
  {
    "input": "Explain the MAXNORM8650/MedAgentSim implementation of arXiv Query: search_query=&id_list=2503.23513&start=0&max_results=10.",
    "output": "MedAgentSim: Self-Evolving Multi-Agent Simulations for Realistic Clinical Interactions, MICCAI 2025 (oral and early accepted)"
  },
  {
    "input": "What is Swin Transformer: Hierarchical Vision Transformer using Shifted Windows about?",
    "output": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows is a 2021 paper in Computer Vision and Pattern Recognition. The implementation is available at microsoft/Swin-Transformer which this is an official implementation for \"swin transformer: hierarchical vision transformer using shifted windows\". It is written in Python. The repository has 15,466 stars, indicating high community interest."
  },
  {
    "input": "When was the Vicuna: An Open-Source Chatbot paper published?",
    "output": "The Vicuna: An Open-Source Chatbot paper was published in 2023."
  },
  {
    "input": "What is RAG: Retrieval-Augmented Generation about?",
    "output": "RAG: Retrieval-Augmented Generation is a 2020 paper in Computation and Language (Natural Language Processing). The implementation is available at facebookresearch/DPR which dense passage retriever - is a set of tools and models for open domain q&a task. It is written in Python. The repository has 1,847 stars, indicating high community interest."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2302.14233&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2302.14233&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at ruiqi-zhong/D5 which the github repo for goal driven discovery of distributional differences via language descriptions It is written in Python. The repository has 71 stars, indicating high community interest."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2201.12086?",
    "output": "BLIP: Bootstrapping Language-Image Pre-training"
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2503.22678?",
    "output": "arXiv Query: search_query=&id_list=2503.22678&start=0&max_results=10"
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2303.04226?",
    "output": "arXiv Query: search_query=&id_list=2303.04226&start=0&max_results=10"
  },
  {
    "input": "Summarize the LLaVA: Large Language and Vision Assistant paper.",
    "output": "LLaVA: Large Language and Vision Assistant is a 2023 paper in Computer Vision and Pattern Recognition. The implementation is available at haotian-liu/LLaVA which [neurips'23 oral] visual instruction tuning (llava) built towards gpt-4v level capabilities and beyond. It is written in Python. The repository has 24,034 stars, indicating high community interest."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2101.04909&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2101.04909&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "How many stars does the Learning to summarize from human feedback repository have?",
    "output": "The openai/summarize-from-feedback repository has 1,055 stars and 152 forks on GitHub."
  },
  {
    "input": "What language is the facebookresearch/fairseq repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2109.03264&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2109.03264&start=0&max_results=10 implementation?",
    "output": "The facebookresearch/fairseq repository has 31,988 stars and 6,630 forks on GitHub."
  },
  {
    "input": "Explain the YOLOX: Exceeding YOLO Series in 2021 paper.",
    "output": "YOLOX: Exceeding YOLO Series in 2021 is a 2018 paper in Computer Vision and Pattern Recognition. The implementation is available at Megvii-BaseDetection/YOLOX which yolox is a high-performance anchor-free yolo, exceeding yolov3~v5 with megengine, onnx, tensorrt, ncnn, and openvino supported. documentation: https://yolox.readthedocs.io/ It is written in Python. The repository has 10,185 stars, indicating high community interest."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2303.06865&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2303.06865&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at FMInference/FlexLLMGen which running large language models on a single gpu for throughput-oriented scenarios. It is written in Python. The repository has 9,379 stars, indicating high community interest."
  },
  {
    "input": "Explain the ur-whitelab/chemcrow-public implementation of ChemCrow: Augmenting large-language models with chemistry tools.",
    "output": "Chemcrow"
  },
  {
    "input": "Give me an overview of JAX: Composable transformations of Python+NumPy programs.",
    "output": "JAX: Composable transformations of Python+NumPy programs is a 2019 paper in cs.MS. The implementation is available at jax-ml/jax which composable transformations of python+numpy programs: differentiate, vectorize, jit to gpu/tpu, and more It is written in Python. The repository has 34,091 stars, indicating high community interest."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 1808.07042?",
    "output": "Know What You Don't Know: Unanswerable Questions for SQuAD"
  },
  {
    "input": "Give me an overview of Competition-Level Code Generation with AlphaCode.",
    "output": "Competition-Level Code Generation with AlphaCode is a 2022 paper in Machine Learning. The implementation is available at google-deepmind/code_contests It is written in C++. The repository has 2,177 stars, indicating high community interest."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2410.12896&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2410.12896&start=0&max_results=10 is available at https://github.com/ibm-granite/granite-3.0-language-models (ibm-granite/granite-3.0-language-models)."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2403.17887&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2403.17887&start=0&max_results=10 is available at https://github.com/OpenBMB/Eurus (OpenBMB/Eurus)."
  },
  {
    "input": "What year was MetaGPT: Meta Programming for Multi-Agent Systems released?",
    "output": "The MetaGPT: Meta Programming for Multi-Agent Systems paper was published in 2023."
  },
  {
    "input": "Explain the Grounding DINO: Marrying DINO with Grounded Pre-Training paper.",
    "output": "Grounding DINO: Marrying DINO with Grounded Pre-Training is a 2023 paper in Computer Vision and Pattern Recognition. The implementation is available at IDEA-Research/GroundingDINO which [eccv 2024] official implementation of the paper \"grounding dino: marrying dino with grounded pre-training for open-set object detection\" It is written in Python. The repository has 9,324 stars, indicating high community interest."
  },
  {
    "input": "How many stars does the Swin Transformer: Hierarchical Vision Transformer using Shifted Windows repository have?",
    "output": "The microsoft/Swin-Transformer repository has 15,466 stars and 2,198 forks on GitHub."
  },
  {
    "input": "How popular is the Very Deep Convolutional Networks for Large-Scale Image Recognition implementation?",
    "output": "The machrisaa/tensorflow-vgg repository has 2,239 stars and 1,073 forks on GitHub."
  },
  {
    "input": "What is DreamFusion: Text-to-3D using 2D Diffusion about?",
    "output": "DreamFusion: Text-to-3D using 2D Diffusion is a 2022 paper in Computer Vision and Pattern Recognition. The implementation is available at ashawkey/stable-dreamfusion which text-to-3d & image-to-3d & mesh exportation with nerf + diffusion. It is written in Python. The repository has 8,773 stars, indicating high community interest."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2303.01469&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2303.01469&start=0&max_results=10 is available at https://github.com/ruiqi-zhong/D5 (ruiqi-zhong/D5)."
  },
  {
    "input": "Explain the GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers paper.",
    "output": "GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers is a 2022 paper in Machine Learning. The implementation is available at IST-DASLab/gptq which code for the iclr 2023 paper \"gptq: accurate post-training quantization of generative pretrained transformers\". It is written in Python. The repository has 2,221 stars, indicating high community interest."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2410.18050&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2410.18050&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is HuggingGPT: Solving AI Tasks with ChatGPT about?",
    "output": "HuggingGPT: Solving AI Tasks with ChatGPT is a 2023 paper in Artificial Intelligence. The implementation is available at microsoft/JARVIS which jarvis, a system to connect llms with ml community. paper: https://arxiv.org/pdf/2303.17580.pdf It is written in Python. The repository has 24,460 stars, indicating high community interest."
  },
  {
    "input": "What programming language is BEiT: BERT Pre-Training of Image Transformers implemented in?",
    "output": "The BEiT: BERT Pre-Training of Image Transformers implementation is written in Python."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2301.11316&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2301.11316&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at autonomousvision/stylegan-t which [icml'23] stylegan-t: unlocking the power of gans for fast large-scale text-to-image synthesis It is written in Python. The repository has 1,196 stars, indicating high community interest."
  },
  {
    "input": "What year was LLaVA: Large Language and Vision Assistant released?",
    "output": "The LLaVA: Large Language and Vision Assistant paper was published in 2023."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2404.03592&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2404.03592&start=0&max_results=10 is available at https://github.com/OpenBMB/Eurus (OpenBMB/Eurus)."
  },
  {
    "input": "What language is the google-research/albert repository written in?",
    "output": "The ALBERT: A Lite BERT implementation is written in Python."
  },
  {
    "input": "Tell me the title of ArXiv paper 1409.1556.",
    "output": "Very Deep Convolutional Networks for Large-Scale Image Recognition"
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2301.13688&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2301.13688&start=0&max_results=10 is available at https://github.com/amazon-science/mm-cot (amazon-science/mm-cot)."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2502.09992&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2502.09992&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at Open-Reasoner-Zero/Open-Reasoner-Zero which official repo for open-reasoner-zero It is written in Python. The repository has 2,068 stars, indicating high community interest."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2301.13188&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2301.13188&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Explain the haotian-liu/LLaVA implementation of LLaVA: Large Language and Vision Assistant.",
    "output": "[NeurIPS'23 Oral] Visual Instruction Tuning (LLaVA) built towards GPT-4V level capabilities and beyond."
  },
  {
    "input": "How many stars does the Training language models to follow instructions with human feedback repository have?",
    "output": "The openai/following-instructions-human-feedback repository has 1,249 stars and 147 forks on GitHub."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2501.19393&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2501.19393&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at simplescaling/s1 which s1: simple test-time scaling It is written in Python. The repository has 6,605 stars, indicating high community interest."
  },
  {
    "input": "What paper has ArXiv ID 2504.11844?",
    "output": "arXiv Query: search_query=&id_list=2504.11844&start=0&max_results=10"
  },
  {
    "input": "How many stars does the MetaGPT: Meta Programming for Multi-Agent Systems repository have?",
    "output": "The FoundationAgents/MetaGPT repository has 59,669 stars and 7,305 forks on GitHub."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2303.04226&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2303.04226&start=0&max_results=10 is available at https://github.com/HazyResearch/safari (HazyResearch/safari)."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2302.00902&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2302.00902&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What year was Adam: A Method for Stochastic Optimization released?",
    "output": "The Adam: A Method for Stochastic Optimization paper was published in 2014."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2301.01751&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2301.01751&start=0&max_results=10 is available at https://github.com/oughtinc/ice (oughtinc/ice)."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2005.14165?",
    "output": "GPT-3: Language Models are Few-Shot Learners"
  },
  {
    "input": "How many stars does the Neural Collaborative Filtering repository have?",
    "output": "The hexiangnan/neural_collaborative_filtering repository has 1,867 stars and 666 forks on GitHub."
  },
  {
    "input": "What is Synthetic Data Generation with Large Language Models about?",
    "output": "Synthetic Data Generation with Large Language Models is a 2023 paper in Machine Learning. The implementation is available at google-research/google-research which google research It is written in Jupyter Notebook. The repository has 36,800 stars, indicating high community interest."
  },
  {
    "input": "What does the lllyasviel/ControlNet repository do?",
    "output": "Let us control diffusion models!"
  },
  {
    "input": "Which field does Deep Graph Library: A Graph-Centric, Highly-Performant Package belong to?",
    "output": "Deep Graph Library: A Graph-Centric, Highly-Performant Package is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Where can I find the code for Grounding DINO: Marrying DINO with Grounded Pre-Training?",
    "output": "The code for Grounding DINO: Marrying DINO with Grounded Pre-Training is available at https://github.com/IDEA-Research/GroundingDINO (IDEA-Research/GroundingDINO)."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2302.00902&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2302.00902&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What category is the StarCoder: May the source be with you! paper in?",
    "output": "StarCoder: May the source be with you! is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "How popular is the RetNet: Retentive Network: A Successor to Transformer implementation?",
    "output": "The microsoft/torchscale repository has 3,121 stars and 222 forks on GitHub."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2312.02120&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2312.02120&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at ContextualAI/HALOs which a library with extensible implementations of dpo, kto, ppo, orpo, and other human-aware loss functions (halos). It is written in Python. The repository has 893 stars, indicating high community interest."
  },
  {
    "input": "What topics does the arXiv Query: search_query=&id_list=2302.00902&start=0&max_results=10 implementation cover?",
    "output": "The haoliuhl/language-quantized-autoencoders repository covers: autoencoders, bert, large-language-models, multimodal, roberta, vq, vqvae."
  },
  {
    "input": "Give me an overview of Vicuna: An Open-Source Chatbot.",
    "output": "Vicuna: An Open-Source Chatbot is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at lm-sys/FastChat which an open platform for training, serving, and evaluating large language models. release repo for vicuna and chatbot arena. It is written in Python. The repository has 39,271 stars, indicating high community interest."
  },
  {
    "input": "Which field does Learning to summarize from human feedback belong to?",
    "output": "Learning to summarize from human feedback is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What does the google-research-datasets/natural-questions repository do?",
    "output": "Natural Questions (NQ) contains real user questions issued to Google search, and answers found from Wikipedia by annotators. NQ is designed for the training and evaluation of automatic question answering systems."
  },
  {
    "input": "Explain the rbgirshick/py-faster-rcnn implementation of Faster R-CNN: Towards Real-Time Object Detection.",
    "output": "Faster R-CNN (Python implementation) -- see https://github.com/ShaoqingRen/faster_rcnn for the official MATLAB version"
  },
  {
    "input": "What category is the Diffusion Models Beat GANs on Image Synthesis paper in?",
    "output": "Diffusion Models Beat GANs on Image Synthesis is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "When was the LLM.int8(): 8-bit Matrix Multiplication for Transformers paper published?",
    "output": "The LLM.int8(): 8-bit Matrix Multiplication for Transformers paper was published in 2022."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2102.01192&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2102.01192&start=0&max_results=10 paper was published in 2021."
  },
  {
    "input": "Give me an overview of LLaVA: Large Language and Vision Assistant.",
    "output": "LLaVA: Large Language and Vision Assistant is a 2023 paper in Computer Vision and Pattern Recognition. The implementation is available at haotian-liu/LLaVA which [neurips'23 oral] visual instruction tuning (llava) built towards gpt-4v level capabilities and beyond. It is written in Python. The repository has 24,034 stars, indicating high community interest."
  },
  {
    "input": "Give me an overview of Denoising Diffusion Implicit Models.",
    "output": "Denoising Diffusion Implicit Models is a 2020 paper in Machine Learning. The implementation is available at ermongroup/ddim which denoising diffusion implicit models It is written in Python. The repository has 1,743 stars, indicating high community interest."
  },
  {
    "input": "What year was DreamFusion: Text-to-3D using 2D Diffusion released?",
    "output": "The DreamFusion: Text-to-3D using 2D Diffusion paper was published in 2022."
  },
  {
    "input": "What does the facebookresearch/fairseq repository do?",
    "output": "Facebook AI Research Sequence-to-Sequence Toolkit written in Python."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2505.23006&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2505.23006&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What are the key topics in the lucidrains/audiolm-pytorch repository?",
    "output": "The lucidrains/audiolm-pytorch repository covers: artificial-intelligence, attention-mechanisms, audio-synthesis, deep-learning, transformers."
  },
  {
    "input": "Which field does Reflexion: Language Agents with Verbal Reinforcement Learning belong to?",
    "output": "Reflexion: Language Agents with Verbal Reinforcement Learning is in the Artificial Intelligence (cs.AI) category."
  },
  {
    "input": "When was the Denoising Diffusion Probabilistic Models paper published?",
    "output": "The Denoising Diffusion Probabilistic Models paper was published in 2020."
  },
  {
    "input": "Where can I find the code for LLaMA-Adapter: Efficient Fine-tuning of Language Models?",
    "output": "The code for LLaMA-Adapter: Efficient Fine-tuning of Language Models is available at https://github.com/OpenGVLab/LLaMA-Adapter (OpenGVLab/LLaMA-Adapter)."
  },
  {
    "input": "Explain the IDEA-Research/GroundingDINO implementation of Grounding DINO: Marrying DINO with Grounded Pre-Training.",
    "output": "[ECCV 2024] Official implementation of the paper \"Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection\""
  },
  {
    "input": "What is the title of the paper with ArXiv ID 1706.03762?",
    "output": "Attention Is All You Need"
  },
  {
    "input": "How popular is the RoBERTa: A Robustly Optimized BERT Pretraining Approach implementation?",
    "output": "The facebookresearch/fairseq repository has 31,985 stars and 6,630 forks on GitHub."
  },
  {
    "input": "What are the key topics in the KimMeen/Time-LLM repository?",
    "output": "The KimMeen/Time-LLM repository covers: cross-modal-learning, cross-modality, deep-learning, language-model, large-language-models, machine-learning, multimodal-deep-learning, multimodal-time-series, prompt-tuning, time-series, time-series-analysis, time-series-forecast, time-series-forecasting."
  },
  {
    "input": "Explain the hexiangnan/neural_collaborative_filtering implementation of Neural Collaborative Filtering.",
    "output": "Neural Collaborative Filtering"
  },
  {
    "input": "What year was Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning released?",
    "output": "The Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning paper was published in 2018."
  },
  {
    "input": "What programming language is Learning to summarize from human feedback implemented in?",
    "output": "The Learning to summarize from human feedback implementation is written in Python."
  },
  {
    "input": "Where can I find the code for Med-PaLM 2: Towards Expert-Level Medical Question Answering?",
    "output": "The code for Med-PaLM 2: Towards Expert-Level Medical Question Answering is available at https://github.com/google-research/google-research (google-research/google-research)."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2406.10209&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2406.10209&start=0&max_results=10 is a 2024 paper in Machine Learning. The implementation is available at hpcaitech/Open-Sora which open-sora: democratizing efficient video production for all It is written in Python. The repository has 27,981 stars, indicating high community interest."
  },
  {
    "input": "Explain the Longformer: The Long-Document Transformer paper.",
    "output": "Longformer: The Long-Document Transformer is a 2020 paper in Computation and Language (Natural Language Processing). The implementation is available at allenai/longformer which longformer: the long-document transformer It is written in Python. The repository has 2,175 stars, indicating high community interest."
  },
  {
    "input": "What language is the Vision-CAIR/MiniGPT-4 repository written in?",
    "output": "The MiniGPT-4: Enhancing Vision-Language Understanding implementation is written in Python."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2301.05062&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2301.05062&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Explain the tatp22/linformer-pytorch implementation of Linformer: Self-Attention with Linear Complexity.",
    "output": "My take on a practical implementation of Linformer for Pytorch."
  },
  {
    "input": "How many stars does the MiniGPT-4: Enhancing Vision-Language Understanding repository have?",
    "output": "The Vision-CAIR/MiniGPT-4 repository has 25,756 stars and 2,934 forks on GitHub."
  },
  {
    "input": "What paper has ArXiv ID 2302.04870?",
    "output": "arXiv Query: search_query=&id_list=2302.04870&start=0&max_results=10"
  },
  {
    "input": "Summarize the BERT Rediscovers the Classical NLP Pipeline paper.",
    "output": "BERT Rediscovers the Classical NLP Pipeline is a 2019 paper in Computation and Language (Natural Language Processing). The implementation is available at clarkkev/attention-analysis It is written in Jupyter Notebook. The repository has 469 stars, indicating high community interest."
  },
  {
    "input": "What are the key topics in the FMInference/FlexLLMGen repository?",
    "output": "The FMInference/FlexLLMGen repository covers: deep-learning, gpt-3, high-throughput, large-language-models, machine-learning, offloading, opt."
  },
  {
    "input": "Describe the microsoft/LoRA repository that implements LoRA: Low-Rank Adaptation of Large Language Models.",
    "output": "Code for loralib, an implementation of \"LoRA: Low-Rank Adaptation of Large Language Models\""
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2406.10209&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2406.10209&start=0&max_results=10 paper was published in 2024."
  },
  {
    "input": "Describe the oughtinc/ice repository that implements arXiv Query: search_query=&id_list=2301.01751&start=0&max_results=10.",
    "output": "Interactive Composition Explorer: a debugger for compositional language model programs"
  },
  {
    "input": "Where can I find the code for Vicuna: An Open-Source Chatbot?",
    "output": "The code for Vicuna: An Open-Source Chatbot is available at https://github.com/lm-sys/FastChat (lm-sys/FastChat)."
  },
  {
    "input": "What's the implementation repository for Learning to summarize from human feedback?",
    "output": "The code for Learning to summarize from human feedback is available at https://github.com/openai/summarize-from-feedback (openai/summarize-from-feedback)."
  },
  {
    "input": "What language is the google-research-datasets/natural-questions repository written in?",
    "output": "The Question and Answer Test-Train Overlap in Open-Domain QA Datasets implementation is written in Python."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2012.06009&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2012.06009&start=0&max_results=10 is a 2020 paper in Machine Learning. The implementation is available at facebookresearch/CovidPrognosis which covid deterioration prediction based on chest x-ray radiographs via moco-trained image representations It is written in Python. The repository has 158 stars, indicating high community interest."
  },
  {
    "input": "Describe the MoonshotAI/Kimi-Audio repository that implements arXiv Query: search_query=&id_list=2504.19413&start=0&max_results=10.",
    "output": "Kimi-Audio, an open-source audio foundation model excelling in audio understanding, generation, and conversation"
  },
  {
    "input": "What's the implementation repository for An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale?",
    "output": "The code for An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale is available at https://github.com/google-research/vision_transformer (google-research/vision_transformer)."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2301.01947&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2301.01947&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at oughtinc/ice which interactive composition explorer: a debugger for compositional language model programs It is written in Python. The repository has 563 stars, indicating high community interest."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2302.04870&start=0&max_results=10 implementation?",
    "output": "The mit-han-lab/offsite-tuning repository has 382 stars and 39 forks on GitHub."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2301.00774&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2301.00774&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Tell me the title of ArXiv paper 2501.19393.",
    "output": "arXiv Query: search_query=&id_list=2501.19393&start=0&max_results=10"
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2503.22708?",
    "output": "arXiv Query: search_query=&id_list=2503.22708&start=0&max_results=10"
  },
  {
    "input": "Explain the haotian-liu/LLaVA implementation of LLaVA-1.5: Improved Baselines with Visual Instruction Tuning.",
    "output": "[NeurIPS'23 Oral] Visual Instruction Tuning (LLaVA) built towards GPT-4V level capabilities and beyond."
  },
  {
    "input": "What are the key topics in the microsoft/unilm repository?",
    "output": "The microsoft/unilm repository covers: nlp, pre-trained-model, unilm, minilm, layoutlm, layoutxlm, beit, document-ai, trocr, beit-3, foundation-models, xlm-e, deepnet, llm, multimodal, mllm, kosmos, kosmos-1, textdiffuser, bitnet."
  },
  {
    "input": "Tell me the title of ArXiv paper 2309.10305.",
    "output": "WizardCoder: Empowering Code LLMs with Evol-Instruct"
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2501.19393&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2501.19393&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2308.12950?",
    "output": "Code Llama: Open Foundation Models for Code"
  },
  {
    "input": "How popular is the LLM.int8(): 8-bit Matrix Multiplication for Transformers implementation?",
    "output": "The bitsandbytes-foundation/bitsandbytes repository has 7,771 stars and 797 forks on GitHub."
  },
  {
    "input": "Describe the zetaalphavector/InPars repository that implements InPars: Data Augmentation for Information Retrieval.",
    "output": "Inquisitive Parrots for Search"
  },
  {
    "input": "What is the ArXiv category of Beir: A Heterogeneous Benchmark for IR?",
    "output": "Beir: A Heterogeneous Benchmark for IR is in the Information Retrieval (cs.IR) category."
  },
  {
    "input": "What is Distilling the Knowledge in a Neural Network about?",
    "output": "Distilling the Knowledge in a Neural Network is a 2015 paper in Machine Learning. The implementation is available at haitongli/knowledge-distillation-pytorch which a pytorch implementation for exploring deep and shallow knowledge distillation (kd) experiments with flexibility It is written in Python. The repository has 1,973 stars, indicating high community interest."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2312.02783&start=0&max_results=10 repository have?",
    "output": "The ContextualAI/HALOs repository has 893 stars and 49 forks on GitHub."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2404.03592&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2404.03592&start=0&max_results=10 is a 2024 paper in Machine Learning. The implementation is available at OpenBMB/Eurus It is written in Python. The repository has 320 stars, indicating high community interest."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2502.03387&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2502.03387&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at GAIR-NLP/LIMO which [colm 2025] limo: less is more for reasoning It is written in Python. The repository has 1,052 stars, indicating high community interest."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2004.08492&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2004.08492&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Tell me the title of ArXiv paper 2112.10752.",
    "output": "High-Resolution Image Synthesis with Latent Diffusion Models"
  },
  {
    "input": "What is the GitHub repository for WizardCoder: Empowering Code LLMs with Evol-Instruct?",
    "output": "The code for WizardCoder: Empowering Code LLMs with Evol-Instruct is available at https://github.com/nlpxucan/WizardLM (nlpxucan/WizardLM)."
  },
  {
    "input": "What topics does the TensorFlow: A System for Large-Scale Machine Learning implementation cover?",
    "output": "The tensorflow/tensorflow repository covers: tensorflow, machine-learning, python, deep-learning, deep-neural-networks, neural-network, ml, distributed."
  },
  {
    "input": "When was the Lookahead Optimizer: k steps forward, 1 step back paper published?",
    "output": "The Lookahead Optimizer: k steps forward, 1 step back paper was published in 2020."
  },
  {
    "input": "What is the ArXiv category of LIMA: Less Is More for Alignment?",
    "output": "LIMA: Less Is More for Alignment is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Explain the RAG: Retrieval-Augmented Generation paper.",
    "output": "RAG: Retrieval-Augmented Generation is a 2020 paper in Computation and Language (Natural Language Processing). The implementation is available at facebookresearch/DPR which dense passage retriever - is a set of tools and models for open domain q&a task. It is written in Python. The repository has 1,847 stars, indicating high community interest."
  },
  {
    "input": "What's the implementation repository for PyTorch: An Imperative Style, High-Performance Deep Learning Library?",
    "output": "The code for PyTorch: An Imperative Style, High-Performance Deep Learning Library is available at https://github.com/pytorch/pytorch (pytorch/pytorch)."
  },
  {
    "input": "When was the AWQ: Activation-aware Weight Quantization for LLM Compression paper published?",
    "output": "The AWQ: Activation-aware Weight Quantization for LLM Compression paper was published in 2023."
  },
  {
    "input": "Explain the openai/frontier-evals implementation of arXiv Query: search_query=&id_list=2504.01848&start=0&max_results=10.",
    "output": "OpenAI Frontier Evals"
  },
  {
    "input": "What is the ArXiv category of MiniGPT-4: Enhancing Vision-Language Understanding?",
    "output": "MiniGPT-4: Enhancing Vision-Language Understanding is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "What's the implementation repository for Lookahead Optimizer: k steps forward, 1 step back?",
    "output": "The code for Lookahead Optimizer: k steps forward, 1 step back is available at https://github.com/alphadl/lookahead.pytorch (alphadl/lookahead.pytorch)."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2303.04671&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2303.04671&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What's the implementation repository for ZeRO: Memory Optimizations Toward Training Trillion Parameter Models?",
    "output": "The code for ZeRO: Memory Optimizations Toward Training Trillion Parameter Models is available at https://github.com/deepspeedai/DeepSpeed (deepspeedai/DeepSpeed)."
  },
  {
    "input": "What topics does the InstructBLIP: Towards General-purpose Vision-Language Models implementation cover?",
    "output": "The salesforce/LAVIS repository covers: deep-learning, deep-learning-library, image-captioning, salesforce, vision-and-language, vision-framework, vision-language-pretraining, vision-language-transformer, visual-question-anwsering, multimodal-datasets, multimodal-deep-learning."
  },
  {
    "input": "What paper has ArXiv ID 1710.10903?",
    "output": "Graph Attention Networks"
  },
  {
    "input": "What paper has ArXiv ID 1301.3781?",
    "output": "Efficient Estimation of Word Representations in Vector Space"
  },
  {
    "input": "What topics does the arXiv Query: search_query=&id_list=2109.03264&start=0&max_results=10 implementation cover?",
    "output": "The facebookresearch/fairseq repository covers: python, pytorch, artificial-intelligence."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2504.00698&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2504.00698&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "Explain the NVlabs/prismer implementation of arXiv Query: search_query=&id_list=2303.03378&start=0&max_results=10.",
    "output": "The implementation of \"Prismer: A Vision-Language Model with Multi-Task Experts\"."
  },
  {
    "input": "What is the ArXiv category of GPT-4 Technical Report?",
    "output": "GPT-4 Technical Report is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Describe the sentient-agi/OpenDeepSearch repository that implements arXiv Query: search_query=&id_list=2503.20201&start=0&max_results=10.",
    "output": "SOTA search powered LLM"
  },
  {
    "input": "What is the ArXiv category of BLIP: Bootstrapping Language-Image Pre-training?",
    "output": "BLIP: Bootstrapping Language-Image Pre-training is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "What programming language is Densely Connected Convolutional Networks implemented in?",
    "output": "The Densely Connected Convolutional Networks implementation is written in Lua."
  },
  {
    "input": "What does the FMInference/FlexLLMGen repository do?",
    "output": "Running large language models on a single GPU for throughput-oriented scenarios."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2302.08242&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2302.08242&start=0&max_results=10 is available at https://github.com/haoliuhl/language-quantized-autoencoders (haoliuhl/language-quantized-autoencoders)."
  },
  {
    "input": "What is the ArXiv category of Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks?",
    "output": "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "When was the Time-LLM: Time Series Forecasting by Reprogramming Large Language Models paper published?",
    "output": "The Time-LLM: Time Series Forecasting by Reprogramming Large Language Models paper was published in 2023."
  },
  {
    "input": "What is LLaMA: Open and Efficient Foundation Language Models about?",
    "output": "LLaMA: Open and Efficient Foundation Language Models is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at meta-llama/llama which inference code for llama models It is written in Python. The repository has 58,946 stars, indicating high community interest."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2406.09403&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2406.09403&start=0&max_results=10 paper was published in 2024."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2301.00808&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2301.00808&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What does the google/automl repository do?",
    "output": "Google Brain AutoML"
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2301.01947&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2301.01947&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What's the implementation repository for Zephyr: Direct Distillation of LM Alignment?",
    "output": "The code for Zephyr: Direct Distillation of LM Alignment is available at https://github.com/huggingface/alignment-handbook (huggingface/alignment-handbook)."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2303.04671&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2303.04671&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2301.12597?",
    "output": "BLIP-2: Bootstrapping Language-Image Pre-training"
  },
  {
    "input": "What paper has ArXiv ID 2306.05685?",
    "output": "Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena"
  },
  {
    "input": "When was the Learning to summarize from human feedback paper published?",
    "output": "The Learning to summarize from human feedback paper was published in 2020."
  },
  {
    "input": "Which field does Decoupled Weight Decay Regularization belong to?",
    "output": "Decoupled Weight Decay Regularization is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Explain the Open-Reasoner-Zero/Open-Reasoner-Zero implementation of arXiv Query: search_query=&id_list=2502.14815&start=0&max_results=10.",
    "output": "Official Repo for Open-Reasoner-Zero"
  },
  {
    "input": "Where can I find the code for WizardCoder: Empowering Code LLMs with Evol-Instruct?",
    "output": "The code for WizardCoder: Empowering Code LLMs with Evol-Instruct is available at https://github.com/nlpxucan/WizardLM (nlpxucan/WizardLM)."
  },
  {
    "input": "Describe the rail-berkeley/softlearning repository that implements Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning.",
    "output": "Softlearning is a reinforcement learning framework for training maximum entropy policies in continuous domains. Includes the official implementation of the Soft Actor-Critic algorithm."
  },
  {
    "input": "What paper has ArXiv ID 2304.08485?",
    "output": "MiniGPT-4: Enhancing Vision-Language Understanding"
  },
  {
    "input": "What paper has ArXiv ID 1903.07293?",
    "output": "PyTorch Geometric: Library for Geometric Deep Learning"
  },
  {
    "input": "What language is the facebookresearch/fairseq repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2104.00355&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "How many stars does the Dream to Control: Learning Behaviors by Latent Imagination repository have?",
    "output": "The danijar/dreamer repository has 565 stars and 113 forks on GitHub."
  },
  {
    "input": "Tell me the title of ArXiv paper 2301.00704.",
    "output": "arXiv Query: search_query=&id_list=2301.00704&start=0&max_results=10"
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2302.00902&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2302.00902&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at haoliuhl/language-quantized-autoencoders which language quantized autoencoders It is written in Python. The repository has 111 stars, indicating high community interest."
  },
  {
    "input": "What category is the Longformer: The Long-Document Transformer paper in?",
    "output": "Longformer: The Long-Document Transformer is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 1901.02860?",
    "output": "Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context"
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2501.19393&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2501.19393&start=0&max_results=10 is available at https://github.com/simplescaling/s1 (simplescaling/s1)."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 1409.1556?",
    "output": "Very Deep Convolutional Networks for Large-Scale Image Recognition"
  },
  {
    "input": "Which field does RAG: Retrieval-Augmented Generation belong to?",
    "output": "RAG: Retrieval-Augmented Generation is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Tell me the title of ArXiv paper 2006.16779.",
    "output": "arXiv Query: search_query=&id_list=2006.16779&start=0&max_results=10"
  },
  {
    "input": "What paper has ArXiv ID 2310.11511?",
    "output": "Zephyr: Direct Distillation of LM Alignment"
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2502.09992&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2502.09992&start=0&max_results=10 is available at https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero (Open-Reasoner-Zero/Open-Reasoner-Zero)."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2303.03846&start=0&max_results=10 implementation?",
    "output": "The HazyResearch/safari repository has 904 stars and 70 forks on GitHub."
  },
  {
    "input": "What language is the jacopotagliabue/you-dont-need-a-bigger-boat repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2107.07346&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What year was ELECTRA: Pre-training Text Encoders as Discriminators released?",
    "output": "The ELECTRA: Pre-training Text Encoders as Discriminators paper was published in 2019."
  },
  {
    "input": "What are the key topics in the huawei-noah/Pretrained-Language-Model repository?",
    "output": "The huawei-noah/Pretrained-Language-Model repository covers: knowledge-distillation, model-compression, quantization, pretrained-models, large-scale-distributed."
  },
  {
    "input": "What are the key topics in the facebookresearch/fairseq repository?",
    "output": "The facebookresearch/fairseq repository covers: python, pytorch, artificial-intelligence."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2301.04856&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2301.04856&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2301.00774&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2301.00774&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at facebookresearch/ConvNeXt-V2 which code release for convnext v2 model It is written in Python. The repository has 1,890 stars, indicating high community interest."
  },
  {
    "input": "Which field does LoRA: Low-Rank Adaptation of Large Language Models belong to?",
    "output": "LoRA: Low-Rank Adaptation of Large Language Models is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Explain the facebookresearch/TransCoder implementation of arXiv Query: search_query=&id_list=2004.13637&start=0&max_results=10.",
    "output": "Public release of the TransCoder research project https://arxiv.org/pdf/2006.03511.pdf"
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2503.22708&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2503.22708&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 1312.5602?",
    "output": "Playing Atari with Deep Reinforcement Learning"
  },
  {
    "input": "Describe the openai/baselines repository that implements Proximal Policy Optimization Algorithms.",
    "output": "OpenAI Baselines: high-quality implementations of reinforcement learning algorithms"
  },
  {
    "input": "What's the implementation repository for Graph Attention Networks?",
    "output": "The code for Graph Attention Networks is available at https://github.com/PetarV-/GAT (PetarV-/GAT)."
  },
  {
    "input": "Where can I find the code for Finetuned Language Models Are Zero-Shot Learners?",
    "output": "The code for Finetuned Language Models Are Zero-Shot Learners is available at https://github.com/google-research/FLAN (google-research/FLAN)."
  },
  {
    "input": "How popular is the ViT: An Image is Worth 16x16 Words implementation?",
    "output": "The google-research/vision_transformer repository has 12,040 stars and 1,432 forks on GitHub."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2503.00735&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2503.00735&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2503.20201&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2503.20201&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at sentient-agi/OpenDeepSearch which sota search powered llm It is written in Python. The repository has 3,723 stars, indicating high community interest."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2009.01325?",
    "output": "Learning to summarize from human feedback"
  },
  {
    "input": "Where can I find the code for The Flan Collection: Designing Data and Methods for Effective Instruction Tuning?",
    "output": "The code for The Flan Collection: Designing Data and Methods for Effective Instruction Tuning is available at https://github.com/google-research/FLAN (google-research/FLAN)."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2301.01751&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2301.01751&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Describe the NVlabs/prismer repository that implements arXiv Query: search_query=&id_list=2303.03378&start=0&max_results=10.",
    "output": "The implementation of \"Prismer: A Vision-Language Model with Multi-Task Experts\"."
  },
  {
    "input": "What language is the liuzhuang13/DenseNet repository written in?",
    "output": "The Densely Connected Convolutional Networks implementation is written in Lua."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 1506.02640?",
    "output": "Faster R-CNN: Towards Real-Time Object Detection"
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2302.12766&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2302.12766&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at siddk/voltron-robotics which voltron: language-driven representation learning for robotics It is written in Python. The repository has 227 stars, indicating high community interest."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2301.11316&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2301.11316&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What language is the pytorch/pytorch repository written in?",
    "output": "The Decoupled Weight Decay Regularization implementation is written in Python."
  },
  {
    "input": "In what year did ALBERT: A Lite BERT come out?",
    "output": "The ALBERT: A Lite BERT paper was published in 2019."
  },
  {
    "input": "What does the hexiangnan/neural_collaborative_filtering repository do?",
    "output": "Neural Collaborative Filtering"
  },
  {
    "input": "Explain the QingruZhang/AdaLoRA implementation of AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning.",
    "output": "AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning (ICLR 2023). "
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2107.07346&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2107.07346&start=0&max_results=10 is available at https://github.com/jacopotagliabue/you-dont-need-a-bigger-boat (jacopotagliabue/you-dont-need-a-bigger-boat)."
  },
  {
    "input": "What are the key topics in the mit-han-lab/offsite-tuning repository?",
    "output": "The mit-han-lab/offsite-tuning repository covers: deep-learning, transfer-learning."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2102.01192&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2102.01192&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2312.02120&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2312.02120&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2302.03011&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2302.03011&start=0&max_results=10 is available at https://github.com/mit-han-lab/offsite-tuning (mit-han-lab/offsite-tuning)."
  },
  {
    "input": "What is PaLM: Scaling Language Modeling with Pathways about?",
    "output": "PaLM: Scaling Language Modeling with Pathways is a 2022 paper in Computation and Language (Natural Language Processing). The implementation is available at lucidrains/PaLM-pytorch which implementation of the specific transformer architecture from palm - scaling language modeling with pathways It is written in Python. The repository has 826 stars, indicating high community interest."
  },
  {
    "input": "What is the ArXiv category of PaLM: Scaling Language Modeling with Pathways?",
    "output": "PaLM: Scaling Language Modeling with Pathways is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What category is the BEiT: BERT Pre-Training of Image Transformers paper in?",
    "output": "BEiT: BERT Pre-Training of Image Transformers is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "Tell me the title of ArXiv paper 2107.07346.",
    "output": "arXiv Query: search_query=&id_list=2107.07346&start=0&max_results=10"
  },
  {
    "input": "What programming language is Reformer: The Efficient Transformer implemented in?",
    "output": "The Reformer: The Efficient Transformer implementation is written in Python."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2302.03011&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2302.03011&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What is the GitHub repository for RetNet: Retentive Network: A Successor to Transformer?",
    "output": "The code for RetNet: Retentive Network: A Successor to Transformer is available at https://github.com/microsoft/torchscale (microsoft/torchscale)."
  },
  {
    "input": "What paper has ArXiv ID 1706.03059?",
    "output": "Depthwise Separable Convolutions for Neural Machine Translation"
  },
  {
    "input": "What does the amazon-science/mm-cot repository do?",
    "output": "Official implementation for \"Multimodal Chain-of-Thought Reasoning in Language Models\" (stay tuned and more will be updated)"
  },
  {
    "input": "Which field does Lookahead Optimizer: k steps forward, 1 step back belong to?",
    "output": "Lookahead Optimizer: k steps forward, 1 step back is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What language is the facebookresearch/ConvNeXt-V2 repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2301.00808&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2004.13637&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2004.13637&start=0&max_results=10 paper was published in 2020."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2502.19328&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2502.19328&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What are the key topics in the haoliuhl/language-quantized-autoencoders repository?",
    "output": "The haoliuhl/language-quantized-autoencoders repository covers: autoencoders, bert, large-language-models, multimodal, roberta, vq, vqvae."
  },
  {
    "input": "What language is the microsoft/torchscale repository written in?",
    "output": "The RetNet: Retentive Network: A Successor to Transformer implementation is written in Python."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2505.22954&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2505.22954&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at ruixin31/Spurious_Rewards It is written in Python. The repository has 342 stars, indicating high community interest."
  },
  {
    "input": "What programming language is Attention Is All You Need implemented in?",
    "output": "The Attention Is All You Need implementation is written in Python."
  },
  {
    "input": "How popular is the Code Llama: Open Foundation Models for Code implementation?",
    "output": "The meta-llama/codellama repository has 16,365 stars and 1,946 forks on GitHub."
  },
  {
    "input": "What language is the plurai-ai/intellagent repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2501.11067&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What does the HazyResearch/safari repository do?",
    "output": "Convolutions for Sequence Modeling"
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2302.00923&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2302.00923&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "Give me an overview of GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers.",
    "output": "GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers is a 2022 paper in Machine Learning. The implementation is available at IST-DASLab/gptq which code for the iclr 2023 paper \"gptq: accurate post-training quantization of generative pretrained transformers\". It is written in Python. The repository has 2,221 stars, indicating high community interest."
  },
  {
    "input": "Give me an overview of Gorilla: Large Language Model Connected with Massive APIs.",
    "output": "Gorilla: Large Language Model Connected with Massive APIs is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at ShishirPatil/gorilla which gorilla: training and evaluating llms for function calls (tool calls) It is written in Python. The repository has 12,574 stars, indicating high community interest."
  },
  {
    "input": "In what year did Proximal Policy Optimization Algorithms come out?",
    "output": "The Proximal Policy Optimization Algorithms paper was published in 2017."
  },
  {
    "input": "Give me an overview of Neural Collaborative Filtering.",
    "output": "Neural Collaborative Filtering is a 2017 paper in Information Retrieval. The implementation is available at hexiangnan/neural_collaborative_filtering which neural collaborative filtering It is written in Python. The repository has 1,867 stars, indicating high community interest."
  },
  {
    "input": "Tell me the title of ArXiv paper 1910.01108.",
    "output": "TinyBERT: Distilling BERT for Natural Language Understanding"
  },
  {
    "input": "Give me an overview of Universal Language Model Fine-tuning for Text Classification.",
    "output": "Universal Language Model Fine-tuning for Text Classification is a 2018 paper in Computation and Language (Natural Language Processing). The implementation is available at fastai/fastai which the fastai deep learning library It is written in Jupyter Notebook. The repository has 27,611 stars, indicating high community interest."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2303.09431?",
    "output": "arXiv Query: search_query=&id_list=2303.09431&start=0&max_results=10"
  },
  {
    "input": "How popular is the JAX: Composable transformations of Python+NumPy programs implementation?",
    "output": "The jax-ml/jax repository has 34,091 stars and 3,264 forks on GitHub."
  },
  {
    "input": "Explain the JohnNay/llm-lobbyist implementation of arXiv Query: search_query=&id_list=2301.01181&start=0&max_results=10.",
    "output": "Code for the paper: \"Large Language Models as Corporate Lobbyists\" (2023)."
  },
  {
    "input": "Summarize the GPT-4 Technical Report paper.",
    "output": "GPT-4 Technical Report is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at openai/evals which evals is a framework for evaluating llms and llm systems, and an open-source registry of benchmarks. It is written in Python. The repository has 17,349 stars, indicating high community interest."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2301.01947&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2301.01947&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at oughtinc/ice which interactive composition explorer: a debugger for compositional language model programs It is written in Python. The repository has 563 stars, indicating high community interest."
  },
  {
    "input": "What paper has ArXiv ID 1909.11942?",
    "output": "ELECTRA: Pre-training Text Encoders as Discriminators"
  },
  {
    "input": "How popular is the Swin Transformer: Hierarchical Vision Transformer using Shifted Windows implementation?",
    "output": "The microsoft/Swin-Transformer repository has 15,466 stars and 2,198 forks on GitHub."
  },
  {
    "input": "What is Prefix-Tuning: Optimizing Continuous Prompts for Generation about?",
    "output": "Prefix-Tuning: Optimizing Continuous Prompts for Generation is a 2021 paper in Computation and Language (Natural Language Processing). The implementation is available at XiangLi1999/PrefixTuning which prefix-tuning: optimizing continuous prompts for generation It is written in Python. The repository has 954 stars, indicating high community interest."
  },
  {
    "input": "What category is the Zephyr: Direct Distillation of LM Alignment paper in?",
    "output": "Zephyr: Direct Distillation of LM Alignment is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2410.12896?",
    "output": "arXiv Query: search_query=&id_list=2410.12896&start=0&max_results=10"
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2303.06865&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2303.06865&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What category is the Distilling the Knowledge in a Neural Network paper in?",
    "output": "Distilling the Knowledge in a Neural Network is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What's the implementation repository for Analyzing the Structure of Attention in a Transformer Language Model?",
    "output": "The code for Analyzing the Structure of Attention in a Transformer Language Model is available at https://github.com/clarkkev/attention-analysis (clarkkev/attention-analysis)."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2301.11316&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2301.11316&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at autonomousvision/stylegan-t which [icml'23] stylegan-t: unlocking the power of gans for fast large-scale text-to-image synthesis It is written in Python. The repository has 1,196 stars, indicating high community interest."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2505.22101&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2505.22101&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at ruixin31/Spurious_Rewards It is written in Python. The repository has 342 stars, indicating high community interest."
  },
  {
    "input": "Explain the autonomousvision/stylegan-t implementation of arXiv Query: search_query=&id_list=2301.10343&start=0&max_results=10.",
    "output": "[ICML'23] StyleGAN-T: Unlocking the Power of GANs for Fast Large-Scale Text-to-Image Synthesis"
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2303.09431&start=0&max_results=10 repository have?",
    "output": "The FMInference/FlexLLMGen repository has 9,379 stars and 584 forks on GitHub."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2303.06865&start=0&max_results=10 implementation?",
    "output": "The FMInference/FlexLLMGen repository has 9,379 stars and 584 forks on GitHub."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2504.02732&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2504.02732&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Describe the facebookresearch/segment-anything repository that implements Segment Anything.",
    "output": "The repository provides code for running inference with the SegmentAnything Model (SAM), links for downloading the trained model checkpoints, and example notebooks that show how to use the model."
  },
  {
    "input": "What is the ArXiv category of Neural Machine Translation by Jointly Learning to Align and Translate?",
    "output": "Neural Machine Translation by Jointly Learning to Align and Translate is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Give me an overview of Linformer: Self-Attention with Linear Complexity.",
    "output": "Linformer: Self-Attention with Linear Complexity is a 2021 paper in Machine Learning. The implementation is available at tatp22/linformer-pytorch which my take on a practical implementation of linformer for pytorch. It is written in Python. The repository has 421 stars, indicating high community interest."
  },
  {
    "input": "In what year did WizardCoder: Empowering Code LLMs with Evol-Instruct come out?",
    "output": "The WizardCoder: Empowering Code LLMs with Evol-Instruct paper was published in 2023."
  },
  {
    "input": "What language is the rail-berkeley/softlearning repository written in?",
    "output": "The Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning implementation is written in Python."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2405.03520&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2405.03520&start=0&max_results=10 paper was published in 2024."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2502.01061&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2502.01061&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at GAIR-NLP/LIMO which [colm 2025] limo: less is more for reasoning It is written in Python. The repository has 1,052 stars, indicating high community interest."
  },
  {
    "input": "What category is the Dream to Control: Learning Behaviors by Latent Imagination paper in?",
    "output": "Dream to Control: Learning Behaviors by Latent Imagination is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks about?",
    "output": "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks is a 2019 paper in Computation and Language (Natural Language Processing). The implementation is available at huggingface/sentence-transformers which state-of-the-art text embeddings It is written in Python. The repository has 17,904 stars, indicating high community interest."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2301.00303&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2301.00303&start=0&max_results=10 is available at https://github.com/facebookresearch/ConvNeXt-V2 (facebookresearch/ConvNeXt-V2)."
  },
  {
    "input": "What programming language is Adam: A Method for Stochastic Optimization implemented in?",
    "output": "The Adam: A Method for Stochastic Optimization implementation is written in Python."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2406.09308&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2406.09308&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2301.00303&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2301.00303&start=0&max_results=10 paper was published in 2022."
  },
  {
    "input": "What category is the Multi-Task Deep Neural Networks for Natural Language Understanding paper in?",
    "output": "Multi-Task Deep Neural Networks for Natural Language Understanding is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What's the implementation repository for LLaVA: Large Language and Vision Assistant?",
    "output": "The code for LLaVA: Large Language and Vision Assistant is available at https://github.com/haotian-liu/LLaVA (haotian-liu/LLaVA)."
  },
  {
    "input": "Summarize the AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning paper.",
    "output": "AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning is a 2022 paper in Machine Learning. The implementation is available at QingruZhang/AdaLoRA which adalora: adaptive budget allocation for parameter-efficient fine-tuning (iclr 2023).  It is written in Python. The repository has 361 stars, indicating high community interest."
  },
  {
    "input": "Tell me the title of ArXiv paper 2504.21801.",
    "output": "arXiv Query: search_query=&id_list=2504.21801&start=0&max_results=10"
  },
  {
    "input": "What is the title of the paper with ArXiv ID 1708.05031?",
    "output": "Neural Collaborative Filtering"
  },
  {
    "input": "What is the ArXiv category of BERT Rediscovers the Classical NLP Pipeline?",
    "output": "BERT Rediscovers the Classical NLP Pipeline is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Describe the KimMeen/Time-LLM repository that implements Time-LLM: Time Series Forecasting by Reprogramming Large Language Models.",
    "output": "[ICLR 2024] Official implementation of \" \ud83e\udd99 Time-LLM: Time Series Forecasting by Reprogramming Large Language Models\""
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2301.13188&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2301.13188&start=0&max_results=10 is available at https://github.com/amazon-science/mm-cot (amazon-science/mm-cot)."
  },
  {
    "input": "Give me an overview of Animate Anyone: Consistent and Controllable Image-to-Video Synthesis.",
    "output": "Animate Anyone: Consistent and Controllable Image-to-Video Synthesis is a 2023 paper in Computer Vision and Pattern Recognition. The implementation is available at HumanAIGC/AnimateAnyone which animate anyone: consistent and controllable image-to-video synthesis for character animation The repository has 14,797 stars, indicating high community interest."
  },
  {
    "input": "Give me an overview of BERT Rediscovers the Classical NLP Pipeline.",
    "output": "BERT Rediscovers the Classical NLP Pipeline is a 2019 paper in Computation and Language (Natural Language Processing). The implementation is available at clarkkev/attention-analysis It is written in Jupyter Notebook. The repository has 469 stars, indicating high community interest."
  },
  {
    "input": "What's the implementation repository for Playing Atari with Deep Reinforcement Learning?",
    "output": "The code for Playing Atari with Deep Reinforcement Learning is available at https://github.com/google-deepmind/dqn (google-deepmind/dqn)."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2505.23006&start=0&max_results=10 repository have?",
    "output": "The ruixin31/Spurious_Rewards repository has 342 stars and 20 forks on GitHub."
  },
  {
    "input": "Summarize the Depthwise Separable Convolutions for Neural Machine Translation paper.",
    "output": "Depthwise Separable Convolutions for Neural Machine Translation is a 2017 paper in Computation and Language (Natural Language Processing). The implementation is available at google-research/google-research which google research It is written in Jupyter Notebook. The repository has 36,800 stars, indicating high community interest."
  },
  {
    "input": "What is the ArXiv category of RAG: Retrieval-Augmented Generation?",
    "output": "RAG: Retrieval-Augmented Generation is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "How many stars does the Reflexion: Language Agents with Verbal Reinforcement Learning repository have?",
    "output": "The noahshinn/reflexion repository has 2,959 stars and 285 forks on GitHub."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2302.08242&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2302.08242&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2502.03387&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2502.03387&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2504.19413&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2504.19413&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "Which field does Attention Is All You Need belong to?",
    "output": "Attention Is All You Need is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Explain the BERT: Pre-training of Deep Bidirectional Transformers paper.",
    "output": "BERT: Pre-training of Deep Bidirectional Transformers is a 2018 paper in Computation and Language (Natural Language Processing). The implementation is available at google-research/bert which tensorflow code and pre-trained models for bert It is written in Python. The repository has 39,687 stars, indicating high community interest."
  },
  {
    "input": "What language is the openai/baselines repository written in?",
    "output": "The Proximal Policy Optimization Algorithms implementation is written in Python."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2504.19413&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2504.19413&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What year was Dense Passage Retrieval for Open-Domain Question Answering released?",
    "output": "The Dense Passage Retrieval for Open-Domain Question Answering paper was published in 2020."
  },
  {
    "input": "When was the BERT: Pre-training of Deep Bidirectional Transformers paper published?",
    "output": "The BERT: Pre-training of Deep Bidirectional Transformers paper was published in 2018."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2301.01751&start=0&max_results=10 implementation?",
    "output": "The oughtinc/ice repository has 563 stars and 70 forks on GitHub."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2302.07459&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2302.07459&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2301.04104&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2301.04104&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "Tell me the title of ArXiv paper 2504.20734.",
    "output": "arXiv Query: search_query=&id_list=2504.20734&start=0&max_results=10"
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2302.04023&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2302.04023&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is the GitHub repository for InPars: Data Augmentation for Information Retrieval?",
    "output": "The code for InPars: Data Augmentation for Information Retrieval is available at https://github.com/zetaalphavector/InPars (zetaalphavector/InPars)."
  },
  {
    "input": "What are the key topics in the danijar/dreamer repository?",
    "output": "The danijar/dreamer repository covers: reinforcement-learning, artificial-intelligence, deep-learning, robotics, world-models."
  },
  {
    "input": "What does the LTH14/fractalgen repository do?",
    "output": "PyTorch implementation of FractalGen https://arxiv.org/abs/2502.17437"
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2301.00774&start=0&max_results=10 implementation?",
    "output": "The facebookresearch/ConvNeXt-V2 repository has 1,890 stars and 155 forks on GitHub."
  },
  {
    "input": "What paper has ArXiv ID 2305.10425?",
    "output": "Time-LLM: Time Series Forecasting by Reprogramming Large Language Models"
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2301.04856&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2301.04856&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at frankxu2004/knnlm-why which repo for icml23 \"why do nearest neighbor language models work?\" It is written in Python. The repository has 59 stars, indicating high community interest."
  },
  {
    "input": "Give me an overview of Proximal Policy Optimization Algorithms.",
    "output": "Proximal Policy Optimization Algorithms is a 2017 paper in Machine Learning. The implementation is available at openai/baselines which openai baselines: high-quality implementations of reinforcement learning algorithms It is written in Python. The repository has 16,566 stars, indicating high community interest."
  },
  {
    "input": "What topics does the S4: Efficiently Modeling Long Sequences implementation cover?",
    "output": "The state-spaces/s4 repository covers: state-space-models, sequence-models, pytorch."
  },
  {
    "input": "Describe the LTH14/fractalgen repository that implements arXiv Query: search_query=&id_list=2502.19328&start=0&max_results=10.",
    "output": "PyTorch implementation of FractalGen https://arxiv.org/abs/2502.17437"
  },
  {
    "input": "What's the implementation repository for Know What You Don't Know: Unanswerable Questions for SQuAD?",
    "output": "The code for Know What You Don't Know: Unanswerable Questions for SQuAD is available at https://github.com/rajpurkar/SQuAD-explorer (rajpurkar/SQuAD-explorer)."
  },
  {
    "input": "Give me an overview of Time-LLM: Time Series Forecasting by Reprogramming Large Language Models.",
    "output": "Time-LLM: Time Series Forecasting by Reprogramming Large Language Models is a 2023 paper in Machine Learning. The implementation is available at KimMeen/Time-LLM which [iclr 2024] official implementation of \" \ud83e\udd99 time-llm: time series forecasting by reprogramming large language models\" It is written in Python. The repository has 2,377 stars, indicating high community interest."
  },
  {
    "input": "Describe the meta-llama/llama repository that implements Llama 2: Open Foundation and Fine-Tuned Chat Models.",
    "output": "Inference code for Llama models"
  },
  {
    "input": "What paper has ArXiv ID 2303.02913?",
    "output": "arXiv Query: search_query=&id_list=2303.02913&start=0&max_results=10"
  },
  {
    "input": "In what year did Deep Residual Learning for Image Recognition come out?",
    "output": "The Deep Residual Learning for Image Recognition paper was published in 2015."
  },
  {
    "input": "What does the pytorch/pytorch repository do?",
    "output": "Tensors and Dynamic neural networks in Python with strong GPU acceleration"
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2302.10866&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2302.10866&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Summarize the AWQ: Activation-aware Weight Quantization for LLM Compression paper.",
    "output": "AWQ: Activation-aware Weight Quantization for LLM Compression is a 2023 paper in Machine Learning. The implementation is available at mit-han-lab/llm-awq which [mlsys 2024 best paper award] awq: activation-aware weight quantization for llm compression and acceleration It is written in Python. The repository has 3,357 stars, indicating high community interest."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2302.12766?",
    "output": "arXiv Query: search_query=&id_list=2302.12766&start=0&max_results=10"
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2501.19393&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2501.19393&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at simplescaling/s1 which s1: simple test-time scaling It is written in Python. The repository has 6,605 stars, indicating high community interest."
  },
  {
    "input": "What language is the haitongli/knowledge-distillation-pytorch repository written in?",
    "output": "The Distilling the Knowledge in a Neural Network implementation is written in Python."
  },
  {
    "input": "Summarize the RAG: Retrieval-Augmented Generation paper.",
    "output": "RAG: Retrieval-Augmented Generation is a 2020 paper in Computation and Language (Natural Language Processing). The implementation is available at facebookresearch/DPR which dense passage retriever - is a set of tools and models for open domain q&a task. It is written in Python. The repository has 1,847 stars, indicating high community interest."
  },
  {
    "input": "Which field does Mistral 7B belong to?",
    "output": "Mistral 7B is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2301.00808&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2301.00808&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2406.07394&start=0&max_results=10 repository have?",
    "output": "The hpcaitech/Open-Sora repository has 27,981 stars and 2,781 forks on GitHub."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2501.11067&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2501.11067&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "What does the namisan/mt-dnn repository do?",
    "output": "Multi-Task Deep Neural Networks for Natural Language Understanding"
  },
  {
    "input": "What's the implementation repository for Mistral 7B?",
    "output": "The code for Mistral 7B is available at https://github.com/mistralai/mistral-inference (mistralai/mistral-inference)."
  },
  {
    "input": "What is YOLOX: Exceeding YOLO Series in 2021 about?",
    "output": "YOLOX: Exceeding YOLO Series in 2021 is a 2018 paper in Computer Vision and Pattern Recognition. The implementation is available at Megvii-BaseDetection/YOLOX which yolox is a high-performance anchor-free yolo, exceeding yolov3~v5 with megengine, onnx, tensorrt, ncnn, and openvino supported. documentation: https://yolox.readthedocs.io/ It is written in Python. The repository has 10,185 stars, indicating high community interest."
  },
  {
    "input": "What category is the GPT-3: Language Models are Few-Shot Learners paper in?",
    "output": "GPT-3: Language Models are Few-Shot Learners is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What programming language is Very Deep Convolutional Networks for Large-Scale Image Recognition implemented in?",
    "output": "The Very Deep Convolutional Networks for Large-Scale Image Recognition implementation is written in Python."
  },
  {
    "input": "Where can I find the code for DreamBooth: Fine Tuning Text-to-Image Diffusion Models?",
    "output": "The code for DreamBooth: Fine Tuning Text-to-Image Diffusion Models is available at https://github.com/google/dreambooth (google/dreambooth)."
  },
  {
    "input": "In what year did Reflexion: Language Agents with Verbal Reinforcement Learning come out?",
    "output": "The Reflexion: Language Agents with Verbal Reinforcement Learning paper was published in 2023."
  },
  {
    "input": "What's the implementation repository for GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers?",
    "output": "The code for GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers is available at https://github.com/IST-DASLab/gptq (IST-DASLab/gptq)."
  },
  {
    "input": "Tell me the title of ArXiv paper 2109.04908.",
    "output": "XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale"
  },
  {
    "input": "What does the huggingface/sentence-transformers repository do?",
    "output": "State-of-the-Art Text Embeddings"
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2406.06326&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2406.06326&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is the GitHub repository for SimCSE: Simple Contrastive Learning of Sentence Embeddings?",
    "output": "The code for SimCSE: Simple Contrastive Learning of Sentence Embeddings is available at https://github.com/princeton-nlp/SimCSE (princeton-nlp/SimCSE)."
  },
  {
    "input": "Give me an overview of Masked Autoencoders Are Scalable Vision Learners.",
    "output": "Masked Autoencoders Are Scalable Vision Learners is a 2022 paper in Computer Vision and Pattern Recognition. The implementation is available at facebookresearch/mae which pytorch implementation of mae https//arxiv.org/abs/2111.06377 It is written in Python. The repository has 8,108 stars, indicating high community interest."
  },
  {
    "input": "Explain the meta-llama/codellama implementation of Code Llama: Open Foundation Models for Code.",
    "output": "Inference code for CodeLlama models"
  },
  {
    "input": "How popular is the Question and Answer Test-Train Overlap in Open-Domain QA Datasets implementation?",
    "output": "The google-research-datasets/natural-questions repository has 1,074 stars and 158 forks on GitHub."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2504.11844&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2504.11844&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What category is the Neural Collaborative Filtering paper in?",
    "output": "Neural Collaborative Filtering is in the Information Retrieval (cs.IR) category."
  },
  {
    "input": "Explain the BEiT: BERT Pre-Training of Image Transformers paper.",
    "output": "BEiT: BERT Pre-Training of Image Transformers is a 2021 paper in Computer Vision and Pattern Recognition. The implementation is available at microsoft/unilm which large-scale self-supervised pre-training across tasks, languages, and modalities It is written in Python. The repository has 21,848 stars, indicating high community interest."
  },
  {
    "input": "How many stars does the FLAN: Finetuned Language Models Are Zero-Shot Learners repository have?",
    "output": "The google-research/FLAN repository has 1,554 stars and 161 forks on GitHub."
  },
  {
    "input": "What is Faster R-CNN: Towards Real-Time Object Detection about?",
    "output": "Faster R-CNN: Towards Real-Time Object Detection is a 2015 paper in Computer Vision and Pattern Recognition. The implementation is available at rbgirshick/py-faster-rcnn which faster r-cnn (python implementation) -- see https://github.com/shaoqingren/faster_rcnn for the official matlab version It is written in Python. The repository has 8,269 stars, indicating high community interest."
  },
  {
    "input": "How many stars does the T5: Exploring the Limits of Transfer Learning repository have?",
    "output": "The google-research/text-to-text-transfer-transformer repository has 6,453 stars and 789 forks on GitHub."
  },
  {
    "input": "Summarize the FlashAttention: Fast and Memory-Efficient Exact Attention paper.",
    "output": "FlashAttention: Fast and Memory-Efficient Exact Attention is a 2022 paper in Machine Learning. The implementation is available at Dao-AILab/flash-attention which fast and memory-efficient exact attention It is written in Python. The repository has 20,719 stars, indicating high community interest."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2303.03846&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2303.03846&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What does the lucidrains/muse-maskgit-pytorch repository do?",
    "output": "Implementation of Muse: Text-to-Image Generation via Masked Generative Transformers, in Pytorch"
  },
  {
    "input": "Where can I find the code for FlashAttention-2: Faster Attention with Better Parallelism?",
    "output": "The code for FlashAttention-2: Faster Attention with Better Parallelism is available at https://github.com/Dao-AILab/flash-attention (Dao-AILab/flash-attention)."
  },
  {
    "input": "In what year did EfficientDet: Scalable and Efficient Object Detection come out?",
    "output": "The EfficientDet: Scalable and Efficient Object Detection paper was published in 2020."
  },
  {
    "input": "What language is the haotian-liu/LLaVA repository written in?",
    "output": "The LLaVA-1.5: Improved Baselines with Visual Instruction Tuning implementation is written in Python."
  },
  {
    "input": "Tell me the title of ArXiv paper 2006.04768.",
    "output": "DeBERTa: Decoding-enhanced BERT with Disentangled Attention"
  },
  {
    "input": "What category is the ZeRO: Memory Optimizations Toward Training Trillion Parameter Models paper in?",
    "output": "ZeRO: Memory Optimizations Toward Training Trillion Parameter Models is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2302.14233?",
    "output": "arXiv Query: search_query=&id_list=2302.14233&start=0&max_results=10"
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2406.09403&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2406.09403&start=0&max_results=10 is available at https://github.com/lamini-ai/Lamini-Memory-Tuning (lamini-ai/Lamini-Memory-Tuning)."
  },
  {
    "input": "What language is the XiangLi1999/PrefixTuning repository written in?",
    "output": "The Prefix-Tuning: Optimizing Continuous Prompts for Generation implementation is written in Python."
  },
  {
    "input": "Give me an overview of Contriever: Unsupervised Dense Information Retrieval.",
    "output": "Contriever: Unsupervised Dense Information Retrieval is a 2022 paper in Information Retrieval. The implementation is available at facebookresearch/contriever which contriever: unsupervised dense information retrieval with contrastive learning It is written in Python. The repository has 763 stars, indicating high community interest."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2301.09515&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2301.09515&start=0&max_results=10 is available at https://github.com/autonomousvision/stylegan-t (autonomousvision/stylegan-t)."
  },
  {
    "input": "Describe the salesforce/BLIP repository that implements BLIP: Bootstrapping Language-Image Pre-training.",
    "output": "PyTorch code for BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation  "
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2302.07459&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2302.07459&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at haoliuhl/language-quantized-autoencoders which language quantized autoencoders It is written in Python. The repository has 111 stars, indicating high community interest."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2301.01947&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2301.01947&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2501.11067&start=0&max_results=10 implementation?",
    "output": "The plurai-ai/intellagent repository has 1,148 stars and 139 forks on GitHub."
  },
  {
    "input": "What topics does the arXiv Query: search_query=&id_list=2302.03917&start=0&max_results=10 implementation cover?",
    "output": "The mit-han-lab/offsite-tuning repository covers: deep-learning, transfer-learning."
  },
  {
    "input": "How popular is the Masked Autoencoders Are Scalable Vision Learners implementation?",
    "output": "The facebookresearch/mae repository has 8,108 stars and 1,332 forks on GitHub."
  },
  {
    "input": "Explain the rail-berkeley/softlearning implementation of Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning.",
    "output": "Softlearning is a reinforcement learning framework for training maximum entropy policies in continuous domains. Includes the official implementation of the Soft Actor-Critic algorithm."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2307.03109?",
    "output": "AlpacaEval: An Automatic Evaluator for Instruction-following"
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2505.22101&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2505.22101&start=0&max_results=10 is available at https://github.com/ruixin31/Spurious_Rewards (ruixin31/Spurious_Rewards)."
  },
  {
    "input": "What does the ashawkey/stable-dreamfusion repository do?",
    "output": "Text-to-3D & Image-to-3D & Mesh Exportation with NeRF + Diffusion."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2301.12652&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2301.12652&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What are the key topics in the facebookresearch/fairseq repository?",
    "output": "The facebookresearch/fairseq repository covers: python, pytorch, artificial-intelligence."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2302.04870?",
    "output": "arXiv Query: search_query=&id_list=2302.04870&start=0&max_results=10"
  },
  {
    "input": "What is the GitHub repository for Time-LLM: Time Series Forecasting by Reprogramming Large Language Models?",
    "output": "The code for Time-LLM: Time Series Forecasting by Reprogramming Large Language Models is available at https://github.com/KimMeen/Time-LLM (KimMeen/Time-LLM)."
  },
  {
    "input": "What's the implementation repository for AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning?",
    "output": "The code for AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning is available at https://github.com/QingruZhang/AdaLoRA (QingruZhang/AdaLoRA)."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2503.20201&start=0&max_results=10 repository have?",
    "output": "The sentient-agi/OpenDeepSearch repository has 3,723 stars and 343 forks on GitHub."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2104.00355&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2104.00355&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is the GitHub repository for Longformer: The Long-Document Transformer?",
    "output": "The code for Longformer: The Long-Document Transformer is available at https://github.com/allenai/longformer (allenai/longformer)."
  },
  {
    "input": "What language is the nlpxucan/WizardLM repository written in?",
    "output": "The WizardCoder: Empowering Code LLMs with Evol-Instruct implementation is written in Python."
  },
  {
    "input": "Which field does Know What You Don't Know: Unanswerable Questions for SQuAD belong to?",
    "output": "Know What You Don't Know: Unanswerable Questions for SQuAD is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2004.13637&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2004.13637&start=0&max_results=10 is available at https://github.com/facebookresearch/TransCoder (facebookresearch/TransCoder)."
  },
  {
    "input": "What paper has ArXiv ID 2006.03511?",
    "output": "arXiv Query: search_query=&id_list=2006.03511&start=0&max_results=10"
  },
  {
    "input": "What programming language is RetNet: Retentive Network: A Successor to Transformer implemented in?",
    "output": "The RetNet: Retentive Network: A Successor to Transformer implementation is written in Python."
  },
  {
    "input": "What category is the InstructBLIP: Towards General-purpose Vision-Language Models paper in?",
    "output": "InstructBLIP: Towards General-purpose Vision-Language Models is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "Tell me the title of ArXiv paper 2005.08100.",
    "output": "Question and Answer Test-Train Overlap in Open-Domain QA Datasets"
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2303.03846&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2303.03846&start=0&max_results=10 implementation is written in Assembly."
  },
  {
    "input": "What is the GitHub repository for Vicuna: An Open-Source Chatbot?",
    "output": "The code for Vicuna: An Open-Source Chatbot is available at https://github.com/lm-sys/FastChat (lm-sys/FastChat)."
  },
  {
    "input": "What paper has ArXiv ID 2205.12755?",
    "output": "Beir: A Heterogeneous Benchmark for IR"
  },
  {
    "input": "Summarize the FLAN: Finetuned Language Models Are Zero-Shot Learners paper.",
    "output": "FLAN: Finetuned Language Models Are Zero-Shot Learners is a 2021 paper in Computation and Language (Natural Language Processing). The implementation is available at google-research/FLAN It is written in Python. The repository has 1,554 stars, indicating high community interest."
  },
  {
    "input": "Where can I find the code for Reflexion: Language Agents with Verbal Reinforcement Learning?",
    "output": "The code for Reflexion: Language Agents with Verbal Reinforcement Learning is available at https://github.com/noahshinn/reflexion (noahshinn/reflexion)."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2505.22954&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2505.22954&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "What are the key topics in the facebookresearch/fairseq repository?",
    "output": "The facebookresearch/fairseq repository covers: python, pytorch, artificial-intelligence."
  },
  {
    "input": "When was the Deep Graph Library: A Graph-Centric, Highly-Performant Package paper published?",
    "output": "The Deep Graph Library: A Graph-Centric, Highly-Performant Package paper was published in 2020."
  },
  {
    "input": "What does the facebookresearch/mae repository do?",
    "output": "PyTorch implementation of MAE https//arxiv.org/abs/2111.06377"
  },
  {
    "input": "What category is the TensorFlow: A System for Large-Scale Machine Learning paper in?",
    "output": "TensorFlow: A System for Large-Scale Machine Learning is in the Distributed, Parallel, and Cluster Computing (cs.DC) category."
  },
  {
    "input": "Describe the lamini-ai/Lamini-Memory-Tuning repository that implements arXiv Query: search_query=&id_list=2406.09308&start=0&max_results=10.",
    "output": "Banishing LLM Hallucinations Requires Rethinking Generalization"
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2505.23006&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2505.23006&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2302.05206&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2302.05206&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at tianjunz/HIR It is written in Python. The repository has 159 stars, indicating high community interest."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2503.01141&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2503.01141&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at LTH14/fractalgen which pytorch implementation of fractalgen https://arxiv.org/abs/2502.17437 It is written in Python. The repository has 1,196 stars, indicating high community interest."
  },
  {
    "input": "What is StarCoder: May the source be with you! about?",
    "output": "StarCoder: May the source be with you! is a 2023 paper in Machine Learning. The implementation is available at bigcode-project/starcoder which home of starcoder: fine-tuning & inference! It is written in Python. The repository has 7,473 stars, indicating high community interest."
  },
  {
    "input": "How many stars does the Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks repository have?",
    "output": "The huggingface/sentence-transformers repository has 17,904 stars and 2,717 forks on GitHub."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2503.00735&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2503.00735&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at LTH14/fractalgen which pytorch implementation of fractalgen https://arxiv.org/abs/2502.17437 It is written in Python. The repository has 1,196 stars, indicating high community interest."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2301.11305&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2301.11305&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Describe the facebookresearch/Detectron repository that implements Feature Pyramid Networks for Object Detection.",
    "output": "FAIR's research platform for object detection research, implementing popular algorithms like Mask R-CNN and RetinaNet."
  },
  {
    "input": "How popular is the XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale implementation?",
    "output": "The facebookresearch/fairseq repository has 31,985 stars and 6,630 forks on GitHub."
  },
  {
    "input": "Which field does AWQ: Activation-aware Weight Quantization for LLM Compression belong to?",
    "output": "AWQ: Activation-aware Weight Quantization for LLM Compression is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Explain the mit-han-lab/offsite-tuning implementation of arXiv Query: search_query=&id_list=2302.03917&start=0&max_results=10.",
    "output": "Offsite-Tuning: Transfer Learning without Full Model"
  },
  {
    "input": "Tell me the title of ArXiv paper 2305.14314.",
    "output": "QLoRA: Efficient Finetuning of Quantized LLMs"
  },
  {
    "input": "In what year did AWQ: Activation-aware Weight Quantization for LLM Compression come out?",
    "output": "The AWQ: Activation-aware Weight Quantization for LLM Compression paper was published in 2023."
  },
  {
    "input": "Explain the tensorflow/models implementation of Wide & Deep Learning for Recommender Systems.",
    "output": "Models and examples built with TensorFlow"
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2301.12652&start=0&max_results=10 repository have?",
    "output": "The amazon-science/mm-cot repository has 3,982 stars and 333 forks on GitHub."
  },
  {
    "input": "Explain the Proximal Policy Optimization Algorithms paper.",
    "output": "Proximal Policy Optimization Algorithms is a 2017 paper in Machine Learning. The implementation is available at openai/baselines which openai baselines: high-quality implementations of reinforcement learning algorithms It is written in Python. The repository has 16,566 stars, indicating high community interest."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2502.12115&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2502.12115&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at Open-Reasoner-Zero/Open-Reasoner-Zero which official repo for open-reasoner-zero It is written in Python. The repository has 2,068 stars, indicating high community interest."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 1902.05188?",
    "output": "JAX: Composable transformations of Python+NumPy programs"
  },
  {
    "input": "What paper has ArXiv ID 2109.07958?",
    "output": "MMLU: Measuring Massive Multitask Language Understanding"
  },
  {
    "input": "What category is the P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning paper in?",
    "output": "P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2502.17437&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2502.17437&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Tell me the title of ArXiv paper 2405.03548.",
    "output": "arXiv Query: search_query=&id_list=2405.03548&start=0&max_results=10"
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2006.03511&start=0&max_results=10 implementation?",
    "output": "The facebookresearch/TransCoder repository has 1,722 stars and 261 forks on GitHub."
  },
  {
    "input": "How many stars does the Sequence to Sequence Learning with Neural Networks repository have?",
    "output": "The farizrahman4u/seq2seq repository has 3,173 stars and 838 forks on GitHub."
  },
  {
    "input": "What is the ArXiv category of T5: Exploring the Limits of Transfer Learning?",
    "output": "T5: Exploring the Limits of Transfer Learning is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2504.01848&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2504.01848&start=0&max_results=10 is available at https://github.com/openai/frontier-evals (openai/frontier-evals)."
  },
  {
    "input": "What is the GitHub repository for An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale?",
    "output": "The code for An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale is available at https://github.com/google-research/vision_transformer (google-research/vision_transformer)."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2109.03264&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2109.03264&start=0&max_results=10 is a 2021 paper in Machine Learning. The implementation is available at facebookresearch/fairseq which facebook ai research sequence-to-sequence toolkit written in python. It is written in Python. The repository has 31,988 stars, indicating high community interest."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2102.01192&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2102.01192&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What does the THUDM/P-tuning-v2 repository do?",
    "output": "An optimized deep prompt tuning strategy comparable to fine-tuning across scales and tasks"
  },
  {
    "input": "What paper has ArXiv ID 2305.07185?",
    "output": "LIMA: Less Is More for Alignment"
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2302.14233&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2302.14233&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at ruiqi-zhong/D5 which the github repo for goal driven discovery of distributional differences via language descriptions It is written in Python. The repository has 71 stars, indicating high community interest."
  },
  {
    "input": "What does the ibm-granite/granite-code-models repository do?",
    "output": "Granite Code Models: A Family of Open Foundation Models for Code Intelligence"
  },
  {
    "input": "What are the key topics in the ashawkey/stable-dreamfusion repository?",
    "output": "The ashawkey/stable-dreamfusion repository covers: text-to-3d, gui, nerf, stable-diffusion, dreamfusion, image-to-3d."
  },
  {
    "input": "Give me an overview of Sequence to Sequence Learning with Neural Networks.",
    "output": "Sequence to Sequence Learning with Neural Networks is a 2014 paper in Computation and Language (Natural Language Processing). The implementation is available at farizrahman4u/seq2seq which sequence to sequence learning with keras It is written in Python. The repository has 3,173 stars, indicating high community interest."
  },
  {
    "input": "Which field does Neural Collaborative Filtering belong to?",
    "output": "Neural Collaborative Filtering is in the Information Retrieval (cs.IR) category."
  },
  {
    "input": "Summarize the Denoising Diffusion Probabilistic Models paper.",
    "output": "Denoising Diffusion Probabilistic Models is a 2020 paper in Machine Learning. The implementation is available at hojonathanho/diffusion which denoising diffusion probabilistic models It is written in Python. The repository has 4,860 stars, indicating high community interest."
  },
  {
    "input": "What's the implementation repository for Attention Is All You Need?",
    "output": "The code for Attention Is All You Need is available at https://github.com/tensorflow/tensor2tensor (tensorflow/tensor2tensor)."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2504.11844&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2504.11844&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "What year was Animate Anyone: Consistent and Controllable Image-to-Video Synthesis released?",
    "output": "The Animate Anyone: Consistent and Controllable Image-to-Video Synthesis paper was published in 2023."
  },
  {
    "input": "What language is the bowang-lab/MedSAM repository written in?",
    "output": "The Segment Anything in Medical Images implementation is written in Jupyter Notebook."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2303.09431&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2303.09431&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What language is the LTH14/fractalgen repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2502.17437&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "In what year did Learning to summarize from human feedback come out?",
    "output": "The Learning to summarize from human feedback paper was published in 2020."
  },
  {
    "input": "What is the GitHub repository for Enriching Word Vectors with Subword Information?",
    "output": "The code for Enriching Word Vectors with Subword Information is available at https://github.com/facebookresearch/fastText (facebookresearch/fastText)."
  },
  {
    "input": "What language is the NVlabs/prismer repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2303.03378&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "How many stars does the Feature Pyramid Networks for Object Detection repository have?",
    "output": "The facebookresearch/Detectron repository has 26,396 stars and 5,440 forks on GitHub."
  },
  {
    "input": "What paper has ArXiv ID 2302.14233?",
    "output": "arXiv Query: search_query=&id_list=2302.14233&start=0&max_results=10"
  },
  {
    "input": "Where can I find the code for Wide & Deep Learning for Recommender Systems?",
    "output": "The code for Wide & Deep Learning for Recommender Systems is available at https://github.com/tensorflow/models (tensorflow/models)."
  },
  {
    "input": "Explain the siddk/voltron-robotics implementation of arXiv Query: search_query=&id_list=2302.12766&start=0&max_results=10.",
    "output": "Voltron: Language-Driven Representation Learning for Robotics"
  },
  {
    "input": "How many stars does the Contriever: Unsupervised Dense Information Retrieval repository have?",
    "output": "The facebookresearch/contriever repository has 763 stars and 67 forks on GitHub."
  },
  {
    "input": "What language is the 0nutation/SpeechGPT repository written in?",
    "output": "The SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities implementation is written in Python."
  },
  {
    "input": "What programming language is Distilling the Knowledge in a Neural Network implemented in?",
    "output": "The Distilling the Knowledge in a Neural Network implementation is written in Python."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2502.09992&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2502.09992&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2210.03629?",
    "output": "ReAct: Synergizing Reasoning and Acting in Language Models"
  },
  {
    "input": "What is the ArXiv category of ReAct: Synergizing Reasoning and Acting in Language Models?",
    "output": "ReAct: Synergizing Reasoning and Acting in Language Models is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2302.04023&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2302.04023&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2301.00303?",
    "output": "arXiv Query: search_query=&id_list=2301.00303&start=0&max_results=10"
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2501.13545&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2501.13545&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at plurai-ai/intellagent which a framework for comprehensive diagnosis and optimization of agents using simulated, realistic synthetic interactions It is written in Python. The repository has 1,148 stars, indicating high community interest."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2104.00355&start=0&max_results=10 repository have?",
    "output": "The facebookresearch/fairseq repository has 31,988 stars and 6,630 forks on GitHub."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2301.01751&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2301.01751&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at oughtinc/ice which interactive composition explorer: a debugger for compositional language model programs It is written in Python. The repository has 563 stars, indicating high community interest."
  },
  {
    "input": "What paper has ArXiv ID 1910.01741?",
    "output": "Dream to Control: Learning Behaviors by Latent Imagination"
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2505.22101&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2505.22101&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What does the Open-Reasoner-Zero/Open-Reasoner-Zero repository do?",
    "output": "Official Repo for Open-Reasoner-Zero"
  },
  {
    "input": "What does the amazon-science/mm-cot repository do?",
    "output": "Official implementation for \"Multimodal Chain-of-Thought Reasoning in Language Models\" (stay tuned and more will be updated)"
  },
  {
    "input": "What programming language is Masked Autoencoders Are Scalable Vision Learners implemented in?",
    "output": "The Masked Autoencoders Are Scalable Vision Learners implementation is written in Python."
  },
  {
    "input": "What does the alphadl/lookahead.pytorch repository do?",
    "output": "lookahead optimizer (Lookahead Optimizer: k steps forward, 1 step back) for pytorch "
  },
  {
    "input": "Describe the openai/summarize-from-feedback repository that implements Learning to summarize from human feedback.",
    "output": "Code for \"Learning to summarize from human feedback\""
  },
  {
    "input": "Describe the rbgirshick/py-faster-rcnn repository that implements Faster R-CNN: Towards Real-Time Object Detection.",
    "output": "Faster R-CNN (Python implementation) -- see https://github.com/ShaoqingRen/faster_rcnn for the official MATLAB version"
  },
  {
    "input": "Explain the facebookresearch/TransCoder implementation of arXiv Query: search_query=&id_list=2006.03511&start=0&max_results=10.",
    "output": "Public release of the TransCoder research project https://arxiv.org/pdf/2006.03511.pdf"
  },
  {
    "input": "In what year did Horovod: fast and easy distributed deep learning in TensorFlow come out?",
    "output": "The Horovod: fast and easy distributed deep learning in TensorFlow paper was published in 2017."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2212.03533?",
    "output": "Text Embeddings by Weakly-Supervised Contrastive Pre-training"
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2505.23006&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2505.23006&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at ruixin31/Spurious_Rewards It is written in Python. The repository has 342 stars, indicating high community interest."
  },
  {
    "input": "What language is the facebookresearch/ConvNeXt-V2 repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2301.00774&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Summarize the AudioPaLM: A Large Language Model for Speech Understanding paper.",
    "output": "AudioPaLM: A Large Language Model for Speech Understanding is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at lucidrains/audiolm-pytorch which implementation of audiolm, a sota language modeling approach to audio generation out of google research, in pytorch It is written in Python. The repository has 2,607 stars, indicating high community interest."
  },
  {
    "input": "What topics does the arXiv Query: search_query=&id_list=2102.01192&start=0&max_results=10 implementation cover?",
    "output": "The facebookresearch/fairseq repository covers: python, pytorch, artificial-intelligence."
  },
  {
    "input": "What does the ContextualAI/HALOs repository do?",
    "output": "A library with extensible implementations of DPO, KTO, PPO, ORPO, and other human-aware loss functions (HALOs)."
  },
  {
    "input": "Explain the Vicuna: An Open-Source Chatbot paper.",
    "output": "Vicuna: An Open-Source Chatbot is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at lm-sys/FastChat which an open platform for training, serving, and evaluating large language models. release repo for vicuna and chatbot arena. It is written in Python. The repository has 39,271 stars, indicating high community interest."
  },
  {
    "input": "What year was BERT Rediscovers the Classical NLP Pipeline released?",
    "output": "The BERT Rediscovers the Classical NLP Pipeline paper was published in 2019."
  },
  {
    "input": "What year was Synthetic Data Generation with Large Language Models released?",
    "output": "The Synthetic Data Generation with Large Language Models paper was published in 2023."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2501.13545&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2501.13545&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2503.22678&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2503.22678&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "How popular is the Feature Pyramid Networks for Object Detection implementation?",
    "output": "The facebookresearch/Detectron repository has 26,396 stars and 5,440 forks on GitHub."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2303.03846&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2303.03846&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at HazyResearch/safari which convolutions for sequence modeling It is written in Assembly. The repository has 904 stars, indicating high community interest."
  },
  {
    "input": "What are the key topics in the huggingface/alignment-handbook repository?",
    "output": "The huggingface/alignment-handbook repository covers: llm, rlhf, transformers."
  },
  {
    "input": "What does the tensorflow/tensorflow repository do?",
    "output": "An Open Source Machine Learning Framework for Everyone"
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2302.05206&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2302.05206&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Describe the XiangLi1999/PrefixTuning repository that implements Prefix-Tuning: Optimizing Continuous Prompts for Generation.",
    "output": "Prefix-Tuning: Optimizing Continuous Prompts for Generation"
  },
  {
    "input": "When was the LLaMA: Open and Efficient Foundation Language Models paper published?",
    "output": "The LLaMA: Open and Efficient Foundation Language Models paper was published in 2023."
  },
  {
    "input": "What year was Beir: A Heterogeneous Benchmark for IR released?",
    "output": "The Beir: A Heterogeneous Benchmark for IR paper was published in 2022."
  },
  {
    "input": "What's the implementation repository for Beir: A Heterogeneous Benchmark for IR?",
    "output": "The code for Beir: A Heterogeneous Benchmark for IR is available at https://github.com/beir-cellar/beir (beir-cellar/beir)."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2502.03387&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2502.03387&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at GAIR-NLP/LIMO which [colm 2025] limo: less is more for reasoning It is written in Python. The repository has 1,052 stars, indicating high community interest."
  },
  {
    "input": "Give me an overview of Semi-Supervised Classification with Graph Convolutional Networks.",
    "output": "Semi-Supervised Classification with Graph Convolutional Networks is a 2016 paper in Machine Learning. The implementation is available at tkipf/gcn which implementation of graph convolutional networks in tensorflow It is written in Python. The repository has 7,334 stars, indicating high community interest."
  },
  {
    "input": "When was the InstructBLIP: Towards General-purpose Vision-Language Models paper published?",
    "output": "The InstructBLIP: Towards General-purpose Vision-Language Models paper was published in 2023."
  },
  {
    "input": "What is the ArXiv category of SQuAD: 100,000+ Questions for Machine Comprehension of Text?",
    "output": "SQuAD: 100,000+ Questions for Machine Comprehension of Text is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2009.06367&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2009.06367&start=0&max_results=10 is available at https://github.com/salesforce/GeDi (salesforce/GeDi)."
  },
  {
    "input": "What year was Neural Collaborative Filtering released?",
    "output": "The Neural Collaborative Filtering paper was published in 2017."
  },
  {
    "input": "How popular is the Competition-Level Code Generation with AlphaCode implementation?",
    "output": "The google-deepmind/code_contests repository has 2,177 stars and 224 forks on GitHub."
  },
  {
    "input": "Tell me the title of ArXiv paper 2502.09992.",
    "output": "arXiv Query: search_query=&id_list=2502.09992&start=0&max_results=10"
  },
  {
    "input": "In what year did QLoRA: Efficient Finetuning of Quantized LLMs come out?",
    "output": "The QLoRA: Efficient Finetuning of Quantized LLMs paper was published in 2023."
  },
  {
    "input": "What is the ArXiv category of Proximal Policy Optimization Algorithms?",
    "output": "Proximal Policy Optimization Algorithms is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2301.02828&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2301.02828&start=0&max_results=10 is available at https://github.com/frankxu2004/knnlm-why (frankxu2004/knnlm-why)."
  },
  {
    "input": "Where can I find the code for DeBERTa: Decoding-enhanced BERT with Disentangled Attention?",
    "output": "The code for DeBERTa: Decoding-enhanced BERT with Disentangled Attention is available at https://github.com/microsoft/DeBERTa (microsoft/DeBERTa)."
  },
  {
    "input": "Describe the Dao-AILab/flash-attention repository that implements FlashAttention-2: Faster Attention with Better Parallelism.",
    "output": "Fast and memory-efficient exact attention"
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2312.02783&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2312.02783&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What paper has ArXiv ID 2101.00027?",
    "output": "SimCSE: Simple Contrastive Learning of Sentence Embeddings"
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2301.13188&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2301.13188&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at amazon-science/mm-cot which official implementation for \"multimodal chain-of-thought reasoning in language models\" (stay tuned and more will be updated) It is written in Python. The repository has 3,982 stars, indicating high community interest."
  },
  {
    "input": "In what year did Contriever: Unsupervised Dense Information Retrieval come out?",
    "output": "The Contriever: Unsupervised Dense Information Retrieval paper was published in 2022."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2504.00698&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2504.00698&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Explain the Decoupled Weight Decay Regularization paper.",
    "output": "Decoupled Weight Decay Regularization is a 2017 paper in Machine Learning. The implementation is available at pytorch/pytorch which tensors and dynamic neural networks in python with strong gpu acceleration It is written in Python. The repository has 95,348 stars, indicating high community interest."
  },
  {
    "input": "What programming language is EfficientDet: Scalable and Efficient Object Detection implemented in?",
    "output": "The EfficientDet: Scalable and Efficient Object Detection implementation is written in Jupyter Notebook."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2502.01061&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2502.01061&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2302.04870&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2302.04870&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2305.07185?",
    "output": "LIMA: Less Is More for Alignment"
  },
  {
    "input": "What does the facebookresearch/fairseq repository do?",
    "output": "Facebook AI Research Sequence-to-Sequence Toolkit written in Python."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2302.07459&start=0&max_results=10 repository have?",
    "output": "The haoliuhl/language-quantized-autoencoders repository has 111 stars and 5 forks on GitHub."
  },
  {
    "input": "What language is the google-research/electra repository written in?",
    "output": "The ELECTRA: Pre-training Text Encoders as Discriminators implementation is written in Python."
  },
  {
    "input": "What's the implementation repository for Deep Residual Learning for Image Recognition?",
    "output": "The code for Deep Residual Learning for Image Recognition is available at https://github.com/KaimingHe/deep-residual-networks (KaimingHe/deep-residual-networks)."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2309.03409?",
    "output": "AudioPaLM: A Large Language Model for Speech Understanding"
  },
  {
    "input": "What topics does the arXiv Query: search_query=&id_list=2501.13545&start=0&max_results=10 implementation cover?",
    "output": "The plurai-ai/intellagent repository covers: agent, evaluation, llmops, simulator, synthetic-data."
  },
  {
    "input": "Summarize the RoBERTa: A Robustly Optimized BERT Pretraining Approach paper.",
    "output": "RoBERTa: A Robustly Optimized BERT Pretraining Approach is a 2019 paper in Computation and Language (Natural Language Processing). The implementation is available at facebookresearch/fairseq which facebook ai research sequence-to-sequence toolkit written in python. It is written in Python. The repository has 31,985 stars, indicating high community interest."
  },
  {
    "input": "Give me an overview of DreamBooth: Fine Tuning Text-to-Image Diffusion Models.",
    "output": "DreamBooth: Fine Tuning Text-to-Image Diffusion Models is a 2022 paper in Computer Vision and Pattern Recognition. The implementation is available at google/dreambooth The repository has 998 stars, indicating high community interest."
  },
  {
    "input": "Explain the huawei-noah/Pretrained-Language-Model implementation of TinyBERT: Distilling BERT for Natural Language Understanding.",
    "output": "Pretrained language model and its related optimization techniques developed by Huawei Noah's Ark Lab."
  },
  {
    "input": "What is the GitHub repository for Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks?",
    "output": "The code for Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks is available at https://github.com/huggingface/sentence-transformers (huggingface/sentence-transformers)."
  },
  {
    "input": "What category is the BLIP-2: Bootstrapping Language-Image Pre-training paper in?",
    "output": "BLIP-2: Bootstrapping Language-Image Pre-training is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "What topics does the arXiv Query: search_query=&id_list=2501.13824&start=0&max_results=10 implementation cover?",
    "output": "The plurai-ai/intellagent repository covers: agent, evaluation, llmops, simulator, synthetic-data."
  },
  {
    "input": "What paper has ArXiv ID 2210.03629?",
    "output": "ReAct: Synergizing Reasoning and Acting in Language Models"
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2405.03520&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2405.03520&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What does the bitsandbytes-foundation/bitsandbytes repository do?",
    "output": "Accessible large language models via k-bit quantization for PyTorch."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2301.01181&start=0&max_results=10 implementation?",
    "output": "The JohnNay/llm-lobbyist repository has 172 stars and 15 forks on GitHub."
  },
  {
    "input": "What does the hojonathanho/diffusion repository do?",
    "output": "Denoising Diffusion Probabilistic Models"
  },
  {
    "input": "What language is the haoliuhl/language-quantized-autoencoders repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2302.07459&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2012.06009&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2012.06009&start=0&max_results=10 paper was published in 2020."
  },
  {
    "input": "What programming language is Denoising Diffusion Probabilistic Models implemented in?",
    "output": "The Denoising Diffusion Probabilistic Models implementation is written in Python."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2301.11316&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2301.11316&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2503.22708&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2503.22708&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at allenai/codescientist which codescientist: an automated scientific discovery system for code-based experiments It is written in Python. The repository has 301 stars, indicating high community interest."
  },
  {
    "input": "What is the GitHub repository for BLIP-2: Bootstrapping Language-Image Pre-training?",
    "output": "The code for BLIP-2: Bootstrapping Language-Image Pre-training is available at https://github.com/salesforce/LAVIS (salesforce/LAVIS)."
  },
  {
    "input": "Summarize the Finetuned Language Models Are Zero-Shot Learners paper.",
    "output": "Finetuned Language Models Are Zero-Shot Learners is a 2021 paper in Computation and Language (Natural Language Processing). The implementation is available at google-research/FLAN It is written in Python. The repository has 1,554 stars, indicating high community interest."
  },
  {
    "input": "Describe the horovod/horovod repository that implements Horovod: fast and easy distributed deep learning in TensorFlow.",
    "output": "Distributed training framework for TensorFlow, Keras, PyTorch, and Apache MXNet."
  },
  {
    "input": "In what year did FLAN: Finetuned Language Models Are Zero-Shot Learners come out?",
    "output": "The FLAN: Finetuned Language Models Are Zero-Shot Learners paper was published in 2021."
  },
  {
    "input": "Explain the RoFormer: Enhanced Transformer with Rotary Position Embedding paper.",
    "output": "RoFormer: Enhanced Transformer with Rotary Position Embedding is a 2021 paper in Computation and Language (Natural Language Processing). The implementation is available at ZhuiyiTechnology/roformer which rotary transformer It is written in Python. The repository has 1,051 stars, indicating high community interest."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2010.11929?",
    "output": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"
  },
  {
    "input": "What paper has ArXiv ID 2111.00396?",
    "output": "S4: Efficiently Modeling Long Sequences"
  },
  {
    "input": "Which field does BLIP: Bootstrapping Language-Image Pre-training belong to?",
    "output": "BLIP: Bootstrapping Language-Image Pre-training is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "What does the frankxu2004/knnlm-why repository do?",
    "output": "Repo for ICML23 \"Why do Nearest Neighbor Language Models Work?\""
  },
  {
    "input": "What's the implementation repository for The Power of Scale for Parameter-Efficient Prompt Tuning?",
    "output": "The code for The Power of Scale for Parameter-Efficient Prompt Tuning is available at https://github.com/google-research/prompt-tuning (google-research/prompt-tuning)."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2301.01947&start=0&max_results=10 implementation?",
    "output": "The oughtinc/ice repository has 563 stars and 70 forks on GitHub."
  },
  {
    "input": "Give me an overview of MiniGPT-4: Enhancing Vision-Language Understanding.",
    "output": "MiniGPT-4: Enhancing Vision-Language Understanding is a 2023 paper in Computer Vision and Pattern Recognition. The implementation is available at Vision-CAIR/MiniGPT-4 which open-sourced codes for minigpt-4 and minigpt-v2 (https://minigpt-4.github.io, https://minigpt-v2.github.io/) It is written in Python. The repository has 25,756 stars, indicating high community interest."
  },
  {
    "input": "How popular is the DistilBERT: a distilled version of BERT implementation?",
    "output": "The huggingface/transformers repository has 152,994 stars and 31,218 forks on GitHub."
  },
  {
    "input": "What category is the LLaMA-Adapter: Efficient Fine-tuning of Language Models paper in?",
    "output": "LLaMA-Adapter: Efficient Fine-tuning of Language Models is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2302.00902&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2302.00902&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2502.01061&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2502.01061&start=0&max_results=10 is available at https://github.com/GAIR-NLP/LIMO (GAIR-NLP/LIMO)."
  },
  {
    "input": "What is the ArXiv category of Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context?",
    "output": "Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What does the allenai/codescientist repository do?",
    "output": "CodeScientist: An automated scientific discovery system for code-based experiments"
  },
  {
    "input": "Describe the ibm-granite/granite-code-models repository that implements arXiv Query: search_query=&id_list=2405.03520&start=0&max_results=10.",
    "output": "Granite Code Models: A Family of Open Foundation Models for Code Intelligence"
  },
  {
    "input": "Where can I find the code for Neural Collaborative Filtering?",
    "output": "The code for Neural Collaborative Filtering is available at https://github.com/hexiangnan/neural_collaborative_filtering (hexiangnan/neural_collaborative_filtering)."
  },
  {
    "input": "Summarize the Adam: A Method for Stochastic Optimization paper.",
    "output": "Adam: A Method for Stochastic Optimization is a 2014 paper in Machine Learning. The implementation is available at pytorch/pytorch which tensors and dynamic neural networks in python with strong gpu acceleration It is written in Python. The repository has 95,348 stars, indicating high community interest."
  },
  {
    "input": "What is Finetuned Language Models Are Zero-Shot Learners about?",
    "output": "Finetuned Language Models Are Zero-Shot Learners is a 2021 paper in Computation and Language (Natural Language Processing). The implementation is available at google-research/FLAN It is written in Python. The repository has 1,554 stars, indicating high community interest."
  },
  {
    "input": "What language is the pytorch/pytorch repository written in?",
    "output": "The Adam: A Method for Stochastic Optimization implementation is written in Python."
  },
  {
    "input": "What is the GitHub repository for ColBERT: Efficient Passage Search via Contextualized Late Interaction?",
    "output": "The code for ColBERT: Efficient Passage Search via Contextualized Late Interaction is available at https://github.com/stanford-futuredata/ColBERT (stanford-futuredata/ColBERT)."
  },
  {
    "input": "When was the MMLU: Measuring Massive Multitask Language Understanding paper published?",
    "output": "The MMLU: Measuring Massive Multitask Language Understanding paper was published in 2021."
  },
  {
    "input": "What is the ArXiv category of EfficientDet: Scalable and Efficient Object Detection?",
    "output": "EfficientDet: Scalable and Efficient Object Detection is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2410.18050&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2410.18050&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "In what year did InstructBLIP: Towards General-purpose Vision-Language Models come out?",
    "output": "The InstructBLIP: Towards General-purpose Vision-Language Models paper was published in 2023."
  },
  {
    "input": "What category is the Time-LLM: Time Series Forecasting by Reprogramming Large Language Models paper in?",
    "output": "Time-LLM: Time Series Forecasting by Reprogramming Large Language Models is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What year was AWQ: Activation-aware Weight Quantization for LLM Compression released?",
    "output": "The AWQ: Activation-aware Weight Quantization for LLM Compression paper was published in 2023."
  },
  {
    "input": "Summarize the Code Llama: Open Foundation Models for Code paper.",
    "output": "Code Llama: Open Foundation Models for Code is a 2023 paper in Machine Learning. The implementation is available at meta-llama/codellama which inference code for codellama models It is written in Python. The repository has 16,365 stars, indicating high community interest."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2503.22708&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2503.22708&start=0&max_results=10 is available at https://github.com/allenai/codescientist (allenai/codescientist)."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2312.02783&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2312.02783&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "How popular is the SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding implementation?",
    "output": "The nyu-mll/jiant repository has 1,672 stars and 297 forks on GitHub."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2301.00303&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2301.00303&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2303.04129&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2303.04129&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "Give me an overview of Wide & Deep Learning for Recommender Systems.",
    "output": "Wide & Deep Learning for Recommender Systems is a 2016 paper in Information Retrieval. The implementation is available at tensorflow/models which models and examples built with tensorflow It is written in Python. The repository has 77,677 stars, indicating high community interest."
  },
  {
    "input": "What is the GitHub repository for Learning to summarize from human feedback?",
    "output": "The code for Learning to summarize from human feedback is available at https://github.com/openai/summarize-from-feedback (openai/summarize-from-feedback)."
  },
  {
    "input": "What topics does the arXiv Query: search_query=&id_list=2302.04023&start=0&max_results=10 implementation cover?",
    "output": "The mit-han-lab/offsite-tuning repository covers: deep-learning, transfer-learning."
  },
  {
    "input": "In what year did Universal Language Model Fine-tuning for Text Classification come out?",
    "output": "The Universal Language Model Fine-tuning for Text Classification paper was published in 2018."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2302.10866&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2302.10866&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at HazyResearch/safari which convolutions for sequence modeling It is written in Assembly. The repository has 904 stars, indicating high community interest."
  },
  {
    "input": "In what year did BLIP-2: Bootstrapping Language-Image Pre-training come out?",
    "output": "The BLIP-2: Bootstrapping Language-Image Pre-training paper was published in 2023."
  },
  {
    "input": "What topics does the Adam: A Method for Stochastic Optimization implementation cover?",
    "output": "The pytorch/pytorch repository covers: neural-network, autograd, gpu, numpy, deep-learning, tensor, python, machine-learning."
  },
  {
    "input": "How popular is the LLaMA: Open and Efficient Foundation Language Models implementation?",
    "output": "The meta-llama/llama repository has 58,946 stars and 9,817 forks on GitHub."
  },
  {
    "input": "What does the HazyResearch/safari repository do?",
    "output": "Convolutions for Sequence Modeling"
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2305.06500?",
    "output": "InstructBLIP: Towards General-purpose Vision-Language Models"
  },
  {
    "input": "Tell me the title of ArXiv paper 1905.09418.",
    "output": "BERT Rediscovers the Classical NLP Pipeline"
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2104.00355&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2104.00355&start=0&max_results=10 is a 2021 paper in Machine Learning. The implementation is available at facebookresearch/fairseq which facebook ai research sequence-to-sequence toolkit written in python. It is written in Python. The repository has 31,988 stars, indicating high community interest."
  },
  {
    "input": "Describe the hexiangnan/neural_collaborative_filtering repository that implements Neural Collaborative Filtering.",
    "output": "Neural Collaborative Filtering"
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2504.00698&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2504.00698&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at allenai/codescientist which codescientist: an automated scientific discovery system for code-based experiments It is written in Python. The repository has 301 stars, indicating high community interest."
  },
  {
    "input": "Give me an overview of InPars: Data Augmentation for Information Retrieval.",
    "output": "InPars: Data Augmentation for Information Retrieval is a 2022 paper in Information Retrieval. The implementation is available at zetaalphavector/InPars which inquisitive parrots for search It is written in Python. The repository has 198 stars, indicating high community interest."
  },
  {
    "input": "What is Segment Anything in Medical Images about?",
    "output": "Segment Anything in Medical Images is a 2023 paper in Computer Vision and Pattern Recognition. The implementation is available at bowang-lab/MedSAM which segment anything in medical images It is written in Jupyter Notebook. The repository has 3,930 stars, indicating high community interest."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2301.12652&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2301.12652&start=0&max_results=10 is available at https://github.com/amazon-science/mm-cot (amazon-science/mm-cot)."
  },
  {
    "input": "Describe the MoonshotAI/Kimi-Audio repository that implements arXiv Query: search_query=&id_list=2504.21801&start=0&max_results=10.",
    "output": "Kimi-Audio, an open-source audio foundation model excelling in audio understanding, generation, and conversation"
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2301.01181&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2301.01181&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at JohnNay/llm-lobbyist which code for the paper: \"large language models as corporate lobbyists\" (2023). It is written in Jupyter Notebook. The repository has 172 stars, indicating high community interest."
  },
  {
    "input": "Which field does Very Deep Convolutional Networks for Large-Scale Image Recognition belong to?",
    "output": "Very Deep Convolutional Networks for Large-Scale Image Recognition is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "Tell me the title of ArXiv paper 2503.23513.",
    "output": "arXiv Query: search_query=&id_list=2503.23513&start=0&max_results=10"
  },
  {
    "input": "What programming language is Wav2Vec 2.0: A Framework for Self-Supervised Learning of Speech implemented in?",
    "output": "The Wav2Vec 2.0: A Framework for Self-Supervised Learning of Speech implementation is written in Python."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2504.01848&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2504.01848&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Tell me the title of ArXiv paper 2002.05202.",
    "output": "Reformer: The Efficient Transformer"
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2012.06009&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2012.06009&start=0&max_results=10 is available at https://github.com/facebookresearch/CovidPrognosis (facebookresearch/CovidPrognosis)."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2306.00978?",
    "output": "AWQ: Activation-aware Weight Quantization for LLM Compression"
  },
  {
    "input": "Summarize the SimCSE: Simple Contrastive Learning of Sentence Embeddings paper.",
    "output": "SimCSE: Simple Contrastive Learning of Sentence Embeddings is a 2021 paper in Computation and Language (Natural Language Processing). The implementation is available at princeton-nlp/SimCSE which [emnlp 2021] simcse: simple contrastive learning of sentence embeddings https://arxiv.org/abs/2104.08821 It is written in Python. The repository has 3,615 stars, indicating high community interest."
  },
  {
    "input": "Explain the Animate Anyone: Consistent and Controllable Image-to-Video Synthesis paper.",
    "output": "Animate Anyone: Consistent and Controllable Image-to-Video Synthesis is a 2023 paper in Computer Vision and Pattern Recognition. The implementation is available at HumanAIGC/AnimateAnyone which animate anyone: consistent and controllable image-to-video synthesis for character animation The repository has 14,797 stars, indicating high community interest."
  },
  {
    "input": "Which field does DreamBooth: Fine Tuning Text-to-Image Diffusion Models belong to?",
    "output": "DreamBooth: Fine Tuning Text-to-Image Diffusion Models is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2302.03011&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2302.03011&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Which field does Zephyr: Direct Distillation of LM Alignment belong to?",
    "output": "Zephyr: Direct Distillation of LM Alignment is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What is the GitHub repository for LLM.int8(): 8-bit Matrix Multiplication for Transformers?",
    "output": "The code for LLM.int8(): 8-bit Matrix Multiplication for Transformers is available at https://github.com/bitsandbytes-foundation/bitsandbytes (bitsandbytes-foundation/bitsandbytes)."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2301.02828?",
    "output": "arXiv Query: search_query=&id_list=2301.02828&start=0&max_results=10"
  },
  {
    "input": "Explain the Feature Pyramid Networks for Object Detection paper.",
    "output": "Feature Pyramid Networks for Object Detection is a 2016 paper in Computer Vision and Pattern Recognition. The implementation is available at facebookresearch/Detectron which fair's research platform for object detection research, implementing popular algorithms like mask r-cnn and retinanet. It is written in Python. The repository has 26,396 stars, indicating high community interest."
  },
  {
    "input": "What paper has ArXiv ID 2307.00109?",
    "output": "Med-PaLM 2: Towards Expert-Level Medical Question Answering"
  },
  {
    "input": "What paper has ArXiv ID 2406.12824?",
    "output": "arXiv Query: search_query=&id_list=2406.12824&start=0&max_results=10"
  },
  {
    "input": "What language is the simplescaling/s1 repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2501.19393&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Where can I find the code for FlashAttention: Fast and Memory-Efficient Exact Attention?",
    "output": "The code for FlashAttention: Fast and Memory-Efficient Exact Attention is available at https://github.com/Dao-AILab/flash-attention (Dao-AILab/flash-attention)."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2505.22954&start=0&max_results=10 implementation?",
    "output": "The ruixin31/Spurious_Rewards repository has 342 stars and 20 forks on GitHub."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2403.17887&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2403.17887&start=0&max_results=10 paper was published in 2024."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 1810.04805?",
    "output": "BERT: Pre-training of Deep Bidirectional Transformers"
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2301.12652&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2301.12652&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at amazon-science/mm-cot which official implementation for \"multimodal chain-of-thought reasoning in language models\" (stay tuned and more will be updated) It is written in Python. The repository has 3,982 stars, indicating high community interest."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2301.01751&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2301.01751&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What paper has ArXiv ID 2505.22101?",
    "output": "arXiv Query: search_query=&id_list=2505.22101&start=0&max_results=10"
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2406.12824&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2406.12824&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2405.03548&start=0&max_results=10 implementation?",
    "output": "The ibm-granite/granite-code-models repository has 1,240 stars and 88 forks on GitHub."
  },
  {
    "input": "Describe the FMInference/FlexLLMGen repository that implements arXiv Query: search_query=&id_list=2303.06865&start=0&max_results=10.",
    "output": "Running large language models on a single GPU for throughput-oriented scenarios."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2302.00923&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2302.00923&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at amazon-science/mm-cot which official implementation for \"multimodal chain-of-thought reasoning in language models\" (stay tuned and more will be updated) It is written in Python. The repository has 3,982 stars, indicating high community interest."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2007.16122&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2007.16122&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is the ArXiv category of RoFormer: Enhanced Transformer with Rotary Position Embedding?",
    "output": "RoFormer: Enhanced Transformer with Rotary Position Embedding is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Explain the KimMeen/Time-LLM implementation of Time-LLM: Time Series Forecasting by Reprogramming Large Language Models.",
    "output": "[ICLR 2024] Official implementation of \" \ud83e\udd99 Time-LLM: Time Series Forecasting by Reprogramming Large Language Models\""
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2302.03917?",
    "output": "arXiv Query: search_query=&id_list=2302.03917&start=0&max_results=10"
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2504.16084&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2504.16084&start=0&max_results=10 is available at https://github.com/Crista23/goal_directedness_llms (Crista23/goal_directedness_llms)."
  },
  {
    "input": "What does the sentient-agi/OpenDeepSearch repository do?",
    "output": "SOTA search powered LLM"
  },
  {
    "input": "What is the GitHub repository for Decoupled Weight Decay Regularization?",
    "output": "The code for Decoupled Weight Decay Regularization is available at https://github.com/pytorch/pytorch (pytorch/pytorch)."
  },
  {
    "input": "What is the GitHub repository for Semi-Supervised Classification with Graph Convolutional Networks?",
    "output": "The code for Semi-Supervised Classification with Graph Convolutional Networks is available at https://github.com/tkipf/gcn (tkipf/gcn)."
  },
  {
    "input": "What does the mit-han-lab/offsite-tuning repository do?",
    "output": "Offsite-Tuning: Transfer Learning without Full Model"
  },
  {
    "input": "What language is the KimMeen/Time-LLM repository written in?",
    "output": "The Time-LLM: Time Series Forecasting by Reprogramming Large Language Models implementation is written in Python."
  },
  {
    "input": "What topics does the Flamingo: A Visual Language Model for Few-Shot Learning implementation cover?",
    "output": "The lucidrains/flamingo-pytorch repository covers: artificial-intelligence, attention-mechanism, deep-learning, transformers, visual-question-answering."
  },
  {
    "input": "Explain the openai/summarize-from-feedback implementation of Learning to summarize from human feedback.",
    "output": "Code for \"Learning to summarize from human feedback\""
  },
  {
    "input": "What does the amazon-science/mm-cot repository do?",
    "output": "Official implementation for \"Multimodal Chain-of-Thought Reasoning in Language Models\" (stay tuned and more will be updated)"
  },
  {
    "input": "What does the haoliuhl/language-quantized-autoencoders repository do?",
    "output": "Language Quantized AutoEncoders"
  },
  {
    "input": "Give me an overview of ALBERT: A Lite BERT.",
    "output": "ALBERT: A Lite BERT is a 2019 paper in Computation and Language (Natural Language Processing). The implementation is available at google-research/albert which albert: a lite bert for self-supervised learning of language representations It is written in Python. The repository has 3,274 stars, indicating high community interest."
  },
  {
    "input": "What programming language is DeBERTa: Decoding-enhanced BERT with Disentangled Attention implemented in?",
    "output": "The DeBERTa: Decoding-enhanced BERT with Disentangled Attention implementation is written in Python."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2504.02732&start=0&max_results=10 repository have?",
    "output": "The MAXNORM8650/MedAgentSim repository has 94 stars and 11 forks on GitHub."
  },
  {
    "input": "What is the GitHub repository for Efficient Estimation of Word Representations in Vector Space?",
    "output": "The code for Efficient Estimation of Word Representations in Vector Space is available at https://github.com/tmikolov/word2vec (tmikolov/word2vec)."
  },
  {
    "input": "In what year did PyTorch: An Imperative Style, High-Performance Deep Learning Library come out?",
    "output": "The PyTorch: An Imperative Style, High-Performance Deep Learning Library paper was published in 2019."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2310.06825?",
    "output": "Mistral 7B"
  },
  {
    "input": "What year was Know What You Don't Know: Unanswerable Questions for SQuAD released?",
    "output": "The Know What You Don't Know: Unanswerable Questions for SQuAD paper was published in 2018."
  },
  {
    "input": "How popular is the MetaGPT: Meta Programming for Multi-Agent Systems implementation?",
    "output": "The FoundationAgents/MetaGPT repository has 59,669 stars and 7,305 forks on GitHub."
  },
  {
    "input": "Explain the TensorFlow: A System for Large-Scale Machine Learning paper.",
    "output": "TensorFlow: A System for Large-Scale Machine Learning is a 2016 paper in Distributed, Parallel, and Cluster Computing. The implementation is available at tensorflow/tensorflow which an open source machine learning framework for everyone It is written in C++. The repository has 192,543 stars, indicating high community interest."
  },
  {
    "input": "How many stars does the ChemCrow: Augmenting large-language models with chemistry tools repository have?",
    "output": "The ur-whitelab/chemcrow-public repository has 838 stars and 130 forks on GitHub."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2302.09778&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2302.09778&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What category is the Learning to summarize from human feedback paper in?",
    "output": "Learning to summarize from human feedback is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "In what year did Code Llama: Open Foundation Models for Code come out?",
    "output": "The Code Llama: Open Foundation Models for Code paper was published in 2023."
  },
  {
    "input": "Where can I find the code for SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities?",
    "output": "The code for SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities is available at https://github.com/0nutation/SpeechGPT (0nutation/SpeechGPT)."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2405.03520&start=0&max_results=10 repository have?",
    "output": "The ibm-granite/granite-code-models repository has 1,240 stars and 88 forks on GitHub."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2501.11067?",
    "output": "arXiv Query: search_query=&id_list=2501.11067&start=0&max_results=10"
  },
  {
    "input": "What programming language is PyTorch Geometric: Library for Geometric Deep Learning implemented in?",
    "output": "The PyTorch Geometric: Library for Geometric Deep Learning implementation is written in Python."
  },
  {
    "input": "What topics does the RoBERTa: A Robustly Optimized BERT Pretraining Approach implementation cover?",
    "output": "The facebookresearch/fairseq repository covers: python, pytorch, artificial-intelligence."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2502.03387&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2502.03387&start=0&max_results=10 is available at https://github.com/GAIR-NLP/LIMO (GAIR-NLP/LIMO)."
  },
  {
    "input": "What are the key topics in the alphadl/lookahead.pytorch repository?",
    "output": "The alphadl/lookahead.pytorch repository covers: pytorch, optimizer, lookahead."
  },
  {
    "input": "How many stars does the Faster R-CNN: Towards Real-Time Object Detection repository have?",
    "output": "The rbgirshick/py-faster-rcnn repository has 8,269 stars and 4,102 forks on GitHub."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2302.10866&start=0&max_results=10 implementation?",
    "output": "The HazyResearch/safari repository has 904 stars and 70 forks on GitHub."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2104.00355&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2104.00355&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is the ArXiv category of Decoupled Weight Decay Regularization?",
    "output": "Decoupled Weight Decay Regularization is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2303.04129?",
    "output": "arXiv Query: search_query=&id_list=2303.04129&start=0&max_results=10"
  },
  {
    "input": "What year was InstructBLIP: Towards General-purpose Vision-Language Models released?",
    "output": "The InstructBLIP: Towards General-purpose Vision-Language Models paper was published in 2023."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2505.23006&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2505.23006&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at ruixin31/Spurious_Rewards It is written in Python. The repository has 342 stars, indicating high community interest."
  },
  {
    "input": "Give me an overview of Diffusion Models Beat GANs on Image Synthesis.",
    "output": "Diffusion Models Beat GANs on Image Synthesis is a 2021 paper in Computer Vision and Pattern Recognition. The implementation is available at openai/guided-diffusion It is written in Python. The repository has 7,161 stars, indicating high community interest."
  },
  {
    "input": "Give me an overview of BLIP: Bootstrapping Language-Image Pre-training.",
    "output": "BLIP: Bootstrapping Language-Image Pre-training is a 2022 paper in Computer Vision and Pattern Recognition. The implementation is available at salesforce/BLIP which pytorch code for blip: bootstrapping language-image pre-training for unified vision-language understanding and generation   It is written in Jupyter Notebook. The repository has 5,583 stars, indicating high community interest."
  },
  {
    "input": "Summarize the High-Resolution Image Synthesis with Latent Diffusion Models paper.",
    "output": "High-Resolution Image Synthesis with Latent Diffusion Models is a 2021 paper in Computer Vision and Pattern Recognition. The implementation is available at CompVis/latent-diffusion which high-resolution image synthesis with latent diffusion models It is written in Jupyter Notebook. The repository has 13,589 stars, indicating high community interest."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2101.04909&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2101.04909&start=0&max_results=10 paper was published in 2021."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2504.02732?",
    "output": "arXiv Query: search_query=&id_list=2504.02732&start=0&max_results=10"
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2410.18050&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2410.18050&start=0&max_results=10 is a 2024 paper in Machine Learning. The implementation is available at ibm-granite/granite-3.0-language-models The repository has 267 stars, indicating high community interest."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2301.01947&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2301.01947&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at oughtinc/ice which interactive composition explorer: a debugger for compositional language model programs It is written in Python. The repository has 563 stars, indicating high community interest."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2410.18050?",
    "output": "arXiv Query: search_query=&id_list=2410.18050&start=0&max_results=10"
  },
  {
    "input": "What are the key topics in the FoundationAgents/MetaGPT repository?",
    "output": "The FoundationAgents/MetaGPT repository covers: agent, gpt, llm, metagpt, multi-agent."
  },
  {
    "input": "Explain the KaimingHe/deep-residual-networks implementation of Deep Residual Learning for Image Recognition.",
    "output": "Deep Residual Learning for Image Recognition "
  },
  {
    "input": "What year was Denoising Diffusion Implicit Models released?",
    "output": "The Denoising Diffusion Implicit Models paper was published in 2020."
  },
  {
    "input": "What is the ArXiv category of AudioPaLM: A Large Language Model for Speech Understanding?",
    "output": "AudioPaLM: A Large Language Model for Speech Understanding is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What are the key topics in the pyg-team/pytorch_geometric repository?",
    "output": "The pyg-team/pytorch_geometric repository covers: pytorch, geometric-deep-learning, graph-neural-networks, deep-learning, graph-convolutional-networks."
  },
  {
    "input": "Give me an overview of RetNet: Retentive Network: A Successor to Transformer.",
    "output": "RetNet: Retentive Network: A Successor to Transformer is a 2022 paper in Machine Learning. The implementation is available at microsoft/torchscale which foundation architecture for (m)llms It is written in Python. The repository has 3,121 stars, indicating high community interest."
  },
  {
    "input": "What year was RoFormer: Enhanced Transformer with Rotary Position Embedding released?",
    "output": "The RoFormer: Enhanced Transformer with Rotary Position Embedding paper was published in 2021."
  },
  {
    "input": "Explain the StarCoder: May the source be with you! paper.",
    "output": "StarCoder: May the source be with you! is a 2023 paper in Machine Learning. The implementation is available at bigcode-project/starcoder which home of starcoder: fine-tuning & inference! It is written in Python. The repository has 7,473 stars, indicating high community interest."
  },
  {
    "input": "What is RetNet: Retentive Network: A Successor to Transformer about?",
    "output": "RetNet: Retentive Network: A Successor to Transformer is a 2022 paper in Machine Learning. The implementation is available at microsoft/torchscale which foundation architecture for (m)llms It is written in Python. The repository has 3,121 stars, indicating high community interest."
  },
  {
    "input": "What year was PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel released?",
    "output": "The PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel paper was published in 2022."
  },
  {
    "input": "How popular is the Distilling the Knowledge in a Neural Network implementation?",
    "output": "The haitongli/knowledge-distillation-pytorch repository has 1,973 stars and 351 forks on GitHub."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2303.03846&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2303.03846&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at HazyResearch/safari which convolutions for sequence modeling It is written in Assembly. The repository has 904 stars, indicating high community interest."
  },
  {
    "input": "What's the implementation repository for Decoupled Weight Decay Regularization?",
    "output": "The code for Decoupled Weight Decay Regularization is available at https://github.com/pytorch/pytorch (pytorch/pytorch)."
  },
  {
    "input": "When was the DreamBooth: Fine Tuning Text-to-Image Diffusion Models paper published?",
    "output": "The DreamBooth: Fine Tuning Text-to-Image Diffusion Models paper was published in 2022."
  },
  {
    "input": "Where can I find the code for The Power of Scale for Parameter-Efficient Prompt Tuning?",
    "output": "The code for The Power of Scale for Parameter-Efficient Prompt Tuning is available at https://github.com/google-research/prompt-tuning (google-research/prompt-tuning)."
  },
  {
    "input": "Explain the lamini-ai/Lamini-Memory-Tuning implementation of arXiv Query: search_query=&id_list=2406.09403&start=0&max_results=10.",
    "output": "Banishing LLM Hallucinations Requires Rethinking Generalization"
  },
  {
    "input": "What year was Competition-Level Code Generation with AlphaCode released?",
    "output": "The Competition-Level Code Generation with AlphaCode paper was published in 2022."
  },
  {
    "input": "Summarize the LLaVA-1.5: Improved Baselines with Visual Instruction Tuning paper.",
    "output": "LLaVA-1.5: Improved Baselines with Visual Instruction Tuning is a 2023 paper in Computer Vision and Pattern Recognition. The implementation is available at haotian-liu/LLaVA which [neurips'23 oral] visual instruction tuning (llava) built towards gpt-4v level capabilities and beyond. It is written in Python. The repository has 24,034 stars, indicating high community interest."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2302.04023&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2302.04023&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2301.11316&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2301.11316&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "Describe the google-research/albert repository that implements ALBERT: A Lite BERT.",
    "output": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"
  },
  {
    "input": "When was the Neural Collaborative Filtering paper published?",
    "output": "The Neural Collaborative Filtering paper was published in 2017."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2504.11844&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2504.11844&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Summarize the Feature Pyramid Networks for Object Detection paper.",
    "output": "Feature Pyramid Networks for Object Detection is a 2016 paper in Computer Vision and Pattern Recognition. The implementation is available at facebookresearch/Detectron which fair's research platform for object detection research, implementing popular algorithms like mask r-cnn and retinanet. It is written in Python. The repository has 26,396 stars, indicating high community interest."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2505.23006?",
    "output": "arXiv Query: search_query=&id_list=2505.23006&start=0&max_results=10"
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2101.04909&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2101.04909&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What paper has ArXiv ID 1708.05031?",
    "output": "Neural Collaborative Filtering"
  },
  {
    "input": "How popular is the InPars: Data Augmentation for Information Retrieval implementation?",
    "output": "The zetaalphavector/InPars repository has 198 stars and 24 forks on GitHub."
  },
  {
    "input": "Describe the HazyResearch/safari repository that implements arXiv Query: search_query=&id_list=2303.04226&start=0&max_results=10.",
    "output": "Convolutions for Sequence Modeling"
  },
  {
    "input": "What category is the Linformer: Self-Attention with Linear Complexity paper in?",
    "output": "Linformer: Self-Attention with Linear Complexity is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Describe the facebookresearch/fairseq repository that implements arXiv Query: search_query=&id_list=2104.00355&start=0&max_results=10.",
    "output": "Facebook AI Research Sequence-to-Sequence Toolkit written in Python."
  },
  {
    "input": "Explain the jasonwei20/eda_nlp implementation of EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification.",
    "output": "Data augmentation for NLP, presented at EMNLP 2019"
  },
  {
    "input": "What language is the allenai/longformer repository written in?",
    "output": "The Longformer: The Long-Document Transformer implementation is written in Python."
  },
  {
    "input": "Tell me the title of ArXiv paper 1503.02531.",
    "output": "Distilling the Knowledge in a Neural Network"
  },
  {
    "input": "What paper has ArXiv ID 1612.03144?",
    "output": "Feature Pyramid Networks for Object Detection"
  },
  {
    "input": "What programming language is CLIP: Learning Transferable Visual Models From Natural Language implemented in?",
    "output": "The CLIP: Learning Transferable Visual Models From Natural Language implementation is written in Jupyter Notebook."
  },
  {
    "input": "What topics does the Decoupled Weight Decay Regularization implementation cover?",
    "output": "The pytorch/pytorch repository covers: neural-network, autograd, gpu, numpy, deep-learning, tensor, python, machine-learning."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2505.22954&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2505.22954&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2302.10866&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2302.10866&start=0&max_results=10 is available at https://github.com/HazyResearch/safari (HazyResearch/safari)."
  },
  {
    "input": "Which field does DistilBERT: a distilled version of BERT belong to?",
    "output": "DistilBERT: a distilled version of BERT is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What topics does the XLNet: Generalized Autoregressive Pretraining implementation cover?",
    "output": "The zihangdai/xlnet repository covers: tensorflow, nlp, deep-learning."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2503.01141&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2503.01141&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What category is the Text Embeddings by Weakly-Supervised Contrastive Pre-training paper in?",
    "output": "Text Embeddings by Weakly-Supervised Contrastive Pre-training is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Explain the meta-llama/llama implementation of LLaMA: Open and Efficient Foundation Language Models.",
    "output": "Inference code for Llama models"
  },
  {
    "input": "Summarize the EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification paper.",
    "output": "EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification is a 2019 paper in Computation and Language (Natural Language Processing). The implementation is available at jasonwei20/eda_nlp which data augmentation for nlp, presented at emnlp 2019 It is written in Python. The repository has 1,649 stars, indicating high community interest."
  },
  {
    "input": "Tell me the title of ArXiv paper 1907.11692.",
    "output": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"
  },
  {
    "input": "Describe the Megvii-BaseDetection/YOLOX repository that implements YOLOX: Exceeding YOLO Series in 2021.",
    "output": "YOLOX is a high-performance anchor-free YOLO, exceeding yolov3~v5 with MegEngine, ONNX, TensorRT, ncnn, and OpenVINO supported. Documentation: https://yolox.readthedocs.io/"
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2303.02913&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2303.02913&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What's the implementation repository for InstructBLIP: Towards General-purpose Vision-Language Models?",
    "output": "The code for InstructBLIP: Towards General-purpose Vision-Language Models is available at https://github.com/salesforce/LAVIS (salesforce/LAVIS)."
  },
  {
    "input": "What paper has ArXiv ID 2301.04104?",
    "output": "arXiv Query: search_query=&id_list=2301.04104&start=0&max_results=10"
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2504.00698&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2504.00698&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "When was the EfficientDet: Scalable and Efficient Object Detection paper published?",
    "output": "The EfficientDet: Scalable and Efficient Object Detection paper was published in 2020."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2303.09431&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2303.09431&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at FMInference/FlexLLMGen which running large language models on a single gpu for throughput-oriented scenarios. It is written in Python. The repository has 9,379 stars, indicating high community interest."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2502.03387&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2502.03387&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2301.00774&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2301.00774&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "In what year did Flamingo: A Visual Language Model for Few-Shot Learning come out?",
    "output": "The Flamingo: A Visual Language Model for Few-Shot Learning paper was published in 2022."
  },
  {
    "input": "What is the ArXiv category of Attention Is All You Need?",
    "output": "Attention Is All You Need is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What language is the chenfei-wu/TaskMatrix repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2303.04671&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What is the GitHub repository for ELECTRA: Pre-training Text Encoders as Discriminators?",
    "output": "The code for ELECTRA: Pre-training Text Encoders as Discriminators is available at https://github.com/google-research/electra (google-research/electra)."
  },
  {
    "input": "What language is the Open-Reasoner-Zero/Open-Reasoner-Zero repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2502.09992&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Give me an overview of ViT: An Image is Worth 16x16 Words.",
    "output": "ViT: An Image is Worth 16x16 Words is a 2021 paper in Computer Vision and Pattern Recognition. The implementation is available at google-research/vision_transformer It is written in Jupyter Notebook. The repository has 12,040 stars, indicating high community interest."
  },
  {
    "input": "Describe the lucidrains/muse-maskgit-pytorch repository that implements arXiv Query: search_query=&id_list=2301.00704&start=0&max_results=10.",
    "output": "Implementation of Muse: Text-to-Image Generation via Masked Generative Transformers, in Pytorch"
  },
  {
    "input": "In what year did Segment Anything in Medical Images come out?",
    "output": "The Segment Anything in Medical Images paper was published in 2023."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2312.02783&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2312.02783&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "Explain the MAXNORM8650/MedAgentSim implementation of arXiv Query: search_query=&id_list=2503.22678&start=0&max_results=10.",
    "output": "MedAgentSim: Self-Evolving Multi-Agent Simulations for Realistic Clinical Interactions, MICCAI 2025 (oral and early accepted)"
  },
  {
    "input": "When was the ELECTRA: Pre-training Text Encoders as Discriminators paper published?",
    "output": "The ELECTRA: Pre-training Text Encoders as Discriminators paper was published in 2019."
  },
  {
    "input": "Tell me the title of ArXiv paper 1901.02860.",
    "output": "Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context"
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2303.01469?",
    "output": "arXiv Query: search_query=&id_list=2303.01469&start=0&max_results=10"
  },
  {
    "input": "Summarize the TinyBERT: Distilling BERT for Natural Language Understanding paper.",
    "output": "TinyBERT: Distilling BERT for Natural Language Understanding is a 2019 paper in Computation and Language (Natural Language Processing). The implementation is available at huawei-noah/Pretrained-Language-Model which pretrained language model and its related optimization techniques developed by huawei noah's ark lab. It is written in Python. The repository has 3,149 stars, indicating high community interest."
  },
  {
    "input": "What's the implementation repository for P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning?",
    "output": "The code for P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning is available at https://github.com/THUDM/P-tuning-v2 (THUDM/P-tuning-v2)."
  },
  {
    "input": "When was the Horovod: fast and easy distributed deep learning in TensorFlow paper published?",
    "output": "The Horovod: fast and easy distributed deep learning in TensorFlow paper was published in 2017."
  },
  {
    "input": "What language is the openai/evals repository written in?",
    "output": "The GPT-4 Technical Report implementation is written in Python."
  },
  {
    "input": "Tell me the title of ArXiv paper 2010.02502.",
    "output": "Denoising Diffusion Implicit Models"
  },
  {
    "input": "What does the noahshinn/reflexion repository do?",
    "output": "[NeurIPS 2023] Reflexion: Language Agents with Verbal Reinforcement Learning"
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2503.00735&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2503.00735&start=0&max_results=10 is available at https://github.com/LTH14/fractalgen (LTH14/fractalgen)."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2301.10343&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2301.10343&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2304.02643?",
    "output": "Segment Anything"
  },
  {
    "input": "Explain the salesforce/LAVIS implementation of BLIP-2: Bootstrapping Language-Image Pre-training.",
    "output": "LAVIS - A One-stop Library for Language-Vision Intelligence"
  },
  {
    "input": "What is the GitHub repository for Densely Connected Convolutional Networks?",
    "output": "The code for Densely Connected Convolutional Networks is available at https://github.com/liuzhuang13/DenseNet (liuzhuang13/DenseNet)."
  },
  {
    "input": "Explain the simplescaling/s1 implementation of arXiv Query: search_query=&id_list=2501.19393&start=0&max_results=10.",
    "output": "s1: Simple test-time scaling"
  },
  {
    "input": "Describe the tatsu-lab/alpaca_eval repository that implements AlpacaEval: An Automatic Evaluator for Instruction-following.",
    "output": "An automatic evaluator for instruction-following language models. Human-validated, high-quality, cheap, and fast."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2303.08774?",
    "output": "GPT-4 Technical Report"
  },
  {
    "input": "Summarize the Masked Autoencoders Are Scalable Vision Learners paper.",
    "output": "Masked Autoencoders Are Scalable Vision Learners is a 2022 paper in Computer Vision and Pattern Recognition. The implementation is available at facebookresearch/mae which pytorch implementation of mae https//arxiv.org/abs/2111.06377 It is written in Python. The repository has 8,108 stars, indicating high community interest."
  },
  {
    "input": "What programming language is LIMA: Less Is More for Alignment implemented in?",
    "output": "The LIMA: Less Is More for Alignment implementation is written in Python."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2301.00303&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2301.00303&start=0&max_results=10 is a 2022 paper in Machine Learning. The implementation is available at facebookresearch/ConvNeXt-V2 which code release for convnext v2 model It is written in Python. The repository has 1,890 stars, indicating high community interest."
  },
  {
    "input": "What category is the S4: Efficiently Modeling Long Sequences paper in?",
    "output": "S4: Efficiently Modeling Long Sequences is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2303.11331?",
    "output": "Grounding DINO: Marrying DINO with Grounded Pre-Training"
  },
  {
    "input": "Describe the mit-han-lab/offsite-tuning repository that implements arXiv Query: search_query=&id_list=2302.03011&start=0&max_results=10.",
    "output": "Offsite-Tuning: Transfer Learning without Full Model"
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2101.04909&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2101.04909&start=0&max_results=10 paper was published in 2021."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2405.03548&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2405.03548&start=0&max_results=10 paper was published in 2024."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2303.03378?",
    "output": "arXiv Query: search_query=&id_list=2303.03378&start=0&max_results=10"
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2302.03011&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2302.03011&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "When was the SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding paper published?",
    "output": "The SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding paper was published in 2019."
  },
  {
    "input": "Tell me the title of ArXiv paper 2312.00752.",
    "output": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces"
  },
  {
    "input": "Where can I find the code for ColBERT: Efficient Passage Search via Contextualized Late Interaction?",
    "output": "The code for ColBERT: Efficient Passage Search via Contextualized Late Interaction is available at https://github.com/stanford-futuredata/ColBERT (stanford-futuredata/ColBERT)."
  },
  {
    "input": "How many stars does the LLM.int8(): 8-bit Matrix Multiplication for Transformers repository have?",
    "output": "The bitsandbytes-foundation/bitsandbytes repository has 7,771 stars and 797 forks on GitHub."
  },
  {
    "input": "How popular is the Med-PaLM 2: Towards Expert-Level Medical Question Answering implementation?",
    "output": "The google-research/google-research repository has 36,800 stars and 8,255 forks on GitHub."
  },
  {
    "input": "Summarize the Grounding DINO: Marrying DINO with Grounded Pre-Training paper.",
    "output": "Grounding DINO: Marrying DINO with Grounded Pre-Training is a 2023 paper in Computer Vision and Pattern Recognition. The implementation is available at IDEA-Research/GroundingDINO which [eccv 2024] official implementation of the paper \"grounding dino: marrying dino with grounded pre-training for open-set object detection\" It is written in Python. The repository has 9,324 stars, indicating high community interest."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2502.01061&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2502.01061&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at GAIR-NLP/LIMO which [colm 2025] limo: less is more for reasoning It is written in Python. The repository has 1,052 stars, indicating high community interest."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2302.09778&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2302.09778&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at ali-vilab/composer which official implementation of \"composer: creative and controllable image synthesis with composable conditions\" The repository has 1,560 stars, indicating high community interest."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2405.02803&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2405.02803&start=0&max_results=10 is a 2024 paper in Machine Learning. The implementation is available at ibm-granite/granite-code-models which granite code models: a family of open foundation models for code intelligence The repository has 1,240 stars, indicating high community interest."
  },
  {
    "input": "What year was Enriching Word Vectors with Subword Information released?",
    "output": "The Enriching Word Vectors with Subword Information paper was published in 2016."
  },
  {
    "input": "How many stars does the ColBERT: Efficient Passage Search via Contextualized Late Interaction repository have?",
    "output": "The stanford-futuredata/ColBERT repository has 3,707 stars and 462 forks on GitHub."
  },
  {
    "input": "In what year did XLNet: Generalized Autoregressive Pretraining come out?",
    "output": "The XLNet: Generalized Autoregressive Pretraining paper was published in 2019."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2002.05202?",
    "output": "Reformer: The Efficient Transformer"
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2303.02506&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2303.02506&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2504.00698&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2504.00698&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "Describe the google-research/bert repository that implements BERT: Pre-training of Deep Bidirectional Transformers.",
    "output": "TensorFlow code and pre-trained models for BERT"
  },
  {
    "input": "What paper has ArXiv ID 2012.06009?",
    "output": "arXiv Query: search_query=&id_list=2012.06009&start=0&max_results=10"
  },
  {
    "input": "Tell me the title of ArXiv paper 2406.06326.",
    "output": "arXiv Query: search_query=&id_list=2406.06326&start=0&max_results=10"
  },
  {
    "input": "Describe the IST-DASLab/gptq repository that implements GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers.",
    "output": "Code for the ICLR 2023 paper \"GPTQ: Accurate Post-training Quantization of Generative Pretrained Transformers\"."
  },
  {
    "input": "Describe the tmikolov/word2vec repository that implements Efficient Estimation of Word Representations in Vector Space.",
    "output": "Automatically exported from code.google.com/p/word2vec"
  },
  {
    "input": "Summarize the Enriching Word Vectors with Subword Information paper.",
    "output": "Enriching Word Vectors with Subword Information is a 2016 paper in Computation and Language (Natural Language Processing). The implementation is available at facebookresearch/fastText which library for fast text representation and classification. It is written in HTML. The repository has 26,432 stars, indicating high community interest."
  },
  {
    "input": "What topics does the arXiv Query: search_query=&id_list=2503.22678&start=0&max_results=10 implementation cover?",
    "output": "The MAXNORM8650/MedAgentSim repository covers: multi-agent-simulations."
  },
  {
    "input": "Tell me the title of ArXiv paper 2302.13971.",
    "output": "LLaMA: Open and Efficient Foundation Language Models"
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2502.12115&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2502.12115&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Tell me the title of ArXiv paper 1608.06993.",
    "output": "Densely Connected Convolutional Networks"
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2303.17580?",
    "output": "HuggingGPT: Solving AI Tasks with ChatGPT"
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2001.04451?",
    "output": "Longformer: The Long-Document Transformer"
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2504.16084&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2504.16084&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What language is the FMInference/FlexLLMGen repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2303.09431&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Describe the jax-ml/jax repository that implements JAX: Composable transformations of Python+NumPy programs.",
    "output": "Composable transformations of Python+NumPy programs: differentiate, vectorize, JIT to GPU/TPU, and more"
  },
  {
    "input": "What is the GitHub repository for InstructBLIP: Towards General-purpose Vision-Language Models?",
    "output": "The code for InstructBLIP: Towards General-purpose Vision-Language Models is available at https://github.com/salesforce/LAVIS (salesforce/LAVIS)."
  },
  {
    "input": "What topics does the arXiv Query: search_query=&id_list=2303.09431&start=0&max_results=10 implementation cover?",
    "output": "The FMInference/FlexLLMGen repository covers: deep-learning, gpt-3, high-throughput, large-language-models, machine-learning, offloading, opt."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 1706.03059?",
    "output": "Depthwise Separable Convolutions for Neural Machine Translation"
  },
  {
    "input": "What programming language is Contriever: Unsupervised Dense Information Retrieval implemented in?",
    "output": "The Contriever: Unsupervised Dense Information Retrieval implementation is written in Python."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2302.14838&start=0&max_results=10 repository have?",
    "output": "The ruiqi-zhong/D5 repository has 71 stars and 13 forks on GitHub."
  },
  {
    "input": "What year was Universal Language Model Fine-tuning for Text Classification released?",
    "output": "The Universal Language Model Fine-tuning for Text Classification paper was published in 2018."
  },
  {
    "input": "When was the InPars: Data Augmentation for Information Retrieval paper published?",
    "output": "The InPars: Data Augmentation for Information Retrieval paper was published in 2022."
  },
  {
    "input": "What are the key topics in the PetarV-/GAT repository?",
    "output": "The PetarV-/GAT repository covers: graph-attention-networks, attention-mechanism, self-attention, tensorflow, neural-networks, python."
  },
  {
    "input": "Give me an overview of Synthetic Data Generation with Large Language Models.",
    "output": "Synthetic Data Generation with Large Language Models is a 2023 paper in Machine Learning. The implementation is available at google-research/google-research which google research It is written in Jupyter Notebook. The repository has 36,800 stars, indicating high community interest."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2503.22708&start=0&max_results=10 repository have?",
    "output": "The allenai/codescientist repository has 301 stars and 37 forks on GitHub."
  },
  {
    "input": "What year was Sequence to Sequence Learning with Neural Networks released?",
    "output": "The Sequence to Sequence Learning with Neural Networks paper was published in 2014."
  },
  {
    "input": "Summarize the LLaMA: Open and Efficient Foundation Language Models paper.",
    "output": "LLaMA: Open and Efficient Foundation Language Models is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at meta-llama/llama which inference code for llama models It is written in Python. The repository has 58,946 stars, indicating high community interest."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2302.05206&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2302.05206&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at tianjunz/HIR It is written in Python. The repository has 159 stars, indicating high community interest."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2302.04023&start=0&max_results=10 repository have?",
    "output": "The mit-han-lab/offsite-tuning repository has 382 stars and 39 forks on GitHub."
  },
  {
    "input": "What's the implementation repository for Horovod: fast and easy distributed deep learning in TensorFlow?",
    "output": "The code for Horovod: fast and easy distributed deep learning in TensorFlow is available at https://github.com/horovod/horovod (horovod/horovod)."
  },
  {
    "input": "Explain the zetaalphavector/InPars implementation of InPars: Data Augmentation for Information Retrieval.",
    "output": "Inquisitive Parrots for Search"
  },
  {
    "input": "What is the title of the paper with ArXiv ID 1909.11942?",
    "output": "ELECTRA: Pre-training Text Encoders as Discriminators"
  },
  {
    "input": "How popular is the PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel implementation?",
    "output": "The pytorch/pytorch repository has 95,348 stars and 26,003 forks on GitHub."
  },
  {
    "input": "Tell me the title of ArXiv paper 1804.09301.",
    "output": "Generating Wikipedia by Summarizing Long Sequences"
  },
  {
    "input": "What is the ArXiv category of Reformer: The Efficient Transformer?",
    "output": "Reformer: The Efficient Transformer is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Tell me the title of ArXiv paper 1905.07830.",
    "output": "SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding"
  },
  {
    "input": "Explain the hpcaitech/Open-Sora implementation of arXiv Query: search_query=&id_list=2406.07394&start=0&max_results=10.",
    "output": "Open-Sora: Democratizing Efficient Video Production for All"
  },
  {
    "input": "Tell me the title of ArXiv paper 2303.01469.",
    "output": "arXiv Query: search_query=&id_list=2303.01469&start=0&max_results=10"
  },
  {
    "input": "What is the title of the paper with ArXiv ID 1909.10351?",
    "output": "DistilBERT: a distilled version of BERT"
  },
  {
    "input": "What's the implementation repository for Direct Preference Optimization: Your Language Model is a Reward Model?",
    "output": "The code for Direct Preference Optimization: Your Language Model is a Reward Model is available at https://github.com/eric-mitchell/direct-preference-optimization (eric-mitchell/direct-preference-optimization)."
  },
  {
    "input": "How popular is the QLoRA: Efficient Finetuning of Quantized LLMs implementation?",
    "output": "The artidoro/qlora repository has 10,764 stars and 866 forks on GitHub."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2303.08518&start=0&max_results=10 repository have?",
    "output": "The FMInference/FlexLLMGen repository has 9,379 stars and 584 forks on GitHub."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2107.07346&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2107.07346&start=0&max_results=10 paper was published in 2021."
  },
  {
    "input": "Describe the FMInference/FlexLLMGen repository that implements arXiv Query: search_query=&id_list=2303.09431&start=0&max_results=10.",
    "output": "Running large language models on a single GPU for throughput-oriented scenarios."
  },
  {
    "input": "What does the rajpurkar/SQuAD-explorer repository do?",
    "output": "Visually Explore the Stanford Question Answering Dataset"
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2410.18050&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2410.18050&start=0&max_results=10 paper was published in 2024."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2403.17887&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2403.17887&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2501.13545&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2501.13545&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at plurai-ai/intellagent which a framework for comprehensive diagnosis and optimization of agents using simulated, realistic synthetic interactions It is written in Python. The repository has 1,148 stars, indicating high community interest."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2108.12409?",
    "output": "Linformer: Self-Attention with Linear Complexity"
  },
  {
    "input": "What paper has ArXiv ID 2302.12246?",
    "output": "arXiv Query: search_query=&id_list=2302.12246&start=0&max_results=10"
  },
  {
    "input": "What are the key topics in the facebookresearch/fairseq repository?",
    "output": "The facebookresearch/fairseq repository covers: python, pytorch, artificial-intelligence."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2312.02120&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2312.02120&start=0&max_results=10 is available at https://github.com/ContextualAI/HALOs (ContextualAI/HALOs)."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2405.03548&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2405.03548&start=0&max_results=10 paper was published in 2024."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2303.02506?",
    "output": "arXiv Query: search_query=&id_list=2303.02506&start=0&max_results=10"
  },
  {
    "input": "How popular is the Finetuned Language Models Are Zero-Shot Learners implementation?",
    "output": "The google-research/FLAN repository has 1,554 stars and 161 forks on GitHub."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2302.05206&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2302.05206&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What paper has ArXiv ID 2504.20734?",
    "output": "arXiv Query: search_query=&id_list=2504.20734&start=0&max_results=10"
  },
  {
    "input": "Summarize the Beir: A Heterogeneous Benchmark for IR paper.",
    "output": "Beir: A Heterogeneous Benchmark for IR is a 2022 paper in Information Retrieval. The implementation is available at beir-cellar/beir which a heterogeneous benchmark for information retrieval. easy to use, evaluate your models across 15+ diverse ir datasets. It is written in Python. The repository has 2,007 stars, indicating high community interest."
  },
  {
    "input": "What are the key topics in the microsoft/LoRA repository?",
    "output": "The microsoft/LoRA repository covers: gpt-2, adaptation, language-model, gpt-3, low-rank, pytorch, deep-learning, roberta, deberta, lora."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2504.21801&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2504.21801&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at MoonshotAI/Kimi-Audio which kimi-audio, an open-source audio foundation model excelling in audio understanding, generation, and conversation It is written in Python. The repository has 4,367 stars, indicating high community interest."
  },
  {
    "input": "How many stars does the Gorilla: Large Language Model Connected with Massive APIs repository have?",
    "output": "The ShishirPatil/gorilla repository has 12,574 stars and 1,283 forks on GitHub."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2112.04426?",
    "output": "ColBERT: Efficient Passage Search via Contextualized Late Interaction"
  },
  {
    "input": "What paper has ArXiv ID 2005.14165?",
    "output": "GPT-3: Language Models are Few-Shot Learners"
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2303.04129&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2303.04129&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at HazyResearch/safari which convolutions for sequence modeling It is written in Assembly. The repository has 904 stars, indicating high community interest."
  },
  {
    "input": "How many stars does the Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context repository have?",
    "output": "The kimiyoung/transformer-xl repository has 3,679 stars and 766 forks on GitHub."
  },
  {
    "input": "What language is the pyg-team/pytorch_geometric repository written in?",
    "output": "The PyTorch Geometric: Library for Geometric Deep Learning implementation is written in Python."
  },
  {
    "input": "Tell me the title of ArXiv paper 2009.06367.",
    "output": "arXiv Query: search_query=&id_list=2009.06367&start=0&max_results=10"
  },
  {
    "input": "What is Vicuna: An Open-Source Chatbot about?",
    "output": "Vicuna: An Open-Source Chatbot is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at lm-sys/FastChat which an open platform for training, serving, and evaluating large language models. release repo for vicuna and chatbot arena. It is written in Python. The repository has 39,271 stars, indicating high community interest."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2406.09403&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2406.09403&start=0&max_results=10 is a 2024 paper in Machine Learning. The implementation is available at lamini-ai/Lamini-Memory-Tuning which banishing llm hallucinations requires rethinking generalization The repository has 275 stars, indicating high community interest."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2504.20734&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2504.20734&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What does the williamleif/GraphSAGE repository do?",
    "output": "Representation learning on large graphs using stochastic graph convolutions."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2006.03511&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2006.03511&start=0&max_results=10 paper was published in 2020."
  },
  {
    "input": "Describe the pytorch/pytorch repository that implements PyTorch: An Imperative Style, High-Performance Deep Learning Library.",
    "output": "Tensors and Dynamic neural networks in Python with strong GPU acceleration"
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2312.02783&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2312.02783&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at ContextualAI/HALOs which a library with extensible implementations of dpo, kto, ppo, orpo, and other human-aware loss functions (halos). It is written in Python. The repository has 893 stars, indicating high community interest."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2502.03387&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2502.03387&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at GAIR-NLP/LIMO which [colm 2025] limo: less is more for reasoning It is written in Python. The repository has 1,052 stars, indicating high community interest."
  },
  {
    "input": "What paper has ArXiv ID 2106.05237?",
    "output": "Wav2Vec 2.0: A Framework for Self-Supervised Learning of Speech"
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2404.03592&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2404.03592&start=0&max_results=10 is available at https://github.com/OpenBMB/Eurus (OpenBMB/Eurus)."
  },
  {
    "input": "What category is the CLIP: Learning Transferable Visual Models From Natural Language paper in?",
    "output": "CLIP: Learning Transferable Visual Models From Natural Language is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "What is Billion-scale similarity search with GPUs about?",
    "output": "Billion-scale similarity search with GPUs is a 2019 paper in Computer Vision and Pattern Recognition. The implementation is available at facebookresearch/faiss which a library for efficient similarity search and clustering of dense vectors. It is written in C++. The repository has 38,143 stars, indicating high community interest."
  },
  {
    "input": "What is MRKL Systems: A modular approach to AGI about?",
    "output": "MRKL Systems: A modular approach to AGI is a 2022 paper in Artificial Intelligence. The implementation is available at langchain-ai/langchain which \ud83e\udd9c\ud83d\udd17 the platform for reliable agents. It is written in Python. The repository has 120,451 stars, indicating high community interest."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2301.04104&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2301.04104&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What's the implementation repository for Reflexion: Language Agents with Verbal Reinforcement Learning?",
    "output": "The code for Reflexion: Language Agents with Verbal Reinforcement Learning is available at https://github.com/noahshinn/reflexion (noahshinn/reflexion)."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2504.01848?",
    "output": "arXiv Query: search_query=&id_list=2504.01848&start=0&max_results=10"
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2107.07346&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2107.07346&start=0&max_results=10 is a 2021 paper in Machine Learning. The implementation is available at jacopotagliabue/you-dont-need-a-bigger-boat which an end-to-end implementation of intent prediction with metaflow and other cool tools It is written in Python. The repository has 873 stars, indicating high community interest."
  },
  {
    "input": "What does the QingruZhang/AdaLoRA repository do?",
    "output": "AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning (ICLR 2023). "
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2503.01141?",
    "output": "arXiv Query: search_query=&id_list=2503.01141&start=0&max_results=10"
  },
  {
    "input": "Describe the Open-Reasoner-Zero/Open-Reasoner-Zero repository that implements arXiv Query: search_query=&id_list=2502.09992&start=0&max_results=10.",
    "output": "Official Repo for Open-Reasoner-Zero"
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2502.12115&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2502.12115&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2406.12824&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2406.12824&start=0&max_results=10 is available at https://github.com/hpcaitech/Open-Sora (hpcaitech/Open-Sora)."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2301.09515&start=0&max_results=10 repository have?",
    "output": "The autonomousvision/stylegan-t repository has 1,196 stars and 61 forks on GitHub."
  },
  {
    "input": "What language is the lucidrains/audiolm-pytorch repository written in?",
    "output": "The AudioPaLM: A Large Language Model for Speech Understanding implementation is written in Python."
  },
  {
    "input": "What is the GitHub repository for Animate Anyone: Consistent and Controllable Image-to-Video Synthesis?",
    "output": "The code for Animate Anyone: Consistent and Controllable Image-to-Video Synthesis is available at https://github.com/HumanAIGC/AnimateAnyone (HumanAIGC/AnimateAnyone)."
  },
  {
    "input": "Summarize the Proximal Policy Optimization Algorithms paper.",
    "output": "Proximal Policy Optimization Algorithms is a 2017 paper in Machine Learning. The implementation is available at openai/baselines which openai baselines: high-quality implementations of reinforcement learning algorithms It is written in Python. The repository has 16,566 stars, indicating high community interest."
  },
  {
    "input": "What's the implementation repository for Vicuna: An Open-Source Chatbot?",
    "output": "The code for Vicuna: An Open-Source Chatbot is available at https://github.com/lm-sys/FastChat (lm-sys/FastChat)."
  },
  {
    "input": "What year was Self-RAG: Learning to Retrieve, Generate, and Critique released?",
    "output": "The Self-RAG: Learning to Retrieve, Generate, and Critique paper was published in 2022."
  },
  {
    "input": "Summarize the Whisper: Robust Speech Recognition via Large-Scale Weak Supervision paper.",
    "output": "Whisper: Robust Speech Recognition via Large-Scale Weak Supervision is a 2022 paper in Computation and Language (Natural Language Processing). The implementation is available at openai/whisper which robust speech recognition via large-scale weak supervision It is written in Python. The repository has 91,236 stars, indicating high community interest."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2004.13637&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2004.13637&start=0&max_results=10 is available at https://github.com/facebookresearch/TransCoder (facebookresearch/TransCoder)."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2303.03378&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2303.03378&start=0&max_results=10 is available at https://github.com/NVlabs/prismer (NVlabs/prismer)."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2503.23513&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2503.23513&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "How popular is the DreamFusion: Text-to-3D using 2D Diffusion implementation?",
    "output": "The ashawkey/stable-dreamfusion repository has 8,773 stars and 768 forks on GitHub."
  },
  {
    "input": "What topics does the MRKL Systems: A modular approach to AGI implementation cover?",
    "output": "The langchain-ai/langchain repository covers: ai, anthropic, gemini, langchain, llm, openai, python, agents, ai-agents, ai-agents-framework, chatgpt, enterprise, framework, generative-ai, multiagent, open-source, pydantic, rag, aiagentframework."
  },
  {
    "input": "How many stars does the Masked Autoencoders Are Scalable Vision Learners repository have?",
    "output": "The facebookresearch/mae repository has 8,108 stars and 1,332 forks on GitHub."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2301.01947&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2301.01947&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at oughtinc/ice which interactive composition explorer: a debugger for compositional language model programs It is written in Python. The repository has 563 stars, indicating high community interest."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2406.10209&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2406.10209&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What year was ViT: An Image is Worth 16x16 Words released?",
    "output": "The ViT: An Image is Worth 16x16 Words paper was published in 2021."
  },
  {
    "input": "What language is the LTH14/fractalgen repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2502.19328&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2303.06865&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2303.06865&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at FMInference/FlexLLMGen which running large language models on a single gpu for throughput-oriented scenarios. It is written in Python. The repository has 9,379 stars, indicating high community interest."
  },
  {
    "input": "Tell me the title of ArXiv paper 2302.12766.",
    "output": "arXiv Query: search_query=&id_list=2302.12766&start=0&max_results=10"
  },
  {
    "input": "What are the key topics in the noahshinn/reflexion repository?",
    "output": "The noahshinn/reflexion repository covers: ai, artificial-intelligence, llm."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2302.05206&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2302.05206&start=0&max_results=10 is available at https://github.com/tianjunz/HIR (tianjunz/HIR)."
  },
  {
    "input": "What's the implementation repository for Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks?",
    "output": "The code for Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks is available at https://github.com/huggingface/sentence-transformers (huggingface/sentence-transformers)."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2301.10343&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2301.10343&start=0&max_results=10 is available at https://github.com/autonomousvision/stylegan-t (autonomousvision/stylegan-t)."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2406.10209&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2406.10209&start=0&max_results=10 is available at https://github.com/hpcaitech/Open-Sora (hpcaitech/Open-Sora)."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2302.00902&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2302.00902&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at haoliuhl/language-quantized-autoencoders which language quantized autoencoders It is written in Python. The repository has 111 stars, indicating high community interest."
  },
  {
    "input": "Give me an overview of Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context.",
    "output": "Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context is a 2019 paper in Computation and Language (Natural Language Processing). The implementation is available at kimiyoung/transformer-xl It is written in Python. The repository has 3,679 stars, indicating high community interest."
  },
  {
    "input": "What is the ArXiv category of Animate Anyone: Consistent and Controllable Image-to-Video Synthesis?",
    "output": "Animate Anyone: Consistent and Controllable Image-to-Video Synthesis is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "What's the implementation repository for CLIP: Learning Transferable Visual Models From Natural Language?",
    "output": "The code for CLIP: Learning Transferable Visual Models From Natural Language is available at https://github.com/openai/CLIP (openai/CLIP)."
  },
  {
    "input": "What is GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers about?",
    "output": "GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers is a 2022 paper in Machine Learning. The implementation is available at IST-DASLab/gptq which code for the iclr 2023 paper \"gptq: accurate post-training quantization of generative pretrained transformers\". It is written in Python. The repository has 2,221 stars, indicating high community interest."
  },
  {
    "input": "Explain the DistilBERT: a distilled version of BERT paper.",
    "output": "DistilBERT: a distilled version of BERT is a 2019 paper in Computation and Language (Natural Language Processing). The implementation is available at huggingface/transformers which \ud83e\udd17 transformers: the model-definition framework for state-of-the-art machine learning models in text, vision, audio, and multimodal models, for both inference and training.  It is written in Python. The repository has 152,994 stars, indicating high community interest."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2303.02506&start=0&max_results=10 repository have?",
    "output": "The NVlabs/prismer repository has 1,307 stars and 74 forks on GitHub."
  },
  {
    "input": "Describe the IDEA-Research/GroundingDINO repository that implements Grounding DINO: Marrying DINO with Grounded Pre-Training.",
    "output": "[ECCV 2024] Official implementation of the paper \"Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection\""
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2303.03846&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2303.03846&start=0&max_results=10 is available at https://github.com/HazyResearch/safari (HazyResearch/safari)."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2403.17887&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2403.17887&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is the ArXiv category of Generating Wikipedia by Summarizing Long Sequences?",
    "output": "Generating Wikipedia by Summarizing Long Sequences is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "In what year did Deep Graph Library: A Graph-Centric, Highly-Performant Package come out?",
    "output": "The Deep Graph Library: A Graph-Centric, Highly-Performant Package paper was published in 2020."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2302.09778&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2302.09778&start=0&max_results=10 is available at https://github.com/ali-vilab/composer (ali-vilab/composer)."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2502.09992&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2502.09992&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2405.03548&start=0&max_results=10 repository have?",
    "output": "The ibm-granite/granite-code-models repository has 1,240 stars and 88 forks on GitHub."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2301.05062&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2301.05062&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at google-deepmind/tracr It is written in Python. The repository has 548 stars, indicating high community interest."
  },
  {
    "input": "Describe the langchain-ai/langchain repository that implements MRKL Systems: A modular approach to AGI.",
    "output": "\ud83e\udd9c\ud83d\udd17 The platform for reliable agents."
  },
  {
    "input": "Which field does Animate Anyone: Consistent and Controllable Image-to-Video Synthesis belong to?",
    "output": "Animate Anyone: Consistent and Controllable Image-to-Video Synthesis is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2302.03011&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2302.03011&start=0&max_results=10 is available at https://github.com/mit-han-lab/offsite-tuning (mit-han-lab/offsite-tuning)."
  },
  {
    "input": "What programming language is BERT: Pre-training of Deep Bidirectional Transformers implemented in?",
    "output": "The BERT: Pre-training of Deep Bidirectional Transformers implementation is written in Python."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2302.00902&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2302.00902&start=0&max_results=10 is available at https://github.com/haoliuhl/language-quantized-autoencoders (haoliuhl/language-quantized-autoencoders)."
  },
  {
    "input": "What is the ArXiv category of Dense Passage Retrieval for Open-Domain Question Answering?",
    "output": "Dense Passage Retrieval for Open-Domain Question Answering is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Describe the nlpxucan/WizardLM repository that implements WizardCoder: Empowering Code LLMs with Evol-Instruct.",
    "output": "LLMs build upon Evol Insturct: WizardLM, WizardCoder, WizardMath"
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2503.22678&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2503.22678&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "Explain the 0nutation/SpeechGPT implementation of SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities.",
    "output": "SpeechGPT Series: Speech Large Language Models"
  },
  {
    "input": "What is the title of the paper with ArXiv ID 1905.07830?",
    "output": "SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding"
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2303.02913&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2303.02913&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at Shark-NLP/OpenICL which openicl is an open-source framework to facilitate research, development, and prototyping of in-context learning. It is written in Python. The repository has 579 stars, indicating high community interest."
  },
  {
    "input": "Describe the ysymyth/ReAct repository that implements ReAct: Synergizing Reasoning and Acting in Language Models.",
    "output": "[ICLR 2023] ReAct: Synergizing Reasoning and Acting in Language Models"
  },
  {
    "input": "Tell me the title of ArXiv paper 2302.08242.",
    "output": "arXiv Query: search_query=&id_list=2302.08242&start=0&max_results=10"
  },
  {
    "input": "Describe the shizhediao/active-prompt repository that implements arXiv Query: search_query=&id_list=2302.12246&start=0&max_results=10.",
    "output": "Source code for the paper \"Active Prompting with Chain-of-Thought for Large Language Models\""
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2301.11316&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2301.11316&start=0&max_results=10 is available at https://github.com/autonomousvision/stylegan-t (autonomousvision/stylegan-t)."
  },
  {
    "input": "What is the GitHub repository for PaLM: Scaling Language Modeling with Pathways?",
    "output": "The code for PaLM: Scaling Language Modeling with Pathways is available at https://github.com/lucidrains/PaLM-pytorch (lucidrains/PaLM-pytorch)."
  },
  {
    "input": "What language is the state-spaces/s4 repository written in?",
    "output": "The S4: Efficiently Modeling Long Sequences implementation is written in Jupyter Notebook."
  },
  {
    "input": "Explain the EfficientDet: Scalable and Efficient Object Detection paper.",
    "output": "EfficientDet: Scalable and Efficient Object Detection is a 2020 paper in Computer Vision and Pattern Recognition. The implementation is available at google/automl which google brain automl It is written in Jupyter Notebook. The repository has 6,425 stars, indicating high community interest."
  },
  {
    "input": "Where can I find the code for Masked Autoencoders Are Scalable Vision Learners?",
    "output": "The code for Masked Autoencoders Are Scalable Vision Learners is available at https://github.com/facebookresearch/mae (facebookresearch/mae)."
  },
  {
    "input": "What paper has ArXiv ID 1706.03762?",
    "output": "Attention Is All You Need"
  },
  {
    "input": "What is InstructBLIP: Towards General-purpose Vision-Language Models about?",
    "output": "InstructBLIP: Towards General-purpose Vision-Language Models is a 2023 paper in Computer Vision and Pattern Recognition. The implementation is available at salesforce/LAVIS which lavis - a one-stop library for language-vision intelligence It is written in Jupyter Notebook. The repository has 11,040 stars, indicating high community interest."
  },
  {
    "input": "What does the CompVis/latent-diffusion repository do?",
    "output": "High-Resolution Image Synthesis with Latent Diffusion Models"
  },
  {
    "input": "What's the implementation repository for BERT: Pre-training of Deep Bidirectional Transformers?",
    "output": "The code for BERT: Pre-training of Deep Bidirectional Transformers is available at https://github.com/google-research/bert (google-research/bert)."
  },
  {
    "input": "Explain the PyTorch Geometric: Library for Geometric Deep Learning paper.",
    "output": "PyTorch Geometric: Library for Geometric Deep Learning is a 2019 paper in Machine Learning. The implementation is available at pyg-team/pytorch_geometric which graph neural network library for pytorch It is written in Python. The repository has 23,186 stars, indicating high community interest."
  },
  {
    "input": "Explain the state-spaces/mamba implementation of Mamba: Linear-Time Sequence Modeling with Selective State Spaces.",
    "output": "Mamba SSM architecture"
  },
  {
    "input": "Describe the allenai/codescientist repository that implements arXiv Query: search_query=&id_list=2503.22708&start=0&max_results=10.",
    "output": "CodeScientist: An automated scientific discovery system for code-based experiments"
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2302.00902&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2302.00902&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at haoliuhl/language-quantized-autoencoders which language quantized autoencoders It is written in Python. The repository has 111 stars, indicating high community interest."
  },
  {
    "input": "What is the GitHub repository for RoFormer: Enhanced Transformer with Rotary Position Embedding?",
    "output": "The code for RoFormer: Enhanced Transformer with Rotary Position Embedding is available at https://github.com/ZhuiyiTechnology/roformer (ZhuiyiTechnology/roformer)."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2301.01181&start=0&max_results=10 repository have?",
    "output": "The JohnNay/llm-lobbyist repository has 172 stars and 15 forks on GitHub."
  },
  {
    "input": "What programming language is The Flan Collection: Designing Data and Methods for Effective Instruction Tuning implemented in?",
    "output": "The The Flan Collection: Designing Data and Methods for Effective Instruction Tuning implementation is written in Python."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 1409.0473?",
    "output": "Sequence to Sequence Learning with Neural Networks"
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2406.07394&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2406.07394&start=0&max_results=10 paper was published in 2024."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2405.03520&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2405.03520&start=0&max_results=10 is available at https://github.com/ibm-granite/granite-code-models (ibm-granite/granite-code-models)."
  },
  {
    "input": "What's the implementation repository for ViT: An Image is Worth 16x16 Words?",
    "output": "The code for ViT: An Image is Worth 16x16 Words is available at https://github.com/google-research/vision_transformer (google-research/vision_transformer)."
  },
  {
    "input": "What language is the facebookresearch/CovidPrognosis repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2101.04909&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "In what year did ZeRO: Memory Optimizations Toward Training Trillion Parameter Models come out?",
    "output": "The ZeRO: Memory Optimizations Toward Training Trillion Parameter Models paper was published in 2019."
  },
  {
    "input": "How popular is the Adam: A Method for Stochastic Optimization implementation?",
    "output": "The pytorch/pytorch repository has 95,348 stars and 26,003 forks on GitHub."
  },
  {
    "input": "Describe the facebookresearch/fairseq repository that implements arXiv Query: search_query=&id_list=2102.01192&start=0&max_results=10.",
    "output": "Facebook AI Research Sequence-to-Sequence Toolkit written in Python."
  },
  {
    "input": "What is the GitHub repository for Graph Attention Networks?",
    "output": "The code for Graph Attention Networks is available at https://github.com/PetarV-/GAT (PetarV-/GAT)."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2303.04226&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2303.04226&start=0&max_results=10 is available at https://github.com/HazyResearch/safari (HazyResearch/safari)."
  },
  {
    "input": "What year was GPT-3: Language Models are Few-Shot Learners released?",
    "output": "The GPT-3: Language Models are Few-Shot Learners paper was published in 2020."
  },
  {
    "input": "Tell me the title of ArXiv paper 2208.01618.",
    "output": "DreamBooth: Fine Tuning Text-to-Image Diffusion Models"
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2403.17887&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2403.17887&start=0&max_results=10 is a 2024 paper in Machine Learning. The implementation is available at OpenBMB/Eurus It is written in Python. The repository has 320 stars, indicating high community interest."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2303.06865&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2303.06865&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What does the IDEA-Research/GroundingDINO repository do?",
    "output": "[ECCV 2024] Official implementation of the paper \"Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection\""
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2501.13545&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2501.13545&start=0&max_results=10 is available at https://github.com/plurai-ai/intellagent (plurai-ai/intellagent)."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2301.11305&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2301.11305&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Tell me the title of ArXiv paper 2312.02783.",
    "output": "arXiv Query: search_query=&id_list=2312.02783&start=0&max_results=10"
  },
  {
    "input": "Explain the HumanAIGC/AnimateAnyone implementation of Animate Anyone: Consistent and Controllable Image-to-Video Synthesis.",
    "output": "Animate Anyone: Consistent and Controllable Image-to-Video Synthesis for Character Animation"
  },
  {
    "input": "Which field does PyTorch: An Imperative Style, High-Performance Deep Learning Library belong to?",
    "output": "PyTorch: An Imperative Style, High-Performance Deep Learning Library is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What year was Time-LLM: Time Series Forecasting by Reprogramming Large Language Models released?",
    "output": "The Time-LLM: Time Series Forecasting by Reprogramming Large Language Models paper was published in 2023."
  },
  {
    "input": "Tell me the title of ArXiv paper 2306.04190.",
    "output": "ChemCrow: Augmenting large-language models with chemistry tools"
  },
  {
    "input": "Summarize the HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction paper.",
    "output": "HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction is a 2021 paper in Computation and Language (Natural Language Processing). The implementation is available at facebookresearch/fairseq which facebook ai research sequence-to-sequence toolkit written in python. It is written in Python. The repository has 31,985 stars, indicating high community interest."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2504.11844&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2504.11844&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Tell me the title of ArXiv paper 1807.11164.",
    "output": "YOLOX: Exceeding YOLO Series in 2021"
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2301.00704&start=0&max_results=10 repository have?",
    "output": "The lucidrains/muse-maskgit-pytorch repository has 916 stars and 88 forks on GitHub."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2302.08242&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2302.08242&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What category is the Mistral 7B paper in?",
    "output": "Mistral 7B is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Explain the amazon-science/mm-cot implementation of arXiv Query: search_query=&id_list=2302.00923&start=0&max_results=10.",
    "output": "Official implementation for \"Multimodal Chain-of-Thought Reasoning in Language Models\" (stay tuned and more will be updated)"
  },
  {
    "input": "What's the implementation repository for StarCoder: May the source be with you!?",
    "output": "The code for StarCoder: May the source be with you! is available at https://github.com/bigcode-project/starcoder (bigcode-project/starcoder)."
  },
  {
    "input": "Tell me the title of ArXiv paper 2305.06161.",
    "output": "StarCoder: May the source be with you!"
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2303.02506&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2303.02506&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2212.10496?",
    "output": "Self-RAG: Learning to Retrieve, Generate, and Critique"
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2403.17887&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2403.17887&start=0&max_results=10 is a 2024 paper in Machine Learning. The implementation is available at OpenBMB/Eurus It is written in Python. The repository has 320 stars, indicating high community interest."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2503.22708&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2503.22708&start=0&max_results=10 is available at https://github.com/allenai/codescientist (allenai/codescientist)."
  },
  {
    "input": "How popular is the Enriching Word Vectors with Subword Information implementation?",
    "output": "The facebookresearch/fastText repository has 26,432 stars and 4,813 forks on GitHub."
  },
  {
    "input": "Summarize the ControlNet: Adding Conditional Control to Text-to-Image Diffusion paper.",
    "output": "ControlNet: Adding Conditional Control to Text-to-Image Diffusion is a 2023 paper in Computer Vision and Pattern Recognition. The implementation is available at lllyasviel/ControlNet which let us control diffusion models! It is written in Python. The repository has 33,344 stars, indicating high community interest."
  },
  {
    "input": "What is the ArXiv category of Vicuna: An Open-Source Chatbot?",
    "output": "Vicuna: An Open-Source Chatbot is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Give me an overview of Graph Attention Networks.",
    "output": "Graph Attention Networks is a 2017 paper in Machine Learning. The implementation is available at PetarV-/GAT which graph attention networks (https://arxiv.org/abs/1710.10903) It is written in Python. The repository has 3,460 stars, indicating high community interest."
  },
  {
    "input": "Give me an overview of HuggingGPT: Solving AI Tasks with ChatGPT.",
    "output": "HuggingGPT: Solving AI Tasks with ChatGPT is a 2023 paper in Artificial Intelligence. The implementation is available at microsoft/JARVIS which jarvis, a system to connect llms with ml community. paper: https://arxiv.org/pdf/2303.17580.pdf It is written in Python. The repository has 24,460 stars, indicating high community interest."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2303.02506&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2303.02506&start=0&max_results=10 is available at https://github.com/NVlabs/prismer (NVlabs/prismer)."
  },
  {
    "input": "What is the ArXiv category of Gorilla: Large Language Model Connected with Massive APIs?",
    "output": "Gorilla: Large Language Model Connected with Massive APIs is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What language is the lm-sys/FastChat repository written in?",
    "output": "The Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena implementation is written in Python."
  },
  {
    "input": "What paper has ArXiv ID 2504.02732?",
    "output": "arXiv Query: search_query=&id_list=2504.02732&start=0&max_results=10"
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2406.09308&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2406.09308&start=0&max_results=10 is a 2024 paper in Machine Learning. The implementation is available at lamini-ai/Lamini-Memory-Tuning which banishing llm hallucinations requires rethinking generalization The repository has 275 stars, indicating high community interest."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2502.01061&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2502.01061&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Give me an overview of Lookahead Optimizer: k steps forward, 1 step back.",
    "output": "Lookahead Optimizer: k steps forward, 1 step back is a 2020 paper in Machine Learning. The implementation is available at alphadl/lookahead.pytorch which lookahead optimizer (lookahead optimizer: k steps forward, 1 step back) for pytorch  It is written in Python. The repository has 337 stars, indicating high community interest."
  },
  {
    "input": "What paper has ArXiv ID 1608.06993?",
    "output": "Densely Connected Convolutional Networks"
  },
  {
    "input": "Give me an overview of Longformer: The Long-Document Transformer.",
    "output": "Longformer: The Long-Document Transformer is a 2020 paper in Computation and Language (Natural Language Processing). The implementation is available at allenai/longformer which longformer: the long-document transformer It is written in Python. The repository has 2,175 stars, indicating high community interest."
  },
  {
    "input": "What does the artidoro/qlora repository do?",
    "output": "QLoRA: Efficient Finetuning of Quantized LLMs"
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2303.02506&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2303.02506&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What paper has ArXiv ID 2302.05206?",
    "output": "arXiv Query: search_query=&id_list=2302.05206&start=0&max_results=10"
  },
  {
    "input": "What does the dmlc/dgl repository do?",
    "output": "Python package built to ease deep learning on graph, on top of existing DL frameworks."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2405.03520&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2405.03520&start=0&max_results=10 paper was published in 2024."
  },
  {
    "input": "What does the ibm-granite/granite-code-models repository do?",
    "output": "Granite Code Models: A Family of Open Foundation Models for Code Intelligence"
  },
  {
    "input": "Explain the microsoft/JARVIS implementation of HuggingGPT: Solving AI Tasks with ChatGPT.",
    "output": "JARVIS, a system to connect LLMs with ML community. Paper: https://arxiv.org/pdf/2303.17580.pdf"
  },
  {
    "input": "What category is the Self-RAG: Learning to Retrieve, Generate, and Critique paper in?",
    "output": "Self-RAG: Learning to Retrieve, Generate, and Critique is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2504.02732&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2504.02732&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2302.10866&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2302.10866&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at HazyResearch/safari which convolutions for sequence modeling It is written in Assembly. The repository has 904 stars, indicating high community interest."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2502.14815&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2502.14815&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What paper has ArXiv ID 2302.09778?",
    "output": "arXiv Query: search_query=&id_list=2302.09778&start=0&max_results=10"
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2502.01061?",
    "output": "arXiv Query: search_query=&id_list=2502.01061&start=0&max_results=10"
  },
  {
    "input": "What does the facebookresearch/fairseq repository do?",
    "output": "Facebook AI Research Sequence-to-Sequence Toolkit written in Python."
  },
  {
    "input": "What is the ArXiv category of Med-PaLM 2: Towards Expert-Level Medical Question Answering?",
    "output": "Med-PaLM 2: Towards Expert-Level Medical Question Answering is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What paper has ArXiv ID 2307.03109?",
    "output": "AlpacaEval: An Automatic Evaluator for Instruction-following"
  },
  {
    "input": "What is RoBERTa: A Robustly Optimized BERT Pretraining Approach about?",
    "output": "RoBERTa: A Robustly Optimized BERT Pretraining Approach is a 2019 paper in Computation and Language (Natural Language Processing). The implementation is available at facebookresearch/fairseq which facebook ai research sequence-to-sequence toolkit written in python. It is written in Python. The repository has 31,985 stars, indicating high community interest."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2504.19413&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2504.19413&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "How many stars does the Wide & Deep Learning for Recommender Systems repository have?",
    "output": "The tensorflow/models repository has 77,677 stars and 45,409 forks on GitHub."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2012.06009&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2012.06009&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Describe the facebookresearch/CovidPrognosis repository that implements arXiv Query: search_query=&id_list=2101.04909&start=0&max_results=10.",
    "output": "COVID deterioration prediction based on chest X-ray radiographs via MoCo-trained image representations"
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2301.11316&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2301.11316&start=0&max_results=10 is available at https://github.com/autonomousvision/stylegan-t (autonomousvision/stylegan-t)."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2501.13545&start=0&max_results=10 implementation?",
    "output": "The plurai-ai/intellagent repository has 1,148 stars and 139 forks on GitHub."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2303.04226&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2303.04226&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "Tell me the title of ArXiv paper 2302.05206.",
    "output": "arXiv Query: search_query=&id_list=2302.05206&start=0&max_results=10"
  },
  {
    "input": "What's the implementation repository for BEiT: BERT Pre-Training of Image Transformers?",
    "output": "The code for BEiT: BERT Pre-Training of Image Transformers is available at https://github.com/microsoft/unilm (microsoft/unilm)."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2406.12824&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2406.12824&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Which field does YOLOX: Exceeding YOLO Series in 2021 belong to?",
    "output": "YOLOX: Exceeding YOLO Series in 2021 is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "When was the Beir: A Heterogeneous Benchmark for IR paper published?",
    "output": "The Beir: A Heterogeneous Benchmark for IR paper was published in 2022."
  },
  {
    "input": "What paper has ArXiv ID 2212.03533?",
    "output": "Text Embeddings by Weakly-Supervised Contrastive Pre-training"
  },
  {
    "input": "Describe the autonomousvision/stylegan-t repository that implements arXiv Query: search_query=&id_list=2301.11305&start=0&max_results=10.",
    "output": "[ICML'23] StyleGAN-T: Unlocking the Power of GANs for Fast Large-Scale Text-to-Image Synthesis"
  },
  {
    "input": "Explain the Denoising Diffusion Implicit Models paper.",
    "output": "Denoising Diffusion Implicit Models is a 2020 paper in Machine Learning. The implementation is available at ermongroup/ddim which denoising diffusion implicit models It is written in Python. The repository has 1,743 stars, indicating high community interest."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2405.03548&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2405.03548&start=0&max_results=10 paper was published in 2024."
  },
  {
    "input": "Summarize the Self-RAG: Learning to Retrieve, Generate, and Critique paper.",
    "output": "Self-RAG: Learning to Retrieve, Generate, and Critique is a 2022 paper in Computation and Language (Natural Language Processing). The implementation is available at AkariAsai/self-rag which this includes the original implementation of self-rag: learning to retrieve, generate and critique through self-reflection by akari asai, zeqiu wu, yizhong wang, avirup sil, and hannaneh hajishirzi. It is written in Python. The repository has 2,252 stars, indicating high community interest."
  },
  {
    "input": "Which field does ReAct: Synergizing Reasoning and Acting in Language Models belong to?",
    "output": "ReAct: Synergizing Reasoning and Acting in Language Models is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "How many stars does the Wav2Vec 2.0: A Framework for Self-Supervised Learning of Speech repository have?",
    "output": "The facebookresearch/fairseq repository has 31,985 stars and 6,630 forks on GitHub."
  },
  {
    "input": "Explain the InPars: Data Augmentation for Information Retrieval paper.",
    "output": "InPars: Data Augmentation for Information Retrieval is a 2022 paper in Information Retrieval. The implementation is available at zetaalphavector/InPars which inquisitive parrots for search It is written in Python. The repository has 198 stars, indicating high community interest."
  },
  {
    "input": "What paper has ArXiv ID 2301.01751?",
    "output": "arXiv Query: search_query=&id_list=2301.01751&start=0&max_results=10"
  },
  {
    "input": "Summarize the The Flan Collection: Designing Data and Methods for Effective Instruction Tuning paper.",
    "output": "The Flan Collection: Designing Data and Methods for Effective Instruction Tuning is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at google-research/FLAN It is written in Python. The repository has 1,554 stars, indicating high community interest."
  },
  {
    "input": "Tell me the title of ArXiv paper 2302.08575.",
    "output": "SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities"
  },
  {
    "input": "What does the frankxu2004/knnlm-why repository do?",
    "output": "Repo for ICML23 \"Why do Nearest Neighbor Language Models Work?\""
  },
  {
    "input": "Where can I find the code for Analyzing the Structure of Attention in a Transformer Language Model?",
    "output": "The code for Analyzing the Structure of Attention in a Transformer Language Model is available at https://github.com/clarkkev/attention-analysis (clarkkev/attention-analysis)."
  },
  {
    "input": "Which field does Neural Machine Translation by Jointly Learning to Align and Translate belong to?",
    "output": "Neural Machine Translation by Jointly Learning to Align and Translate is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What category is the FlashAttention-2: Faster Attention with Better Parallelism paper in?",
    "output": "FlashAttention-2: Faster Attention with Better Parallelism is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2302.00923&start=0&max_results=10 repository have?",
    "output": "The amazon-science/mm-cot repository has 3,982 stars and 333 forks on GitHub."
  },
  {
    "input": "What paper has ArXiv ID 2304.02643?",
    "output": "Segment Anything"
  },
  {
    "input": "What is the ArXiv category of SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities?",
    "output": "SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2303.08518&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2303.08518&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "Which field does Linformer: Self-Attention with Linear Complexity belong to?",
    "output": "Linformer: Self-Attention with Linear Complexity is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Summarize the PaLM: Scaling Language Modeling with Pathways paper.",
    "output": "PaLM: Scaling Language Modeling with Pathways is a 2022 paper in Computation and Language (Natural Language Processing). The implementation is available at lucidrains/PaLM-pytorch which implementation of the specific transformer architecture from palm - scaling language modeling with pathways It is written in Python. The repository has 826 stars, indicating high community interest."
  },
  {
    "input": "Explain the SimCSE: Simple Contrastive Learning of Sentence Embeddings paper.",
    "output": "SimCSE: Simple Contrastive Learning of Sentence Embeddings is a 2021 paper in Computation and Language (Natural Language Processing). The implementation is available at princeton-nlp/SimCSE which [emnlp 2021] simcse: simple contrastive learning of sentence embeddings https://arxiv.org/abs/2104.08821 It is written in Python. The repository has 3,615 stars, indicating high community interest."
  },
  {
    "input": "What paper has ArXiv ID 1802.05365?",
    "output": "Universal Language Model Fine-tuning for Text Classification"
  },
  {
    "input": "Summarize the Know What You Don't Know: Unanswerable Questions for SQuAD paper.",
    "output": "Know What You Don't Know: Unanswerable Questions for SQuAD is a 2018 paper in Computation and Language (Natural Language Processing). The implementation is available at rajpurkar/SQuAD-explorer which visually explore the stanford question answering dataset It is written in JavaScript. The repository has 570 stars, indicating high community interest."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2504.16084&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2504.16084&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at Crista23/goal_directedness_llms It is written in Python. The repository has 12 stars, indicating high community interest."
  },
  {
    "input": "What paper has ArXiv ID 2502.17437?",
    "output": "arXiv Query: search_query=&id_list=2502.17437&start=0&max_results=10"
  },
  {
    "input": "In what year did Neural Message Passing for Quantum Chemistry come out?",
    "output": "The Neural Message Passing for Quantum Chemistry paper was published in 2017."
  },
  {
    "input": "What does the google-research/google-research repository do?",
    "output": "Google Research"
  },
  {
    "input": "Where can I find the code for Codex: Evaluating Large Language Models Trained on Code?",
    "output": "The code for Codex: Evaluating Large Language Models Trained on Code is available at https://github.com/openai/human-eval (openai/human-eval)."
  },
  {
    "input": "Describe the lisa-groundhog/GroundHog repository that implements Neural Machine Translation by Jointly Learning to Align and Translate.",
    "output": "Library for implementing RNNs with Theano"
  },
  {
    "input": "What programming language is Horovod: fast and easy distributed deep learning in TensorFlow implemented in?",
    "output": "The Horovod: fast and easy distributed deep learning in TensorFlow implementation is written in Python."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2406.06326&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2406.06326&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2302.04023&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2302.04023&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "How many stars does the AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning repository have?",
    "output": "The QingruZhang/AdaLoRA repository has 361 stars and 36 forks on GitHub."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2501.19393&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2501.19393&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at simplescaling/s1 which s1: simple test-time scaling It is written in Python. The repository has 6,605 stars, indicating high community interest."
  },
  {
    "input": "What topics does the Zephyr: Direct Distillation of LM Alignment implementation cover?",
    "output": "The huggingface/alignment-handbook repository covers: llm, rlhf, transformers."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2406.07394&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2406.07394&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Which field does Adam: A Method for Stochastic Optimization belong to?",
    "output": "Adam: A Method for Stochastic Optimization is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Explain the IST-DASLab/gptq implementation of GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers.",
    "output": "Code for the ICLR 2023 paper \"GPTQ: Accurate Post-training Quantization of Generative Pretrained Transformers\"."
  },
  {
    "input": "What category is the EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification paper in?",
    "output": "EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2302.05206&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2302.05206&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Which field does Code Llama: Open Foundation Models for Code belong to?",
    "output": "Code Llama: Open Foundation Models for Code is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is the ArXiv category of Swin Transformer: Hierarchical Vision Transformer using Shifted Windows?",
    "output": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "What year was ZeRO: Memory Optimizations Toward Training Trillion Parameter Models released?",
    "output": "The ZeRO: Memory Optimizations Toward Training Trillion Parameter Models paper was published in 2019."
  },
  {
    "input": "What is the ArXiv category of JAX: Composable transformations of Python+NumPy programs?",
    "output": "JAX: Composable transformations of Python+NumPy programs is in the cs.MS (cs.MS) category."
  },
  {
    "input": "What's the implementation repository for Whisper: Robust Speech Recognition via Large-Scale Weak Supervision?",
    "output": "The code for Whisper: Robust Speech Recognition via Large-Scale Weak Supervision is available at https://github.com/openai/whisper (openai/whisper)."
  },
  {
    "input": "Give me an overview of WizardCoder: Empowering Code LLMs with Evol-Instruct.",
    "output": "WizardCoder: Empowering Code LLMs with Evol-Instruct is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at nlpxucan/WizardLM which llms build upon evol insturct: wizardlm, wizardcoder, wizardmath It is written in Python. The repository has 9,460 stars, indicating high community interest."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2301.11305?",
    "output": "arXiv Query: search_query=&id_list=2301.11305&start=0&max_results=10"
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2501.11067&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2501.11067&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2302.03011&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2302.03011&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What category is the GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers paper in?",
    "output": "GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2501.13545&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2501.13545&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What's the implementation repository for Efficient Estimation of Word Representations in Vector Space?",
    "output": "The code for Efficient Estimation of Word Representations in Vector Space is available at https://github.com/tmikolov/word2vec (tmikolov/word2vec)."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2303.04129&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2303.04129&start=0&max_results=10 implementation is written in Assembly."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2502.01061&start=0&max_results=10 implementation?",
    "output": "The GAIR-NLP/LIMO repository has 1,052 stars and 52 forks on GitHub."
  },
  {
    "input": "What category is the GPT-4 Technical Report paper in?",
    "output": "GPT-4 Technical Report is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Where can I find the code for CLIP: Learning Transferable Visual Models From Natural Language?",
    "output": "The code for CLIP: Learning Transferable Visual Models From Natural Language is available at https://github.com/openai/CLIP (openai/CLIP)."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 1906.04341?",
    "output": "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks"
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2502.03387&start=0&max_results=10 repository have?",
    "output": "The GAIR-NLP/LIMO repository has 1,052 stars and 52 forks on GitHub."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2502.14815&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2502.14815&start=0&max_results=10 is available at https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero (Open-Reasoner-Zero/Open-Reasoner-Zero)."
  },
  {
    "input": "How many stars does the Denoising Diffusion Probabilistic Models repository have?",
    "output": "The hojonathanho/diffusion repository has 4,860 stars and 455 forks on GitHub."
  },
  {
    "input": "What is the GitHub repository for LLaMA: Open and Efficient Foundation Language Models?",
    "output": "The code for LLaMA: Open and Efficient Foundation Language Models is available at https://github.com/meta-llama/llama (meta-llama/llama)."
  },
  {
    "input": "What are the key topics in the ShishirPatil/gorilla repository?",
    "output": "The ShishirPatil/gorilla repository covers: api, llm, api-documentation, chatgpt, gpt-4-api, claude-api, openai-api, openai-functions."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2406.09308&start=0&max_results=10 implementation?",
    "output": "The lamini-ai/Lamini-Memory-Tuning repository has 275 stars and 12 forks on GitHub."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2501.11067&start=0&max_results=10 repository have?",
    "output": "The plurai-ai/intellagent repository has 1,148 stars and 139 forks on GitHub."
  },
  {
    "input": "What's the implementation repository for Dream to Control: Learning Behaviors by Latent Imagination?",
    "output": "The code for Dream to Control: Learning Behaviors by Latent Imagination is available at https://github.com/danijar/dreamer (danijar/dreamer)."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2301.05062&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2301.05062&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2301.11316&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2301.11316&start=0&max_results=10 is available at https://github.com/autonomousvision/stylegan-t (autonomousvision/stylegan-t)."
  },
  {
    "input": "Describe the FMInference/FlexLLMGen repository that implements arXiv Query: search_query=&id_list=2303.08518&start=0&max_results=10.",
    "output": "Running large language models on a single GPU for throughput-oriented scenarios."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2503.00735&start=0&max_results=10 implementation?",
    "output": "The LTH14/fractalgen repository has 1,196 stars and 65 forks on GitHub."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2302.04870&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2302.04870&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at mit-han-lab/offsite-tuning which offsite-tuning: transfer learning without full model It is written in Python. The repository has 382 stars, indicating high community interest."
  },
  {
    "input": "Explain the lucidrains/PaLM-pytorch implementation of PaLM: Scaling Language Modeling with Pathways.",
    "output": "Implementation of the specific Transformer architecture from PaLM - Scaling Language Modeling with Pathways"
  },
  {
    "input": "What is the ArXiv category of ZeRO: Memory Optimizations Toward Training Trillion Parameter Models?",
    "output": "ZeRO: Memory Optimizations Toward Training Trillion Parameter Models is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Which field does FLAN: Finetuned Language Models Are Zero-Shot Learners belong to?",
    "output": "FLAN: Finetuned Language Models Are Zero-Shot Learners is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What is BEiT: BERT Pre-Training of Image Transformers about?",
    "output": "BEiT: BERT Pre-Training of Image Transformers is a 2021 paper in Computer Vision and Pattern Recognition. The implementation is available at microsoft/unilm which large-scale self-supervised pre-training across tasks, languages, and modalities It is written in Python. The repository has 21,848 stars, indicating high community interest."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2404.03592&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2404.03592&start=0&max_results=10 paper was published in 2024."
  },
  {
    "input": "Which field does AudioPaLM: A Large Language Model for Speech Understanding belong to?",
    "output": "AudioPaLM: A Large Language Model for Speech Understanding is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Tell me the title of ArXiv paper 2502.14815.",
    "output": "arXiv Query: search_query=&id_list=2502.14815&start=0&max_results=10"
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2107.07346&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2107.07346&start=0&max_results=10 is a 2021 paper in Machine Learning. The implementation is available at jacopotagliabue/you-dont-need-a-bigger-boat which an end-to-end implementation of intent prediction with metaflow and other cool tools It is written in Python. The repository has 873 stars, indicating high community interest."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2312.02120&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2312.02120&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2406.06326&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2406.06326&start=0&max_results=10 is available at https://github.com/lamini-ai/Lamini-Memory-Tuning (lamini-ai/Lamini-Memory-Tuning)."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2301.09515&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2301.09515&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "In what year did Multi-Task Deep Neural Networks for Natural Language Understanding come out?",
    "output": "The Multi-Task Deep Neural Networks for Natural Language Understanding paper was published in 2021."
  },
  {
    "input": "Explain the liuzhuang13/DenseNet implementation of Densely Connected Convolutional Networks.",
    "output": "Densely Connected Convolutional Networks, In CVPR 2017 (Best Paper Award)."
  },
  {
    "input": "What is the GitHub repository for Dream to Control: Learning Behaviors by Latent Imagination?",
    "output": "The code for Dream to Control: Learning Behaviors by Latent Imagination is available at https://github.com/danijar/dreamer (danijar/dreamer)."
  },
  {
    "input": "Tell me the title of ArXiv paper 1808.07042.",
    "output": "Know What You Don't Know: Unanswerable Questions for SQuAD"
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2301.10343&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2301.10343&start=0&max_results=10 is available at https://github.com/autonomousvision/stylegan-t (autonomousvision/stylegan-t)."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2406.06326&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2406.06326&start=0&max_results=10 is a 2024 paper in Machine Learning. The implementation is available at lamini-ai/Lamini-Memory-Tuning which banishing llm hallucinations requires rethinking generalization The repository has 275 stars, indicating high community interest."
  },
  {
    "input": "Summarize the Longformer: The Long-Document Transformer paper.",
    "output": "Longformer: The Long-Document Transformer is a 2020 paper in Computation and Language (Natural Language Processing). The implementation is available at allenai/longformer which longformer: the long-document transformer It is written in Python. The repository has 2,175 stars, indicating high community interest."
  },
  {
    "input": "What is Flamingo: A Visual Language Model for Few-Shot Learning about?",
    "output": "Flamingo: A Visual Language Model for Few-Shot Learning is a 2022 paper in Computer Vision and Pattern Recognition. The implementation is available at lucidrains/flamingo-pytorch which implementation of \ud83e\udda9 flamingo, state-of-the-art few-shot visual question answering attention net out of deepmind, in pytorch It is written in Python. The repository has 1,270 stars, indicating high community interest."
  },
  {
    "input": "What programming language is XLNet: Generalized Autoregressive Pretraining implemented in?",
    "output": "The XLNet: Generalized Autoregressive Pretraining implementation is written in Python."
  },
  {
    "input": "What topics does the arXiv Query: search_query=&id_list=2302.03011&start=0&max_results=10 implementation cover?",
    "output": "The mit-han-lab/offsite-tuning repository covers: deep-learning, transfer-learning."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2502.19328&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2502.19328&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at LTH14/fractalgen which pytorch implementation of fractalgen https://arxiv.org/abs/2502.17437 It is written in Python. The repository has 1,196 stars, indicating high community interest."
  },
  {
    "input": "Describe the mistralai/mistral-inference repository that implements Mistral 7B.",
    "output": "Official inference library for Mistral models"
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2312.02120&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2312.02120&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2301.00808&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2301.00808&start=0&max_results=10 is available at https://github.com/facebookresearch/ConvNeXt-V2 (facebookresearch/ConvNeXt-V2)."
  },
  {
    "input": "Describe the nyu-mll/jiant repository that implements SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding.",
    "output": "jiant is an nlp toolkit"
  },
  {
    "input": "How popular is the EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification implementation?",
    "output": "The jasonwei20/eda_nlp repository has 1,649 stars and 317 forks on GitHub."
  },
  {
    "input": "When was the Swin Transformer: Hierarchical Vision Transformer using Shifted Windows paper published?",
    "output": "The Swin Transformer: Hierarchical Vision Transformer using Shifted Windows paper was published in 2021."
  },
  {
    "input": "In what year did BLIP: Bootstrapping Language-Image Pre-training come out?",
    "output": "The BLIP: Bootstrapping Language-Image Pre-training paper was published in 2022."
  },
  {
    "input": "Summarize the An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale paper.",
    "output": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale is a 2020 paper in Computer Vision and Pattern Recognition. The implementation is available at google-research/vision_transformer It is written in Jupyter Notebook. The repository has 12,040 stars, indicating high community interest."
  },
  {
    "input": "What year was QLoRA: Efficient Finetuning of Quantized LLMs released?",
    "output": "The QLoRA: Efficient Finetuning of Quantized LLMs paper was published in 2023."
  },
  {
    "input": "What does the lucidrains/audiolm-pytorch repository do?",
    "output": "Implementation of AudioLM, a SOTA Language Modeling Approach to Audio Generation out of Google Research, in Pytorch"
  },
  {
    "input": "Summarize the Text Embeddings by Weakly-Supervised Contrastive Pre-training paper.",
    "output": "Text Embeddings by Weakly-Supervised Contrastive Pre-training is a 2022 paper in Computation and Language (Natural Language Processing). The implementation is available at microsoft/unilm which large-scale self-supervised pre-training across tasks, languages, and modalities It is written in Python. The repository has 21,848 stars, indicating high community interest."
  },
  {
    "input": "When was the Playing Atari with Deep Reinforcement Learning paper published?",
    "output": "The Playing Atari with Deep Reinforcement Learning paper was published in 2013."
  },
  {
    "input": "Explain the bitsandbytes-foundation/bitsandbytes implementation of LLM.int8(): 8-bit Matrix Multiplication for Transformers.",
    "output": "Accessible large language models via k-bit quantization for PyTorch."
  },
  {
    "input": "What does the lamini-ai/Lamini-Memory-Tuning repository do?",
    "output": "Banishing LLM Hallucinations Requires Rethinking Generalization"
  },
  {
    "input": "Which field does Training language models to follow instructions with human feedback belong to?",
    "output": "Training language models to follow instructions with human feedback is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What language is the fastai/fastai repository written in?",
    "output": "The Universal Language Model Fine-tuning for Text Classification implementation is written in Jupyter Notebook."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2302.00902&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2302.00902&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What language is the amazon-science/mm-cot repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2301.13688&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What is BLIP-2: Bootstrapping Language-Image Pre-training about?",
    "output": "BLIP-2: Bootstrapping Language-Image Pre-training is a 2023 paper in Computer Vision and Pattern Recognition. The implementation is available at salesforce/LAVIS which lavis - a one-stop library for language-vision intelligence It is written in Jupyter Notebook. The repository has 11,040 stars, indicating high community interest."
  },
  {
    "input": "Explain the openai/gpt-3 implementation of GPT-3: Language Models are Few-Shot Learners.",
    "output": "GPT-3: Language Models are Few-Shot Learners"
  },
  {
    "input": "What is the GitHub repository for LLaVA-1.5: Improved Baselines with Visual Instruction Tuning?",
    "output": "The code for LLaVA-1.5: Improved Baselines with Visual Instruction Tuning is available at https://github.com/haotian-liu/LLaVA (haotian-liu/LLaVA)."
  },
  {
    "input": "Summarize the Denoising Diffusion Implicit Models paper.",
    "output": "Denoising Diffusion Implicit Models is a 2020 paper in Machine Learning. The implementation is available at ermongroup/ddim which denoising diffusion implicit models It is written in Python. The repository has 1,743 stars, indicating high community interest."
  },
  {
    "input": "What topics does the AlpacaEval: An Automatic Evaluator for Instruction-following implementation cover?",
    "output": "The tatsu-lab/alpaca_eval repository covers: deep-learning, evaluation, foundation-models, instruction-following, large-language-models, leaderboard, nlp, rlhf."
  },
  {
    "input": "What does the Dao-AILab/flash-attention repository do?",
    "output": "Fast and memory-efficient exact attention"
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2301.00808&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2301.00808&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at facebookresearch/ConvNeXt-V2 which code release for convnext v2 model It is written in Python. The repository has 1,890 stars, indicating high community interest."
  },
  {
    "input": "What language is the siddk/voltron-robotics repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2302.12766&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "In what year did FlashAttention-2: Faster Attention with Better Parallelism come out?",
    "output": "The FlashAttention-2: Faster Attention with Better Parallelism paper was published in 2023."
  },
  {
    "input": "Which field does High-Resolution Image Synthesis with Latent Diffusion Models belong to?",
    "output": "High-Resolution Image Synthesis with Latent Diffusion Models is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "How popular is the Grounding DINO: Marrying DINO with Grounded Pre-Training implementation?",
    "output": "The IDEA-Research/GroundingDINO repository has 9,324 stars and 969 forks on GitHub."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2302.00923&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2302.00923&start=0&max_results=10 is available at https://github.com/amazon-science/mm-cot (amazon-science/mm-cot)."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2104.00355&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2104.00355&start=0&max_results=10 is a 2021 paper in Machine Learning. The implementation is available at facebookresearch/fairseq which facebook ai research sequence-to-sequence toolkit written in python. It is written in Python. The repository has 31,988 stars, indicating high community interest."
  },
  {
    "input": "What language is the Open-Reasoner-Zero/Open-Reasoner-Zero repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2502.14815&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2505.22101&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2505.22101&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "How popular is the GPT-3: Language Models are Few-Shot Learners implementation?",
    "output": "The openai/gpt-3 repository has 15,784 stars and 2,294 forks on GitHub."
  },
  {
    "input": "What is the GitHub repository for Whisper: Robust Speech Recognition via Large-Scale Weak Supervision?",
    "output": "The code for Whisper: Robust Speech Recognition via Large-Scale Weak Supervision is available at https://github.com/openai/whisper (openai/whisper)."
  },
  {
    "input": "What is The Power of Scale for Parameter-Efficient Prompt Tuning about?",
    "output": "The Power of Scale for Parameter-Efficient Prompt Tuning is a 2021 paper in Computation and Language (Natural Language Processing). The implementation is available at google-research/prompt-tuning which original implementation of prompt tuning from lester, et al, 2021 It is written in Python. The repository has 698 stars, indicating high community interest."
  },
  {
    "input": "How many stars does the AWQ: Activation-aware Weight Quantization for LLM Compression repository have?",
    "output": "The mit-han-lab/llm-awq repository has 3,357 stars and 283 forks on GitHub."
  },
  {
    "input": "Tell me the title of ArXiv paper 2501.11067.",
    "output": "arXiv Query: search_query=&id_list=2501.11067&start=0&max_results=10"
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2406.09403?",
    "output": "arXiv Query: search_query=&id_list=2406.09403&start=0&max_results=10"
  },
  {
    "input": "What is An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale about?",
    "output": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale is a 2020 paper in Computer Vision and Pattern Recognition. The implementation is available at google-research/vision_transformer It is written in Jupyter Notebook. The repository has 12,040 stars, indicating high community interest."
  },
  {
    "input": "How popular is the Gorilla: Large Language Model Connected with Massive APIs implementation?",
    "output": "The ShishirPatil/gorilla repository has 12,574 stars and 1,283 forks on GitHub."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2505.22101&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2505.22101&start=0&max_results=10 is available at https://github.com/ruixin31/Spurious_Rewards (ruixin31/Spurious_Rewards)."
  },
  {
    "input": "In what year did Finetuned Language Models Are Zero-Shot Learners come out?",
    "output": "The Finetuned Language Models Are Zero-Shot Learners paper was published in 2021."
  },
  {
    "input": "In what year did LLaVA-1.5: Improved Baselines with Visual Instruction Tuning come out?",
    "output": "The LLaVA-1.5: Improved Baselines with Visual Instruction Tuning paper was published in 2023."
  },
  {
    "input": "Describe the linkedin/gdmix repository that implements arXiv Query: search_query=&id_list=2007.16122&start=0&max_results=10.",
    "output": "A deep ranking personalization framework"
  },
  {
    "input": "Which field does LIMA: Less Is More for Alignment belong to?",
    "output": "LIMA: Less Is More for Alignment is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What programming language is DreamFusion: Text-to-3D using 2D Diffusion implemented in?",
    "output": "The DreamFusion: Text-to-3D using 2D Diffusion implementation is written in Python."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2302.12246&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2302.12246&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at shizhediao/active-prompt which source code for the paper \"active prompting with chain-of-thought for large language models\" It is written in Python. The repository has 247 stars, indicating high community interest."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2503.23513&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2503.23513&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is the GitHub repository for PyTorch Geometric: Library for Geometric Deep Learning?",
    "output": "The code for PyTorch Geometric: Library for Geometric Deep Learning is available at https://github.com/pyg-team/pytorch_geometric (pyg-team/pytorch_geometric)."
  },
  {
    "input": "When was the StarCoder: May the source be with you! paper published?",
    "output": "The StarCoder: May the source be with you! paper was published in 2023."
  },
  {
    "input": "What is the ArXiv category of InstructBLIP: Towards General-purpose Vision-Language Models?",
    "output": "InstructBLIP: Towards General-purpose Vision-Language Models is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2109.03264&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2109.03264&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2405.04324&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2405.04324&start=0&max_results=10 paper was published in 2024."
  },
  {
    "input": "What paper has ArXiv ID 2203.02155?",
    "output": "Training language models to follow instructions with human feedback"
  },
  {
    "input": "What are the key topics in the state-spaces/s4 repository?",
    "output": "The state-spaces/s4 repository covers: state-space-models, sequence-models, pytorch."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2303.04226&start=0&max_results=10 repository have?",
    "output": "The HazyResearch/safari repository has 904 stars and 70 forks on GitHub."
  },
  {
    "input": "In what year did Attention Is All You Need come out?",
    "output": "The Attention Is All You Need paper was published in 2017."
  },
  {
    "input": "Describe the lamini-ai/Lamini-Memory-Tuning repository that implements arXiv Query: search_query=&id_list=2406.06326&start=0&max_results=10.",
    "output": "Banishing LLM Hallucinations Requires Rethinking Generalization"
  },
  {
    "input": "Tell me the title of ArXiv paper 1412.6980.",
    "output": "Adam: A Method for Stochastic Optimization"
  },
  {
    "input": "In what year did Med-PaLM 2: Towards Expert-Level Medical Question Answering come out?",
    "output": "The Med-PaLM 2: Towards Expert-Level Medical Question Answering paper was published in 2023."
  },
  {
    "input": "Explain the LLaVA-1.5: Improved Baselines with Visual Instruction Tuning paper.",
    "output": "LLaVA-1.5: Improved Baselines with Visual Instruction Tuning is a 2023 paper in Computer Vision and Pattern Recognition. The implementation is available at haotian-liu/LLaVA which [neurips'23 oral] visual instruction tuning (llava) built towards gpt-4v level capabilities and beyond. It is written in Python. The repository has 24,034 stars, indicating high community interest."
  },
  {
    "input": "How many stars does the ALBERT: A Lite BERT repository have?",
    "output": "The google-research/albert repository has 3,274 stars and 574 forks on GitHub."
  },
  {
    "input": "What category is the Codex: Evaluating Large Language Models Trained on Code paper in?",
    "output": "Codex: Evaluating Large Language Models Trained on Code is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What topics does the Distilling the Knowledge in a Neural Network implementation cover?",
    "output": "The haitongli/knowledge-distillation-pytorch repository covers: pytorch, knowledge-distillation, deep-neural-networks, cifar10, model-compression, dark-knowledge, computer-vision."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2303.02506&start=0&max_results=10 implementation?",
    "output": "The NVlabs/prismer repository has 1,307 stars and 74 forks on GitHub."
  },
  {
    "input": "Explain the ViT: An Image is Worth 16x16 Words paper.",
    "output": "ViT: An Image is Worth 16x16 Words is a 2021 paper in Computer Vision and Pattern Recognition. The implementation is available at google-research/vision_transformer It is written in Jupyter Notebook. The repository has 12,040 stars, indicating high community interest."
  },
  {
    "input": "What does the facebookresearch/TransCoder repository do?",
    "output": "Public release of the TransCoder research project https://arxiv.org/pdf/2006.03511.pdf"
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2104.00355&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2104.00355&start=0&max_results=10 paper was published in 2021."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2504.19413&start=0&max_results=10 repository have?",
    "output": "The MoonshotAI/Kimi-Audio repository has 4,367 stars and 316 forks on GitHub."
  },
  {
    "input": "What programming language is SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding implemented in?",
    "output": "The SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding implementation is written in Python."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 1707.06347?",
    "output": "Proximal Policy Optimization Algorithms"
  },
  {
    "input": "What is the ArXiv category of S4: Efficiently Modeling Long Sequences?",
    "output": "S4: Efficiently Modeling Long Sequences is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What paper has ArXiv ID 2307.01952?",
    "output": "Animate Anyone: Consistent and Controllable Image-to-Video Synthesis"
  },
  {
    "input": "What paper has ArXiv ID 2301.12597?",
    "output": "BLIP-2: Bootstrapping Language-Image Pre-training"
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2301.01181&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2301.01181&start=0&max_results=10 is available at https://github.com/JohnNay/llm-lobbyist (JohnNay/llm-lobbyist)."
  },
  {
    "input": "What is the ArXiv category of BERT: Pre-training of Deep Bidirectional Transformers?",
    "output": "BERT: Pre-training of Deep Bidirectional Transformers is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Give me an overview of Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena.",
    "output": "Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at lm-sys/FastChat which an open platform for training, serving, and evaluating large language models. release repo for vicuna and chatbot arena. It is written in Python. The repository has 39,271 stars, indicating high community interest."
  },
  {
    "input": "What is Universal Language Model Fine-tuning for Text Classification about?",
    "output": "Universal Language Model Fine-tuning for Text Classification is a 2018 paper in Computation and Language (Natural Language Processing). The implementation is available at fastai/fastai which the fastai deep learning library It is written in Jupyter Notebook. The repository has 27,611 stars, indicating high community interest."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2504.02732&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2504.02732&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "What is FlashAttention: Fast and Memory-Efficient Exact Attention about?",
    "output": "FlashAttention: Fast and Memory-Efficient Exact Attention is a 2022 paper in Machine Learning. The implementation is available at Dao-AILab/flash-attention which fast and memory-efficient exact attention It is written in Python. The repository has 20,719 stars, indicating high community interest."
  },
  {
    "input": "What year was DreamBooth: Fine Tuning Text-to-Image Diffusion Models released?",
    "output": "The DreamBooth: Fine Tuning Text-to-Image Diffusion Models paper was published in 2022."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2107.07346&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2107.07346&start=0&max_results=10 paper was published in 2021."
  },
  {
    "input": "What paper has ArXiv ID 2405.02803?",
    "output": "arXiv Query: search_query=&id_list=2405.02803&start=0&max_results=10"
  },
  {
    "input": "What does the 0nutation/SpeechGPT repository do?",
    "output": "SpeechGPT Series: Speech Large Language Models"
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2312.02120&start=0&max_results=10 implementation?",
    "output": "The ContextualAI/HALOs repository has 893 stars and 49 forks on GitHub."
  },
  {
    "input": "Where can I find the code for Multi-Task Deep Neural Networks for Natural Language Understanding?",
    "output": "The code for Multi-Task Deep Neural Networks for Natural Language Understanding is available at https://github.com/namisan/mt-dnn (namisan/mt-dnn)."
  },
  {
    "input": "What programming language is MiniGPT-4: Enhancing Vision-Language Understanding implemented in?",
    "output": "The MiniGPT-4: Enhancing Vision-Language Understanding implementation is written in Python."
  },
  {
    "input": "Describe the bitsandbytes-foundation/bitsandbytes repository that implements LLM.int8(): 8-bit Matrix Multiplication for Transformers.",
    "output": "Accessible large language models via k-bit quantization for PyTorch."
  },
  {
    "input": "When was the XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale paper published?",
    "output": "The XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale paper was published in 2021."
  },
  {
    "input": "What category is the Sequence to Sequence Learning with Neural Networks paper in?",
    "output": "Sequence to Sequence Learning with Neural Networks is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "When was the Deep Residual Learning for Image Recognition paper published?",
    "output": "The Deep Residual Learning for Image Recognition paper was published in 2015."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2308.08155?",
    "output": "MetaGPT: Meta Programming for Multi-Agent Systems"
  },
  {
    "input": "Summarize the Densely Connected Convolutional Networks paper.",
    "output": "Densely Connected Convolutional Networks is a 2016 paper in Computer Vision and Pattern Recognition. The implementation is available at liuzhuang13/DenseNet which densely connected convolutional networks, in cvpr 2017 (best paper award). It is written in Lua. The repository has 4,843 stars, indicating high community interest."
  },
  {
    "input": "Tell me the title of ArXiv paper 2303.02506.",
    "output": "arXiv Query: search_query=&id_list=2303.02506&start=0&max_results=10"
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2102.01192&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2102.01192&start=0&max_results=10 is a 2021 paper in Machine Learning. The implementation is available at facebookresearch/fairseq which facebook ai research sequence-to-sequence toolkit written in python. It is written in Python. The repository has 31,988 stars, indicating high community interest."
  },
  {
    "input": "Which field does Competition-Level Code Generation with AlphaCode belong to?",
    "output": "Competition-Level Code Generation with AlphaCode is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What year was Deep Residual Learning for Image Recognition released?",
    "output": "The Deep Residual Learning for Image Recognition paper was published in 2015."
  },
  {
    "input": "What is Linformer: Self-Attention with Linear Complexity about?",
    "output": "Linformer: Self-Attention with Linear Complexity is a 2021 paper in Machine Learning. The implementation is available at tatp22/linformer-pytorch which my take on a practical implementation of linformer for pytorch. It is written in Python. The repository has 421 stars, indicating high community interest."
  },
  {
    "input": "What paper has ArXiv ID 2304.10592?",
    "output": "LLaVA: Large Language and Vision Assistant"
  },
  {
    "input": "Tell me the title of ArXiv paper 1606.07792.",
    "output": "Wide & Deep Learning for Recommender Systems"
  },
  {
    "input": "How popular is the T5: Exploring the Limits of Transfer Learning implementation?",
    "output": "The google-research/text-to-text-transfer-transformer repository has 6,453 stars and 789 forks on GitHub."
  },
  {
    "input": "What is the ArXiv category of Self-RAG: Learning to Retrieve, Generate, and Critique?",
    "output": "Self-RAG: Learning to Retrieve, Generate, and Critique is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What paper has ArXiv ID 2302.07459?",
    "output": "arXiv Query: search_query=&id_list=2302.07459&start=0&max_results=10"
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2502.12115&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2502.12115&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at Open-Reasoner-Zero/Open-Reasoner-Zero which official repo for open-reasoner-zero It is written in Python. The repository has 2,068 stars, indicating high community interest."
  },
  {
    "input": "What is the GitHub repository for Reflexion: Language Agents with Verbal Reinforcement Learning?",
    "output": "The code for Reflexion: Language Agents with Verbal Reinforcement Learning is available at https://github.com/noahshinn/reflexion (noahshinn/reflexion)."
  },
  {
    "input": "When was the Graph Attention Networks paper published?",
    "output": "The Graph Attention Networks paper was published in 2017."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2303.04226&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2303.04226&start=0&max_results=10 implementation is written in Assembly."
  },
  {
    "input": "Explain the Universal Language Model Fine-tuning for Text Classification paper.",
    "output": "Universal Language Model Fine-tuning for Text Classification is a 2018 paper in Computation and Language (Natural Language Processing). The implementation is available at fastai/fastai which the fastai deep learning library It is written in Jupyter Notebook. The repository has 27,611 stars, indicating high community interest."
  },
  {
    "input": "How many stars does the Analyzing the Structure of Attention in a Transformer Language Model repository have?",
    "output": "The clarkkev/attention-analysis repository has 469 stars and 82 forks on GitHub."
  },
  {
    "input": "Summarize the RoFormer: Enhanced Transformer with Rotary Position Embedding paper.",
    "output": "RoFormer: Enhanced Transformer with Rotary Position Embedding is a 2021 paper in Computation and Language (Natural Language Processing). The implementation is available at ZhuiyiTechnology/roformer which rotary transformer It is written in Python. The repository has 1,051 stars, indicating high community interest."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2503.23513&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2503.23513&start=0&max_results=10 is available at https://github.com/MAXNORM8650/MedAgentSim (MAXNORM8650/MedAgentSim)."
  },
  {
    "input": "Explain the Depthwise Separable Convolutions for Neural Machine Translation paper.",
    "output": "Depthwise Separable Convolutions for Neural Machine Translation is a 2017 paper in Computation and Language (Natural Language Processing). The implementation is available at google-research/google-research which google research It is written in Jupyter Notebook. The repository has 36,800 stars, indicating high community interest."
  },
  {
    "input": "What does the haotian-liu/LLaVA repository do?",
    "output": "[NeurIPS'23 Oral] Visual Instruction Tuning (LLaVA) built towards GPT-4V level capabilities and beyond."
  },
  {
    "input": "What is the ArXiv category of LoRA: Low-Rank Adaptation of Large Language Models?",
    "output": "LoRA: Low-Rank Adaptation of Large Language Models is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What are the key topics in the facebookresearch/CovidPrognosis repository?",
    "output": "The facebookresearch/CovidPrognosis repository covers: medical-imaging, deep-learning, radiography, x-ray, pytorch, covid-19, medical-image-analysis."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2302.03917&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2302.03917&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What does the mit-han-lab/offsite-tuning repository do?",
    "output": "Offsite-Tuning: Transfer Learning without Full Model"
  },
  {
    "input": "What year was Gorilla: Large Language Model Connected with Massive APIs released?",
    "output": "The Gorilla: Large Language Model Connected with Massive APIs paper was published in 2023."
  },
  {
    "input": "Describe the MAXNORM8650/MedAgentSim repository that implements arXiv Query: search_query=&id_list=2503.22678&start=0&max_results=10.",
    "output": "MedAgentSim: Self-Evolving Multi-Agent Simulations for Realistic Clinical Interactions, MICCAI 2025 (oral and early accepted)"
  },
  {
    "input": "Where can I find the code for Time-LLM: Time Series Forecasting by Reprogramming Large Language Models?",
    "output": "The code for Time-LLM: Time Series Forecasting by Reprogramming Large Language Models is available at https://github.com/KimMeen/Time-LLM (KimMeen/Time-LLM)."
  },
  {
    "input": "What is the GitHub repository for PyTorch: An Imperative Style, High-Performance Deep Learning Library?",
    "output": "The code for PyTorch: An Imperative Style, High-Performance Deep Learning Library is available at https://github.com/pytorch/pytorch (pytorch/pytorch)."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2301.00774&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2301.00774&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at facebookresearch/ConvNeXt-V2 which code release for convnext v2 model It is written in Python. The repository has 1,890 stars, indicating high community interest."
  },
  {
    "input": "How popular is the Lookahead Optimizer: k steps forward, 1 step back implementation?",
    "output": "The alphadl/lookahead.pytorch repository has 337 stars and 64 forks on GitHub."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2301.02828&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2301.02828&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at frankxu2004/knnlm-why which repo for icml23 \"why do nearest neighbor language models work?\" It is written in Python. The repository has 59 stars, indicating high community interest."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2502.01061&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2502.01061&start=0&max_results=10 is available at https://github.com/GAIR-NLP/LIMO (GAIR-NLP/LIMO)."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2502.12115&start=0&max_results=10 implementation?",
    "output": "The Open-Reasoner-Zero/Open-Reasoner-Zero repository has 2,068 stars and 119 forks on GitHub."
  },
  {
    "input": "Describe the CompVis/latent-diffusion repository that implements High-Resolution Image Synthesis with Latent Diffusion Models.",
    "output": "High-Resolution Image Synthesis with Latent Diffusion Models"
  },
  {
    "input": "Tell me the title of ArXiv paper 2502.19328.",
    "output": "arXiv Query: search_query=&id_list=2502.19328&start=0&max_results=10"
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2301.00303&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2301.00303&start=0&max_results=10 is available at https://github.com/facebookresearch/ConvNeXt-V2 (facebookresearch/ConvNeXt-V2)."
  },
  {
    "input": "What is Denoising Diffusion Implicit Models about?",
    "output": "Denoising Diffusion Implicit Models is a 2020 paper in Machine Learning. The implementation is available at ermongroup/ddim which denoising diffusion implicit models It is written in Python. The repository has 1,743 stars, indicating high community interest."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2301.00774&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2301.00774&start=0&max_results=10 is available at https://github.com/facebookresearch/ConvNeXt-V2 (facebookresearch/ConvNeXt-V2)."
  },
  {
    "input": "When was the Grounding DINO: Marrying DINO with Grounded Pre-Training paper published?",
    "output": "The Grounding DINO: Marrying DINO with Grounded Pre-Training paper was published in 2023."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2107.07346&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2107.07346&start=0&max_results=10 is available at https://github.com/jacopotagliabue/you-dont-need-a-bigger-boat (jacopotagliabue/you-dont-need-a-bigger-boat)."
  },
  {
    "input": "Explain the microsoft/LoRA implementation of LoRA: Low-Rank Adaptation of Large Language Models.",
    "output": "Code for loralib, an implementation of \"LoRA: Low-Rank Adaptation of Large Language Models\""
  },
  {
    "input": "What category is the Swin Transformer: Hierarchical Vision Transformer using Shifted Windows paper in?",
    "output": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "Describe the microsoft/Swin-Transformer repository that implements Swin Transformer: Hierarchical Vision Transformer using Shifted Windows.",
    "output": "This is an official implementation for \"Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\"."
  },
  {
    "input": "Explain the google-research/google-research implementation of Synthetic Data Generation with Large Language Models.",
    "output": "Google Research"
  },
  {
    "input": "Give me an overview of Adam: A Method for Stochastic Optimization.",
    "output": "Adam: A Method for Stochastic Optimization is a 2014 paper in Machine Learning. The implementation is available at pytorch/pytorch which tensors and dynamic neural networks in python with strong gpu acceleration It is written in Python. The repository has 95,348 stars, indicating high community interest."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2112.10752?",
    "output": "High-Resolution Image Synthesis with Latent Diffusion Models"
  },
  {
    "input": "What language is the namisan/mt-dnn repository written in?",
    "output": "The Multi-Task Deep Neural Networks for Natural Language Understanding implementation is written in Python."
  },
  {
    "input": "How many stars does the Finetuned Language Models Are Zero-Shot Learners repository have?",
    "output": "The google-research/FLAN repository has 1,554 stars and 161 forks on GitHub."
  },
  {
    "input": "Describe the eric-mitchell/direct-preference-optimization repository that implements Direct Preference Optimization: Your Language Model is a Reward Model.",
    "output": "Reference implementation for DPO (Direct Preference Optimization)"
  },
  {
    "input": "What paper has ArXiv ID 2302.08242?",
    "output": "arXiv Query: search_query=&id_list=2302.08242&start=0&max_results=10"
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2109.03264&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2109.03264&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What's the implementation repository for Med-PaLM 2: Towards Expert-Level Medical Question Answering?",
    "output": "The code for Med-PaLM 2: Towards Expert-Level Medical Question Answering is available at https://github.com/google-research/google-research (google-research/google-research)."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2505.22101&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2505.22101&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What paper has ArXiv ID 1907.11692?",
    "output": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"
  },
  {
    "input": "How many stars does the LoRA: Low-Rank Adaptation of Large Language Models repository have?",
    "output": "The microsoft/LoRA repository has 12,979 stars and 863 forks on GitHub."
  },
  {
    "input": "Explain the beir-cellar/beir implementation of Beir: A Heterogeneous Benchmark for IR.",
    "output": "A Heterogeneous Benchmark for Information Retrieval. Easy to use, evaluate your models across 15+ diverse IR datasets."
  },
  {
    "input": "What paper has ArXiv ID 2503.00735?",
    "output": "arXiv Query: search_query=&id_list=2503.00735&start=0&max_results=10"
  },
  {
    "input": "Explain the Reformer: The Efficient Transformer paper.",
    "output": "Reformer: The Efficient Transformer is a 2020 paper in Machine Learning. The implementation is available at google/trax which trax \u2014 deep learning with clear code and speed It is written in Python. The repository has 8,293 stars, indicating high community interest."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2502.03387&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2502.03387&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "What is EfficientDet: Scalable and Efficient Object Detection about?",
    "output": "EfficientDet: Scalable and Efficient Object Detection is a 2020 paper in Computer Vision and Pattern Recognition. The implementation is available at google/automl which google brain automl It is written in Jupyter Notebook. The repository has 6,425 stars, indicating high community interest."
  },
  {
    "input": "What topics does the arXiv Query: search_query=&id_list=2303.06865&start=0&max_results=10 implementation cover?",
    "output": "The FMInference/FlexLLMGen repository covers: deep-learning, gpt-3, high-throughput, large-language-models, machine-learning, offloading, opt."
  },
  {
    "input": "Describe the facebookresearch/TransCoder repository that implements arXiv Query: search_query=&id_list=2006.03511&start=0&max_results=10.",
    "output": "Public release of the TransCoder research project https://arxiv.org/pdf/2006.03511.pdf"
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2312.02120&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2312.02120&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2109.03264&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2109.03264&start=0&max_results=10 is available at https://github.com/facebookresearch/fairseq (facebookresearch/fairseq)."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2301.01181&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2301.01181&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at JohnNay/llm-lobbyist which code for the paper: \"large language models as corporate lobbyists\" (2023). It is written in Jupyter Notebook. The repository has 172 stars, indicating high community interest."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2107.07346&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2107.07346&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Which field does FlashAttention: Fast and Memory-Efficient Exact Attention belong to?",
    "output": "FlashAttention: Fast and Memory-Efficient Exact Attention is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What category is the Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks paper in?",
    "output": "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "How popular is the Wav2Vec 2.0: A Framework for Self-Supervised Learning of Speech implementation?",
    "output": "The facebookresearch/fairseq repository has 31,985 stars and 6,630 forks on GitHub."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2109.03264&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2109.03264&start=0&max_results=10 is a 2021 paper in Machine Learning. The implementation is available at facebookresearch/fairseq which facebook ai research sequence-to-sequence toolkit written in python. It is written in Python. The repository has 31,988 stars, indicating high community interest."
  },
  {
    "input": "What year was Billion-scale similarity search with GPUs released?",
    "output": "The Billion-scale similarity search with GPUs paper was published in 2019."
  },
  {
    "input": "What year was RAG: Retrieval-Augmented Generation released?",
    "output": "The RAG: Retrieval-Augmented Generation paper was published in 2020."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2303.02506&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2303.02506&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What topics does the Depthwise Separable Convolutions for Neural Machine Translation implementation cover?",
    "output": "The google-research/google-research repository covers: machine-learning, ai, research."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2303.04226&start=0&max_results=10 implementation?",
    "output": "The HazyResearch/safari repository has 904 stars and 70 forks on GitHub."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2406.07394&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2406.07394&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2007.16122&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2007.16122&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What does the plurai-ai/intellagent repository do?",
    "output": "A framework for comprehensive diagnosis and optimization of agents using simulated, realistic synthetic interactions"
  },
  {
    "input": "Tell me the title of ArXiv paper 2405.02803.",
    "output": "arXiv Query: search_query=&id_list=2405.02803&start=0&max_results=10"
  },
  {
    "input": "When was the Segment Anything in Medical Images paper published?",
    "output": "The Segment Anything in Medical Images paper was published in 2023."
  },
  {
    "input": "Summarize the SQuAD: 100,000+ Questions for Machine Comprehension of Text paper.",
    "output": "SQuAD: 100,000+ Questions for Machine Comprehension of Text is a 2016 paper in Computation and Language (Natural Language Processing). The implementation is available at rajpurkar/SQuAD-explorer which visually explore the stanford question answering dataset It is written in JavaScript. The repository has 570 stars, indicating high community interest."
  },
  {
    "input": "What are the key topics in the liuzhuang13/DenseNet repository?",
    "output": "The liuzhuang13/DenseNet repository covers: deep-learning."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2406.12824&start=0&max_results=10 implementation?",
    "output": "The hpcaitech/Open-Sora repository has 27,981 stars and 2,781 forks on GitHub."
  },
  {
    "input": "Explain the bigcode-project/starcoder implementation of StarCoder: May the source be with you!.",
    "output": "Home of StarCoder: fine-tuning & inference!"
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2102.01192&start=0&max_results=10 implementation?",
    "output": "The facebookresearch/fairseq repository has 31,988 stars and 6,630 forks on GitHub."
  },
  {
    "input": "Tell me the title of ArXiv paper 1409.0473.",
    "output": "Sequence to Sequence Learning with Neural Networks"
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2403.17887&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2403.17887&start=0&max_results=10 is available at https://github.com/OpenBMB/Eurus (OpenBMB/Eurus)."
  },
  {
    "input": "Give me an overview of Enriching Word Vectors with Subword Information.",
    "output": "Enriching Word Vectors with Subword Information is a 2016 paper in Computation and Language (Natural Language Processing). The implementation is available at facebookresearch/fastText which library for fast text representation and classification. It is written in HTML. The repository has 26,432 stars, indicating high community interest."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2503.00735?",
    "output": "arXiv Query: search_query=&id_list=2503.00735&start=0&max_results=10"
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2502.01061&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2502.01061&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "What does the zihangdai/xlnet repository do?",
    "output": "XLNet: Generalized Autoregressive Pretraining for Language Understanding"
  },
  {
    "input": "What is the GitHub repository for Flamingo: A Visual Language Model for Few-Shot Learning?",
    "output": "The code for Flamingo: A Visual Language Model for Few-Shot Learning is available at https://github.com/lucidrains/flamingo-pytorch (lucidrains/flamingo-pytorch)."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2406.10209&start=0&max_results=10 repository have?",
    "output": "The hpcaitech/Open-Sora repository has 27,981 stars and 2,781 forks on GitHub."
  },
  {
    "input": "What category is the RoBERTa: A Robustly Optimized BERT Pretraining Approach paper in?",
    "output": "RoBERTa: A Robustly Optimized BERT Pretraining Approach is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Give me an overview of Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks.",
    "output": "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks is a 2019 paper in Computation and Language (Natural Language Processing). The implementation is available at huggingface/sentence-transformers which state-of-the-art text embeddings It is written in Python. The repository has 17,904 stars, indicating high community interest."
  },
  {
    "input": "What does the hpcaitech/Open-Sora repository do?",
    "output": "Open-Sora: Democratizing Efficient Video Production for All"
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2406.10209&start=0&max_results=10 implementation?",
    "output": "The hpcaitech/Open-Sora repository has 27,981 stars and 2,781 forks on GitHub."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 1710.10903?",
    "output": "Graph Attention Networks"
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2502.17437&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2502.17437&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "Summarize the Segment Anything paper.",
    "output": "Segment Anything is a 2023 paper in Computer Vision and Pattern Recognition. The implementation is available at facebookresearch/segment-anything which the repository provides code for running inference with the segmentanything model (sam), links for downloading the trained model checkpoints, and example notebooks that show how to use the model. It is written in Jupyter Notebook. The repository has 52,657 stars, indicating high community interest."
  },
  {
    "input": "What programming language is Reflexion: Language Agents with Verbal Reinforcement Learning implemented in?",
    "output": "The Reflexion: Language Agents with Verbal Reinforcement Learning implementation is written in Python."
  },
  {
    "input": "What is the ArXiv category of StarCoder: May the source be with you!?",
    "output": "StarCoder: May the source be with you! is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is the ArXiv category of Wav2Vec 2.0: A Framework for Self-Supervised Learning of Speech?",
    "output": "Wav2Vec 2.0: A Framework for Self-Supervised Learning of Speech is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What language is the sentient-agi/OpenDeepSearch repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2503.20201&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Explain the FlashAttention-2: Faster Attention with Better Parallelism paper.",
    "output": "FlashAttention-2: Faster Attention with Better Parallelism is a 2023 paper in Machine Learning. The implementation is available at Dao-AILab/flash-attention which fast and memory-efficient exact attention It is written in Python. The repository has 20,719 stars, indicating high community interest."
  },
  {
    "input": "What paper has ArXiv ID 2312.02783?",
    "output": "arXiv Query: search_query=&id_list=2312.02783&start=0&max_results=10"
  },
  {
    "input": "How many stars does the BERT: Pre-training of Deep Bidirectional Transformers repository have?",
    "output": "The google-research/bert repository has 39,687 stars and 9,711 forks on GitHub."
  },
  {
    "input": "What is the ArXiv category of GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers?",
    "output": "GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What language is the machrisaa/tensorflow-vgg repository written in?",
    "output": "The Very Deep Convolutional Networks for Large-Scale Image Recognition implementation is written in Python."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2004.08492&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2004.08492&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What paper has ArXiv ID 2211.10438?",
    "output": "SmoothQuant: Accurate and Efficient Post-Training Quantization"
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2410.12896&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2410.12896&start=0&max_results=10 is a 2024 paper in Machine Learning. The implementation is available at ibm-granite/granite-3.0-language-models The repository has 267 stars, indicating high community interest."
  },
  {
    "input": "When was the Wav2Vec 2.0: A Framework for Self-Supervised Learning of Speech paper published?",
    "output": "The Wav2Vec 2.0: A Framework for Self-Supervised Learning of Speech paper was published in 2021."
  },
  {
    "input": "In what year did Adam: A Method for Stochastic Optimization come out?",
    "output": "The Adam: A Method for Stochastic Optimization paper was published in 2014."
  },
  {
    "input": "What is Horovod: fast and easy distributed deep learning in TensorFlow about?",
    "output": "Horovod: fast and easy distributed deep learning in TensorFlow is a 2017 paper in Machine Learning. The implementation is available at horovod/horovod which distributed training framework for tensorflow, keras, pytorch, and apache mxnet. It is written in Python. The repository has 14,631 stars, indicating high community interest."
  },
  {
    "input": "Summarize the Reformer: The Efficient Transformer paper.",
    "output": "Reformer: The Efficient Transformer is a 2020 paper in Machine Learning. The implementation is available at google/trax which trax \u2014 deep learning with clear code and speed It is written in Python. The repository has 8,293 stars, indicating high community interest."
  },
  {
    "input": "Describe the pytorch/pytorch repository that implements PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel.",
    "output": "Tensors and Dynamic neural networks in Python with strong GPU acceleration"
  },
  {
    "input": "Tell me the title of ArXiv paper 2212.04356.",
    "output": "Whisper: Robust Speech Recognition via Large-Scale Weak Supervision"
  },
  {
    "input": "Which field does Wide & Deep Learning for Recommender Systems belong to?",
    "output": "Wide & Deep Learning for Recommender Systems is in the Information Retrieval (cs.IR) category."
  },
  {
    "input": "Explain the ibm-granite/granite-code-models implementation of arXiv Query: search_query=&id_list=2405.03548&start=0&max_results=10.",
    "output": "Granite Code Models: A Family of Open Foundation Models for Code Intelligence"
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2502.09992&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2502.09992&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2302.08242&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2302.08242&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at haoliuhl/language-quantized-autoencoders which language quantized autoencoders It is written in Python. The repository has 111 stars, indicating high community interest."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2406.09308&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2406.09308&start=0&max_results=10 is available at https://github.com/lamini-ai/Lamini-Memory-Tuning (lamini-ai/Lamini-Memory-Tuning)."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2504.19413&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2504.19413&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at MoonshotAI/Kimi-Audio which kimi-audio, an open-source audio foundation model excelling in audio understanding, generation, and conversation It is written in Python. The repository has 4,367 stars, indicating high community interest."
  },
  {
    "input": "What's the implementation repository for DreamBooth: Fine Tuning Text-to-Image Diffusion Models?",
    "output": "The code for DreamBooth: Fine Tuning Text-to-Image Diffusion Models is available at https://github.com/google/dreambooth (google/dreambooth)."
  },
  {
    "input": "What are the key topics in the facebookresearch/fairseq repository?",
    "output": "The facebookresearch/fairseq repository covers: python, pytorch, artificial-intelligence."
  },
  {
    "input": "Tell me the title of ArXiv paper 1607.01759.",
    "output": "Enriching Word Vectors with Subword Information"
  },
  {
    "input": "Where can I find the code for LLaMA: Open and Efficient Foundation Language Models?",
    "output": "The code for LLaMA: Open and Efficient Foundation Language Models is available at https://github.com/meta-llama/llama (meta-llama/llama)."
  },
  {
    "input": "What is the ArXiv category of DreamFusion: Text-to-3D using 2D Diffusion?",
    "output": "DreamFusion: Text-to-3D using 2D Diffusion is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "What paper has ArXiv ID 2103.14030?",
    "output": "BEiT: BERT Pre-Training of Image Transformers"
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2504.20734&start=0&max_results=10 repository have?",
    "output": "The MoonshotAI/Kimi-Audio repository has 4,367 stars and 316 forks on GitHub."
  },
  {
    "input": "What is the ArXiv category of Adam: A Method for Stochastic Optimization?",
    "output": "Adam: A Method for Stochastic Optimization is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2302.00902&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2302.00902&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2501.19393&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2501.19393&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "How popular is the Text Embeddings by Weakly-Supervised Contrastive Pre-training implementation?",
    "output": "The microsoft/unilm repository has 21,848 stars and 2,674 forks on GitHub."
  },
  {
    "input": "Where can I find the code for QLoRA: Efficient Finetuning of Quantized LLMs?",
    "output": "The code for QLoRA: Efficient Finetuning of Quantized LLMs is available at https://github.com/artidoro/qlora (artidoro/qlora)."
  },
  {
    "input": "How popular is the LIMA: Less Is More for Alignment implementation?",
    "output": "The meta-llama/llama repository has 58,946 stars and 9,817 forks on GitHub."
  },
  {
    "input": "What are the key topics in the facebookresearch/fairseq repository?",
    "output": "The facebookresearch/fairseq repository covers: python, pytorch, artificial-intelligence."
  },
  {
    "input": "What is the GitHub repository for Feature Pyramid Networks for Object Detection?",
    "output": "The code for Feature Pyramid Networks for Object Detection is available at https://github.com/facebookresearch/Detectron (facebookresearch/Detectron)."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2501.13824&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2501.13824&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "Tell me the title of ArXiv paper 2303.08774.",
    "output": "GPT-4 Technical Report"
  },
  {
    "input": "What is the GitHub repository for Codex: Evaluating Large Language Models Trained on Code?",
    "output": "The code for Codex: Evaluating Large Language Models Trained on Code is available at https://github.com/openai/human-eval (openai/human-eval)."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2505.22101&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2505.22101&start=0&max_results=10 is available at https://github.com/ruixin31/Spurious_Rewards (ruixin31/Spurious_Rewards)."
  },
  {
    "input": "Tell me the title of ArXiv paper 2204.14198.",
    "output": "Flamingo: A Visual Language Model for Few-Shot Learning"
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2303.04671&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2303.04671&start=0&max_results=10 is available at https://github.com/chenfei-wu/TaskMatrix (chenfei-wu/TaskMatrix)."
  },
  {
    "input": "What does the hendrycks/test repository do?",
    "output": "Measuring Massive Multitask Language Understanding | ICLR 2021"
  },
  {
    "input": "What does the facebookresearch/DPR repository do?",
    "output": "Dense Passage Retriever - is a set of tools and models for open domain Q&A task."
  },
  {
    "input": "Summarize the BERT: Pre-training of Deep Bidirectional Transformers paper.",
    "output": "BERT: Pre-training of Deep Bidirectional Transformers is a 2018 paper in Computation and Language (Natural Language Processing). The implementation is available at google-research/bert which tensorflow code and pre-trained models for bert It is written in Python. The repository has 39,687 stars, indicating high community interest."
  },
  {
    "input": "What programming language is Billion-scale similarity search with GPUs implemented in?",
    "output": "The Billion-scale similarity search with GPUs implementation is written in C++."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2301.00808&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2301.00808&start=0&max_results=10 is available at https://github.com/facebookresearch/ConvNeXt-V2 (facebookresearch/ConvNeXt-V2)."
  },
  {
    "input": "Describe the huggingface/sentence-transformers repository that implements Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks.",
    "output": "State-of-the-Art Text Embeddings"
  },
  {
    "input": "How many stars does the P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning repository have?",
    "output": "The THUDM/P-tuning-v2 repository has 2,067 stars and 206 forks on GitHub."
  },
  {
    "input": "Summarize the The Power of Scale for Parameter-Efficient Prompt Tuning paper.",
    "output": "The Power of Scale for Parameter-Efficient Prompt Tuning is a 2021 paper in Computation and Language (Natural Language Processing). The implementation is available at google-research/prompt-tuning which original implementation of prompt tuning from lester, et al, 2021 It is written in Python. The repository has 698 stars, indicating high community interest."
  },
  {
    "input": "What's the implementation repository for HuggingGPT: Solving AI Tasks with ChatGPT?",
    "output": "The code for HuggingGPT: Solving AI Tasks with ChatGPT is available at https://github.com/microsoft/JARVIS (microsoft/JARVIS)."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2504.21801&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2504.21801&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at MoonshotAI/Kimi-Audio which kimi-audio, an open-source audio foundation model excelling in audio understanding, generation, and conversation It is written in Python. The repository has 4,367 stars, indicating high community interest."
  },
  {
    "input": "Which field does LLM.int8(): 8-bit Matrix Multiplication for Transformers belong to?",
    "output": "LLM.int8(): 8-bit Matrix Multiplication for Transformers is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Explain the horovod/horovod implementation of Horovod: fast and easy distributed deep learning in TensorFlow.",
    "output": "Distributed training framework for TensorFlow, Keras, PyTorch, and Apache MXNet."
  },
  {
    "input": "What year was Text Embeddings by Weakly-Supervised Contrastive Pre-training released?",
    "output": "The Text Embeddings by Weakly-Supervised Contrastive Pre-training paper was published in 2022."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2302.14233&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2302.14233&start=0&max_results=10 is available at https://github.com/ruiqi-zhong/D5 (ruiqi-zhong/D5)."
  },
  {
    "input": "What programming language is Feature Pyramid Networks for Object Detection implemented in?",
    "output": "The Feature Pyramid Networks for Object Detection implementation is written in Python."
  },
  {
    "input": "Explain the Mistral 7B paper.",
    "output": "Mistral 7B is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at mistralai/mistral-inference which official inference library for mistral models It is written in Jupyter Notebook. The repository has 10,550 stars, indicating high community interest."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2301.09515?",
    "output": "arXiv Query: search_query=&id_list=2301.09515&start=0&max_results=10"
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2312.02783&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2312.02783&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "When was the GPT-3: Language Models are Few-Shot Learners paper published?",
    "output": "The GPT-3: Language Models are Few-Shot Learners paper was published in 2020."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2303.02913&start=0&max_results=10 implementation?",
    "output": "The Shark-NLP/OpenICL repository has 579 stars and 30 forks on GitHub."
  },
  {
    "input": "In what year did The Power of Scale for Parameter-Efficient Prompt Tuning come out?",
    "output": "The The Power of Scale for Parameter-Efficient Prompt Tuning paper was published in 2021."
  },
  {
    "input": "What is the GitHub repository for StarCoder: May the source be with you!?",
    "output": "The code for StarCoder: May the source be with you! is available at https://github.com/bigcode-project/starcoder (bigcode-project/starcoder)."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2302.07459&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2302.07459&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at haoliuhl/language-quantized-autoencoders which language quantized autoencoders It is written in Python. The repository has 111 stars, indicating high community interest."
  },
  {
    "input": "What does the Megvii-BaseDetection/YOLOX repository do?",
    "output": "YOLOX is a high-performance anchor-free YOLO, exceeding yolov3~v5 with MegEngine, ONNX, TensorRT, ncnn, and OpenVINO supported. Documentation: https://yolox.readthedocs.io/"
  },
  {
    "input": "What language is the Crista23/goal_directedness_llms repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2504.11844&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What does the IST-DASLab/gptq repository do?",
    "output": "Code for the ICLR 2023 paper \"GPTQ: Accurate Post-training Quantization of Generative Pretrained Transformers\"."
  },
  {
    "input": "Describe the frankxu2004/knnlm-why repository that implements arXiv Query: search_query=&id_list=2301.02828&start=0&max_results=10.",
    "output": "Repo for ICML23 \"Why do Nearest Neighbor Language Models Work?\""
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2503.23513&start=0&max_results=10 implementation?",
    "output": "The MAXNORM8650/MedAgentSim repository has 94 stars and 11 forks on GitHub."
  },
  {
    "input": "Which field does LLaVA: Large Language and Vision Assistant belong to?",
    "output": "LLaVA: Large Language and Vision Assistant is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "What's the implementation repository for Swin Transformer: Hierarchical Vision Transformer using Shifted Windows?",
    "output": "The code for Swin Transformer: Hierarchical Vision Transformer using Shifted Windows is available at https://github.com/microsoft/Swin-Transformer (microsoft/Swin-Transformer)."
  },
  {
    "input": "What language is the uber/orbit repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2004.08492&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What paper has ArXiv ID 2501.11067?",
    "output": "arXiv Query: search_query=&id_list=2501.11067&start=0&max_results=10"
  },
  {
    "input": "What year was Flamingo: A Visual Language Model for Few-Shot Learning released?",
    "output": "The Flamingo: A Visual Language Model for Few-Shot Learning paper was published in 2022."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2006.16779&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2006.16779&start=0&max_results=10 is available at https://github.com/google-research/pegasus (google-research/pegasus)."
  },
  {
    "input": "What is FLAN: Finetuned Language Models Are Zero-Shot Learners about?",
    "output": "FLAN: Finetuned Language Models Are Zero-Shot Learners is a 2021 paper in Computation and Language (Natural Language Processing). The implementation is available at google-research/FLAN It is written in Python. The repository has 1,554 stars, indicating high community interest."
  },
  {
    "input": "What programming language is Mistral 7B implemented in?",
    "output": "The Mistral 7B implementation is written in Jupyter Notebook."
  },
  {
    "input": "What programming language is Code Llama: Open Foundation Models for Code implemented in?",
    "output": "The Code Llama: Open Foundation Models for Code implementation is written in Python."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2406.09403&start=0&max_results=10 repository have?",
    "output": "The lamini-ai/Lamini-Memory-Tuning repository has 275 stars and 12 forks on GitHub."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2012.06009?",
    "output": "arXiv Query: search_query=&id_list=2012.06009&start=0&max_results=10"
  },
  {
    "input": "How popular is the Whisper: Robust Speech Recognition via Large-Scale Weak Supervision implementation?",
    "output": "The openai/whisper repository has 91,236 stars and 11,444 forks on GitHub."
  },
  {
    "input": "How popular is the Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context implementation?",
    "output": "The kimiyoung/transformer-xl repository has 3,679 stars and 766 forks on GitHub."
  },
  {
    "input": "How many stars does the Dense Passage Retrieval for Open-Domain Question Answering repository have?",
    "output": "The facebookresearch/DPR repository has 1,847 stars and 315 forks on GitHub."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2502.12115?",
    "output": "arXiv Query: search_query=&id_list=2502.12115&start=0&max_results=10"
  },
  {
    "input": "What category is the LLaMA: Open and Efficient Foundation Language Models paper in?",
    "output": "LLaMA: Open and Efficient Foundation Language Models is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Give me an overview of BEiT: BERT Pre-Training of Image Transformers.",
    "output": "BEiT: BERT Pre-Training of Image Transformers is a 2021 paper in Computer Vision and Pattern Recognition. The implementation is available at microsoft/unilm which large-scale self-supervised pre-training across tasks, languages, and modalities It is written in Python. The repository has 21,848 stars, indicating high community interest."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2406.07394&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2406.07394&start=0&max_results=10 is available at https://github.com/hpcaitech/Open-Sora (hpcaitech/Open-Sora)."
  },
  {
    "input": "Tell me the title of ArXiv paper 2306.00978.",
    "output": "AWQ: Activation-aware Weight Quantization for LLM Compression"
  },
  {
    "input": "Explain the ermongroup/ddim implementation of Denoising Diffusion Implicit Models.",
    "output": "Denoising Diffusion Implicit Models"
  },
  {
    "input": "What programming language is RAG: Retrieval-Augmented Generation implemented in?",
    "output": "The RAG: Retrieval-Augmented Generation implementation is written in Python."
  },
  {
    "input": "What is the GitHub repository for Neural Message Passing for Quantum Chemistry?",
    "output": "The code for Neural Message Passing for Quantum Chemistry is available at https://github.com/priba/nmp_qc (priba/nmp_qc)."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2301.11305&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2301.11305&start=0&max_results=10 is available at https://github.com/autonomousvision/stylegan-t (autonomousvision/stylegan-t)."
  },
  {
    "input": "In what year did JAX: Composable transformations of Python+NumPy programs come out?",
    "output": "The JAX: Composable transformations of Python+NumPy programs paper was published in 2019."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2301.10343&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2301.10343&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Tell me the title of ArXiv paper 1706.03762.",
    "output": "Attention Is All You Need"
  },
  {
    "input": "Explain the Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena paper.",
    "output": "Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at lm-sys/FastChat which an open platform for training, serving, and evaluating large language models. release repo for vicuna and chatbot arena. It is written in Python. The repository has 39,271 stars, indicating high community interest."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2503.00735&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2503.00735&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2503.23513&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2503.23513&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2302.09778&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2302.09778&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is the GitHub repository for XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale?",
    "output": "The code for XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale is available at https://github.com/facebookresearch/fairseq (facebookresearch/fairseq)."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2505.22954&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2505.22954&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at ruixin31/Spurious_Rewards It is written in Python. The repository has 342 stars, indicating high community interest."
  },
  {
    "input": "Explain the linkedin/gdmix implementation of arXiv Query: search_query=&id_list=2007.16122&start=0&max_results=10.",
    "output": "A deep ranking personalization framework"
  },
  {
    "input": "What does the ZhuiyiTechnology/roformer repository do?",
    "output": "Rotary Transformer"
  },
  {
    "input": "What are the key topics in the haitongli/knowledge-distillation-pytorch repository?",
    "output": "The haitongli/knowledge-distillation-pytorch repository covers: pytorch, knowledge-distillation, deep-neural-networks, cifar10, model-compression, dark-knowledge, computer-vision."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2405.02803&start=0&max_results=10 implementation?",
    "output": "The ibm-granite/granite-code-models repository has 1,240 stars and 88 forks on GitHub."
  },
  {
    "input": "What is Deep Graph Library: A Graph-Centric, Highly-Performant Package about?",
    "output": "Deep Graph Library: A Graph-Centric, Highly-Performant Package is a 2020 paper in Machine Learning. The implementation is available at dmlc/dgl which python package built to ease deep learning on graph, on top of existing dl frameworks. It is written in Python. The repository has 14,149 stars, indicating high community interest."
  },
  {
    "input": "Explain the haoliuhl/language-quantized-autoencoders implementation of arXiv Query: search_query=&id_list=2302.00902&start=0&max_results=10.",
    "output": "Language Quantized AutoEncoders"
  },
  {
    "input": "How popular is the Prefix-Tuning: Optimizing Continuous Prompts for Generation implementation?",
    "output": "The XiangLi1999/PrefixTuning repository has 954 stars and 164 forks on GitHub."
  },
  {
    "input": "Give me an overview of Very Deep Convolutional Networks for Large-Scale Image Recognition.",
    "output": "Very Deep Convolutional Networks for Large-Scale Image Recognition is a 2014 paper in Computer Vision and Pattern Recognition. The implementation is available at machrisaa/tensorflow-vgg which vgg19 and vgg16 on tensorflow It is written in Python. The repository has 2,239 stars, indicating high community interest."
  },
  {
    "input": "When was the Generating Wikipedia by Summarizing Long Sequences paper published?",
    "output": "The Generating Wikipedia by Summarizing Long Sequences paper was published in 2018."
  },
  {
    "input": "Tell me the title of ArXiv paper 2009.01325.",
    "output": "Learning to summarize from human feedback"
  },
  {
    "input": "Summarize the DreamBooth: Fine Tuning Text-to-Image Diffusion Models paper.",
    "output": "DreamBooth: Fine Tuning Text-to-Image Diffusion Models is a 2022 paper in Computer Vision and Pattern Recognition. The implementation is available at google/dreambooth The repository has 998 stars, indicating high community interest."
  },
  {
    "input": "Where can I find the code for Linformer: Self-Attention with Linear Complexity?",
    "output": "The code for Linformer: Self-Attention with Linear Complexity is available at https://github.com/tatp22/linformer-pytorch (tatp22/linformer-pytorch)."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2101.04909&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2101.04909&start=0&max_results=10 is a 2021 paper in Machine Learning. The implementation is available at facebookresearch/CovidPrognosis which covid deterioration prediction based on chest x-ray radiographs via moco-trained image representations It is written in Python. The repository has 158 stars, indicating high community interest."
  },
  {
    "input": "How many stars does the PyTorch Geometric: Library for Geometric Deep Learning repository have?",
    "output": "The pyg-team/pytorch_geometric repository has 23,186 stars and 3,921 forks on GitHub."
  },
  {
    "input": "Tell me the title of ArXiv paper 2301.12652.",
    "output": "arXiv Query: search_query=&id_list=2301.12652&start=0&max_results=10"
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2303.08518?",
    "output": "arXiv Query: search_query=&id_list=2303.08518&start=0&max_results=10"
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2306.04190?",
    "output": "ChemCrow: Augmenting large-language models with chemistry tools"
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2006.16779&start=0&max_results=10 repository have?",
    "output": "The google-research/pegasus repository has 1,644 stars and 318 forks on GitHub."
  },
  {
    "input": "What paper has ArXiv ID 1606.05250?",
    "output": "SQuAD: 100,000+ Questions for Machine Comprehension of Text"
  },
  {
    "input": "What programming language is Synthetic Data Generation with Large Language Models implemented in?",
    "output": "The Synthetic Data Generation with Large Language Models implementation is written in Jupyter Notebook."
  },
  {
    "input": "What's the implementation repository for ReAct: Synergizing Reasoning and Acting in Language Models?",
    "output": "The code for ReAct: Synergizing Reasoning and Acting in Language Models is available at https://github.com/ysymyth/ReAct (ysymyth/ReAct)."
  },
  {
    "input": "What programming language is Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena implemented in?",
    "output": "The Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena implementation is written in Python."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2501.19393&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2501.19393&start=0&max_results=10 is available at https://github.com/simplescaling/s1 (simplescaling/s1)."
  },
  {
    "input": "What is Attention Is All You Need about?",
    "output": "Attention Is All You Need is a 2017 paper in Computation and Language (Natural Language Processing). The implementation is available at tensorflow/tensor2tensor which library of deep learning models and datasets designed to make deep learning more accessible and accelerate ml research. It is written in Python. The repository has 16,760 stars, indicating high community interest."
  },
  {
    "input": "In what year did Densely Connected Convolutional Networks come out?",
    "output": "The Densely Connected Convolutional Networks paper was published in 2016."
  },
  {
    "input": "What is the ArXiv category of Longformer: The Long-Document Transformer?",
    "output": "Longformer: The Long-Document Transformer is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What are the key topics in the dmlc/dgl repository?",
    "output": "The dmlc/dgl repository covers: deep-learning, graph-neural-networks."
  },
  {
    "input": "What year was Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena released?",
    "output": "The Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena paper was published in 2023."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2302.04023&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2302.04023&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Give me an overview of DreamFusion: Text-to-3D using 2D Diffusion.",
    "output": "DreamFusion: Text-to-3D using 2D Diffusion is a 2022 paper in Computer Vision and Pattern Recognition. The implementation is available at ashawkey/stable-dreamfusion which text-to-3d & image-to-3d & mesh exportation with nerf + diffusion. It is written in Python. The repository has 8,773 stars, indicating high community interest."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2504.21801?",
    "output": "arXiv Query: search_query=&id_list=2504.21801&start=0&max_results=10"
  },
  {
    "input": "Tell me the title of ArXiv paper 2309.03409.",
    "output": "AudioPaLM: A Large Language Model for Speech Understanding"
  },
  {
    "input": "What year was EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification released?",
    "output": "The EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification paper was published in 2019."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2302.04023&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2302.04023&start=0&max_results=10 is available at https://github.com/mit-han-lab/offsite-tuning (mit-han-lab/offsite-tuning)."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2502.17437&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2502.17437&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Explain the ruiqi-zhong/D5 implementation of arXiv Query: search_query=&id_list=2302.14233&start=0&max_results=10.",
    "output": "The GitHub repo for Goal Driven Discovery of Distributional Differences via Language Descriptions"
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2203.07814?",
    "output": "Competition-Level Code Generation with AlphaCode"
  },
  {
    "input": "Explain the facebookresearch/ConvNeXt-V2 implementation of arXiv Query: search_query=&id_list=2301.00774&start=0&max_results=10.",
    "output": "Code release for ConvNeXt V2 model"
  },
  {
    "input": "Explain the priba/nmp_qc implementation of Neural Message Passing for Quantum Chemistry.",
    "output": "Our own implementation of Neural Message Passing for Computer Vision paper"
  },
  {
    "input": "What topics does the P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning implementation cover?",
    "output": "The THUDM/P-tuning-v2 repository covers: natural-language-processing, prompt-tuning, pretrained-language-model, p-tuning, parameter-efficient-learning."
  },
  {
    "input": "What year was SimCSE: Simple Contrastive Learning of Sentence Embeddings released?",
    "output": "The SimCSE: Simple Contrastive Learning of Sentence Embeddings paper was published in 2021."
  },
  {
    "input": "What category is the Whisper: Robust Speech Recognition via Large-Scale Weak Supervision paper in?",
    "output": "Whisper: Robust Speech Recognition via Large-Scale Weak Supervision is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What paper has ArXiv ID 2101.04909?",
    "output": "arXiv Query: search_query=&id_list=2101.04909&start=0&max_results=10"
  },
  {
    "input": "Explain the Linformer: Self-Attention with Linear Complexity paper.",
    "output": "Linformer: Self-Attention with Linear Complexity is a 2021 paper in Machine Learning. The implementation is available at tatp22/linformer-pytorch which my take on a practical implementation of linformer for pytorch. It is written in Python. The repository has 421 stars, indicating high community interest."
  },
  {
    "input": "Explain the Med-PaLM 2: Towards Expert-Level Medical Question Answering paper.",
    "output": "Med-PaLM 2: Towards Expert-Level Medical Question Answering is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at google-research/google-research which google research It is written in Jupyter Notebook. The repository has 36,800 stars, indicating high community interest."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2302.14233&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2302.14233&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is the ArXiv category of The Power of Scale for Parameter-Efficient Prompt Tuning?",
    "output": "The Power of Scale for Parameter-Efficient Prompt Tuning is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What category is the Prefix-Tuning: Optimizing Continuous Prompts for Generation paper in?",
    "output": "Prefix-Tuning: Optimizing Continuous Prompts for Generation is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Which field does DreamFusion: Text-to-3D using 2D Diffusion belong to?",
    "output": "DreamFusion: Text-to-3D using 2D Diffusion is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "What year was HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction released?",
    "output": "The HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction paper was published in 2021."
  },
  {
    "input": "Give me an overview of Codex: Evaluating Large Language Models Trained on Code.",
    "output": "Codex: Evaluating Large Language Models Trained on Code is a 2021 paper in Machine Learning. The implementation is available at openai/human-eval which code for the paper \"evaluating large language models trained on code\" It is written in Python. The repository has 3,030 stars, indicating high community interest."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2502.17437&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2502.17437&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at LTH14/fractalgen which pytorch implementation of fractalgen https://arxiv.org/abs/2502.17437 It is written in Python. The repository has 1,196 stars, indicating high community interest."
  },
  {
    "input": "Which field does GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers belong to?",
    "output": "GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Summarize the Multi-Task Deep Neural Networks for Natural Language Understanding paper.",
    "output": "Multi-Task Deep Neural Networks for Natural Language Understanding is a 2021 paper in Computation and Language (Natural Language Processing). The implementation is available at namisan/mt-dnn which multi-task deep neural networks for natural language understanding It is written in Python. The repository has 2,258 stars, indicating high community interest."
  },
  {
    "input": "Tell me the title of ArXiv paper 2503.22708.",
    "output": "arXiv Query: search_query=&id_list=2503.22708&start=0&max_results=10"
  },
  {
    "input": "What does the facebookresearch/segment-anything repository do?",
    "output": "The repository provides code for running inference with the SegmentAnything Model (SAM), links for downloading the trained model checkpoints, and example notebooks that show how to use the model."
  },
  {
    "input": "What is Grounding DINO: Marrying DINO with Grounded Pre-Training about?",
    "output": "Grounding DINO: Marrying DINO with Grounded Pre-Training is a 2023 paper in Computer Vision and Pattern Recognition. The implementation is available at IDEA-Research/GroundingDINO which [eccv 2024] official implementation of the paper \"grounding dino: marrying dino with grounded pre-training for open-set object detection\" It is written in Python. The repository has 9,324 stars, indicating high community interest."
  },
  {
    "input": "In what year did Animate Anyone: Consistent and Controllable Image-to-Video Synthesis come out?",
    "output": "The Animate Anyone: Consistent and Controllable Image-to-Video Synthesis paper was published in 2023."
  },
  {
    "input": "What category is the Feature Pyramid Networks for Object Detection paper in?",
    "output": "Feature Pyramid Networks for Object Detection is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "What is the GitHub repository for DistilBERT: a distilled version of BERT?",
    "output": "The code for DistilBERT: a distilled version of BERT is available at https://github.com/huggingface/transformers (huggingface/transformers)."
  },
  {
    "input": "What topics does the XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale implementation cover?",
    "output": "The facebookresearch/fairseq repository covers: python, pytorch, artificial-intelligence."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2504.01848&start=0&max_results=10 implementation?",
    "output": "The openai/frontier-evals repository has 949 stars and 108 forks on GitHub."
  },
  {
    "input": "How many stars does the DreamFusion: Text-to-3D using 2D Diffusion repository have?",
    "output": "The ashawkey/stable-dreamfusion repository has 8,773 stars and 768 forks on GitHub."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2503.00735&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2503.00735&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2302.08242&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2302.08242&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "Tell me the title of ArXiv paper 2504.11844.",
    "output": "arXiv Query: search_query=&id_list=2504.11844&start=0&max_results=10"
  },
  {
    "input": "What year was DeBERTa: Decoding-enhanced BERT with Disentangled Attention released?",
    "output": "The DeBERTa: Decoding-enhanced BERT with Disentangled Attention paper was published in 2020."
  },
  {
    "input": "What does the tmikolov/word2vec repository do?",
    "output": "Automatically exported from code.google.com/p/word2vec"
  },
  {
    "input": "What paper has ArXiv ID 2201.12086?",
    "output": "BLIP: Bootstrapping Language-Image Pre-training"
  },
  {
    "input": "In what year did SimCSE: Simple Contrastive Learning of Sentence Embeddings come out?",
    "output": "The SimCSE: Simple Contrastive Learning of Sentence Embeddings paper was published in 2021."
  },
  {
    "input": "What paper has ArXiv ID 2112.04426?",
    "output": "ColBERT: Efficient Passage Search via Contextualized Late Interaction"
  },
  {
    "input": "Where can I find the code for Code Llama: Open Foundation Models for Code?",
    "output": "The code for Code Llama: Open Foundation Models for Code is available at https://github.com/meta-llama/codellama (meta-llama/codellama)."
  },
  {
    "input": "Summarize the MRKL Systems: A modular approach to AGI paper.",
    "output": "MRKL Systems: A modular approach to AGI is a 2022 paper in Artificial Intelligence. The implementation is available at langchain-ai/langchain which \ud83e\udd9c\ud83d\udd17 the platform for reliable agents. It is written in Python. The repository has 120,451 stars, indicating high community interest."
  },
  {
    "input": "In what year did YOLOX: Exceeding YOLO Series in 2021 come out?",
    "output": "The YOLOX: Exceeding YOLO Series in 2021 paper was published in 2018."
  },
  {
    "input": "Where can I find the code for BLIP: Bootstrapping Language-Image Pre-training?",
    "output": "The code for BLIP: Bootstrapping Language-Image Pre-training is available at https://github.com/salesforce/BLIP (salesforce/BLIP)."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2301.04856&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2301.04856&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2302.14838&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2302.14838&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at ruiqi-zhong/D5 which the github repo for goal driven discovery of distributional differences via language descriptions It is written in Python. The repository has 71 stars, indicating high community interest."
  },
  {
    "input": "What is the GitHub repository for MMLU: Measuring Massive Multitask Language Understanding?",
    "output": "The code for MMLU: Measuring Massive Multitask Language Understanding is available at https://github.com/hendrycks/test (hendrycks/test)."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2501.13824&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2501.13824&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "What are the key topics in the FMInference/FlexLLMGen repository?",
    "output": "The FMInference/FlexLLMGen repository covers: deep-learning, gpt-3, high-throughput, large-language-models, machine-learning, offloading, opt."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2303.04671&start=0&max_results=10 implementation?",
    "output": "The chenfei-wu/TaskMatrix repository has 34,352 stars and 3,268 forks on GitHub."
  },
  {
    "input": "Which field does Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning belong to?",
    "output": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2502.12115&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2502.12115&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2301.00704&start=0&max_results=10 implementation?",
    "output": "The lucidrains/muse-maskgit-pytorch repository has 916 stars and 88 forks on GitHub."
  },
  {
    "input": "What is the GitHub repository for Wide & Deep Learning for Recommender Systems?",
    "output": "The code for Wide & Deep Learning for Recommender Systems is available at https://github.com/tensorflow/models (tensorflow/models)."
  },
  {
    "input": "What language is the lucidrains/PaLM-pytorch repository written in?",
    "output": "The PaLM: Scaling Language Modeling with Pathways implementation is written in Python."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2104.00355&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2104.00355&start=0&max_results=10 is available at https://github.com/facebookresearch/fairseq (facebookresearch/fairseq)."
  },
  {
    "input": "Give me an overview of Distilling the Knowledge in a Neural Network.",
    "output": "Distilling the Knowledge in a Neural Network is a 2015 paper in Machine Learning. The implementation is available at haitongli/knowledge-distillation-pytorch which a pytorch implementation for exploring deep and shallow knowledge distillation (kd) experiments with flexibility It is written in Python. The repository has 1,973 stars, indicating high community interest."
  },
  {
    "input": "Describe the amazon-science/mm-cot repository that implements arXiv Query: search_query=&id_list=2301.12652&start=0&max_results=10.",
    "output": "Official implementation for \"Multimodal Chain-of-Thought Reasoning in Language Models\" (stay tuned and more will be updated)"
  },
  {
    "input": "What are the key topics in the hendrycks/test repository?",
    "output": "The hendrycks/test repository covers: muti-task, transfer-learning, gpt-3, few-shot-learning."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2405.03520?",
    "output": "arXiv Query: search_query=&id_list=2405.03520&start=0&max_results=10"
  },
  {
    "input": "What year was DistilBERT: a distilled version of BERT released?",
    "output": "The DistilBERT: a distilled version of BERT paper was published in 2019."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2305.15334?",
    "output": "Gorilla: Large Language Model Connected with Massive APIs"
  },
  {
    "input": "What's the implementation repository for Distilling the Knowledge in a Neural Network?",
    "output": "The code for Distilling the Knowledge in a Neural Network is available at https://github.com/haitongli/knowledge-distillation-pytorch (haitongli/knowledge-distillation-pytorch)."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2302.03917&start=0&max_results=10 implementation?",
    "output": "The mit-han-lab/offsite-tuning repository has 382 stars and 39 forks on GitHub."
  },
  {
    "input": "What programming language is Decoupled Weight Decay Regularization implemented in?",
    "output": "The Decoupled Weight Decay Regularization implementation is written in Python."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2502.03387&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2502.03387&start=0&max_results=10 is available at https://github.com/GAIR-NLP/LIMO (GAIR-NLP/LIMO)."
  },
  {
    "input": "How popular is the Vicuna: An Open-Source Chatbot implementation?",
    "output": "The lm-sys/FastChat repository has 39,271 stars and 4,772 forks on GitHub."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 1711.05101?",
    "output": "Decoupled Weight Decay Regularization"
  },
  {
    "input": "What is the GitHub repository for Generating Wikipedia by Summarizing Long Sequences?",
    "output": "The code for Generating Wikipedia by Summarizing Long Sequences is available at https://github.com/tensorflow/tensor2tensor (tensorflow/tensor2tensor)."
  },
  {
    "input": "What paper has ArXiv ID 1508.07909?",
    "output": "Neural Machine Translation by Jointly Learning to Align and Translate"
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2302.12246&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2302.12246&start=0&max_results=10 is available at https://github.com/shizhediao/active-prompt (shizhediao/active-prompt)."
  },
  {
    "input": "What's the implementation repository for Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena?",
    "output": "The code for Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena is available at https://github.com/lm-sys/FastChat (lm-sys/FastChat)."
  },
  {
    "input": "What is ColBERT: Efficient Passage Search via Contextualized Late Interaction about?",
    "output": "ColBERT: Efficient Passage Search via Contextualized Late Interaction is a 2021 paper in Information Retrieval. The implementation is available at stanford-futuredata/ColBERT which colbert: state-of-the-art neural search (sigir'20, tacl'21, neurips'21, naacl'22, cikm'22, acl'23, emnlp'23) It is written in Python. The repository has 3,707 stars, indicating high community interest."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2406.10209&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2406.10209&start=0&max_results=10 is a 2024 paper in Machine Learning. The implementation is available at hpcaitech/Open-Sora which open-sora: democratizing efficient video production for all It is written in Python. The repository has 27,981 stars, indicating high community interest."
  },
  {
    "input": "Where can I find the code for SQuAD: 100,000+ Questions for Machine Comprehension of Text?",
    "output": "The code for SQuAD: 100,000+ Questions for Machine Comprehension of Text is available at https://github.com/rajpurkar/SQuAD-explorer (rajpurkar/SQuAD-explorer)."
  },
  {
    "input": "Tell me the title of ArXiv paper 2304.10592.",
    "output": "LLaVA: Large Language and Vision Assistant"
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2410.18050&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2410.18050&start=0&max_results=10 is available at https://github.com/ibm-granite/granite-3.0-language-models (ibm-granite/granite-3.0-language-models)."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2501.13824?",
    "output": "arXiv Query: search_query=&id_list=2501.13824&start=0&max_results=10"
  },
  {
    "input": "What are the key topics in the plurai-ai/intellagent repository?",
    "output": "The plurai-ai/intellagent repository covers: agent, evaluation, llmops, simulator, synthetic-data."
  },
  {
    "input": "Which field does MiniGPT-4: Enhancing Vision-Language Understanding belong to?",
    "output": "MiniGPT-4: Enhancing Vision-Language Understanding is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "When was the T5: Exploring the Limits of Transfer Learning paper published?",
    "output": "The T5: Exploring the Limits of Transfer Learning paper was published in 2019."
  },
  {
    "input": "What year was Question and Answer Test-Train Overlap in Open-Domain QA Datasets released?",
    "output": "The Question and Answer Test-Train Overlap in Open-Domain QA Datasets paper was published in 2020."
  },
  {
    "input": "What language is the haoliuhl/language-quantized-autoencoders repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2302.08242&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What is the ArXiv category of ColBERT: Efficient Passage Search via Contextualized Late Interaction?",
    "output": "ColBERT: Efficient Passage Search via Contextualized Late Interaction is in the Information Retrieval (cs.IR) category."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2503.20201&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2503.20201&start=0&max_results=10 is available at https://github.com/sentient-agi/OpenDeepSearch (sentient-agi/OpenDeepSearch)."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2109.03264&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2109.03264&start=0&max_results=10 paper was published in 2021."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2503.23513&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2503.23513&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "Give me an overview of PyTorch Geometric: Library for Geometric Deep Learning.",
    "output": "PyTorch Geometric: Library for Geometric Deep Learning is a 2019 paper in Machine Learning. The implementation is available at pyg-team/pytorch_geometric which graph neural network library for pytorch It is written in Python. The repository has 23,186 stars, indicating high community interest."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2302.09778&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2302.09778&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2504.01848&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2504.01848&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What paper has ArXiv ID 2303.04129?",
    "output": "arXiv Query: search_query=&id_list=2303.04129&start=0&max_results=10"
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2301.00808&start=0&max_results=10 repository have?",
    "output": "The facebookresearch/ConvNeXt-V2 repository has 1,890 stars and 155 forks on GitHub."
  },
  {
    "input": "Explain the ibm-granite/granite-code-models implementation of arXiv Query: search_query=&id_list=2405.03520&start=0&max_results=10.",
    "output": "Granite Code Models: A Family of Open Foundation Models for Code Intelligence"
  },
  {
    "input": "What year was CLIP: Learning Transferable Visual Models From Natural Language released?",
    "output": "The CLIP: Learning Transferable Visual Models From Natural Language paper was published in 2021."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2406.06326&start=0&max_results=10 repository have?",
    "output": "The lamini-ai/Lamini-Memory-Tuning repository has 275 stars and 12 forks on GitHub."
  },
  {
    "input": "What is the ArXiv category of Deep Graph Library: A Graph-Centric, Highly-Performant Package?",
    "output": "Deep Graph Library: A Graph-Centric, Highly-Performant Package is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Where can I find the code for BERT: Pre-training of Deep Bidirectional Transformers?",
    "output": "The code for BERT: Pre-training of Deep Bidirectional Transformers is available at https://github.com/google-research/bert (google-research/bert)."
  },
  {
    "input": "Explain the ysymyth/ReAct implementation of ReAct: Synergizing Reasoning and Acting in Language Models.",
    "output": "[ICLR 2023] ReAct: Synergizing Reasoning and Acting in Language Models"
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2405.03520&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2405.03520&start=0&max_results=10 is a 2024 paper in Machine Learning. The implementation is available at ibm-granite/granite-code-models which granite code models: a family of open foundation models for code intelligence The repository has 1,240 stars, indicating high community interest."
  },
  {
    "input": "Explain the bowang-lab/MedSAM implementation of Segment Anything in Medical Images.",
    "output": "Segment Anything in Medical Images"
  },
  {
    "input": "Where can I find the code for AlpacaEval: An Automatic Evaluator for Instruction-following?",
    "output": "The code for AlpacaEval: An Automatic Evaluator for Instruction-following is available at https://github.com/tatsu-lab/alpaca_eval (tatsu-lab/alpaca_eval)."
  },
  {
    "input": "What's the implementation repository for LLM.int8(): 8-bit Matrix Multiplication for Transformers?",
    "output": "The code for LLM.int8(): 8-bit Matrix Multiplication for Transformers is available at https://github.com/bitsandbytes-foundation/bitsandbytes (bitsandbytes-foundation/bitsandbytes)."
  },
  {
    "input": "What does the facebookresearch/contriever repository do?",
    "output": "Contriever: Unsupervised Dense Information Retrieval with Contrastive Learning"
  },
  {
    "input": "What is the GitHub repository for P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning?",
    "output": "The code for P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning is available at https://github.com/THUDM/P-tuning-v2 (THUDM/P-tuning-v2)."
  },
  {
    "input": "What is ChemCrow: Augmenting large-language models with chemistry tools about?",
    "output": "ChemCrow: Augmenting large-language models with chemistry tools is a 2023 paper in Artificial Intelligence. The implementation is available at ur-whitelab/chemcrow-public which chemcrow It is written in Python. The repository has 838 stars, indicating high community interest."
  },
  {
    "input": "Tell me the title of ArXiv paper 2301.09515.",
    "output": "arXiv Query: search_query=&id_list=2301.09515&start=0&max_results=10"
  },
  {
    "input": "In what year did ColBERT: Efficient Passage Search via Contextualized Late Interaction come out?",
    "output": "The ColBERT: Efficient Passage Search via Contextualized Late Interaction paper was published in 2021."
  },
  {
    "input": "Give me an overview of EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification.",
    "output": "EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification is a 2019 paper in Computation and Language (Natural Language Processing). The implementation is available at jasonwei20/eda_nlp which data augmentation for nlp, presented at emnlp 2019 It is written in Python. The repository has 1,649 stars, indicating high community interest."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2505.22954&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2505.22954&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at ruixin31/Spurious_Rewards It is written in Python. The repository has 342 stars, indicating high community interest."
  },
  {
    "input": "Tell me the title of ArXiv paper 1711.00489.",
    "output": "Horovod: fast and easy distributed deep learning in TensorFlow"
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2405.02803&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2405.02803&start=0&max_results=10 is available at https://github.com/ibm-granite/granite-code-models (ibm-granite/granite-code-models)."
  },
  {
    "input": "What language is the google-research/google-research repository written in?",
    "output": "The Synthetic Data Generation with Large Language Models implementation is written in Jupyter Notebook."
  },
  {
    "input": "Describe the bigcode-project/starcoder repository that implements StarCoder: May the source be with you!.",
    "output": "Home of StarCoder: fine-tuning & inference!"
  },
  {
    "input": "Tell me the title of ArXiv paper 1612.03144.",
    "output": "Feature Pyramid Networks for Object Detection"
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2503.01141&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2503.01141&start=0&max_results=10 is available at https://github.com/LTH14/fractalgen (LTH14/fractalgen)."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2302.04870&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2302.04870&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at mit-han-lab/offsite-tuning which offsite-tuning: transfer learning without full model It is written in Python. The repository has 382 stars, indicating high community interest."
  },
  {
    "input": "Where can I find the code for An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale?",
    "output": "The code for An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale is available at https://github.com/google-research/vision_transformer (google-research/vision_transformer)."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2302.12766&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2302.12766&start=0&max_results=10 is available at https://github.com/siddk/voltron-robotics (siddk/voltron-robotics)."
  },
  {
    "input": "Tell me the title of ArXiv paper 2301.01181.",
    "output": "arXiv Query: search_query=&id_list=2301.01181&start=0&max_results=10"
  },
  {
    "input": "What programming language is Depthwise Separable Convolutions for Neural Machine Translation implemented in?",
    "output": "The Depthwise Separable Convolutions for Neural Machine Translation implementation is written in Jupyter Notebook."
  },
  {
    "input": "Explain the THUDM/P-tuning-v2 implementation of P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning.",
    "output": "An optimized deep prompt tuning strategy comparable to fine-tuning across scales and tasks"
  },
  {
    "input": "Tell me the title of ArXiv paper 2308.08155.",
    "output": "MetaGPT: Meta Programming for Multi-Agent Systems"
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2201.08239?",
    "output": "Contriever: Unsupervised Dense Information Retrieval"
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2301.01947&start=0&max_results=10 repository have?",
    "output": "The oughtinc/ice repository has 563 stars and 70 forks on GitHub."
  },
  {
    "input": "Describe the ContextualAI/HALOs repository that implements arXiv Query: search_query=&id_list=2312.02120&start=0&max_results=10.",
    "output": "A library with extensible implementations of DPO, KTO, PPO, ORPO, and other human-aware loss functions (HALOs)."
  },
  {
    "input": "Summarize the InstructBLIP: Towards General-purpose Vision-Language Models paper.",
    "output": "InstructBLIP: Towards General-purpose Vision-Language Models is a 2023 paper in Computer Vision and Pattern Recognition. The implementation is available at salesforce/LAVIS which lavis - a one-stop library for language-vision intelligence It is written in Jupyter Notebook. The repository has 11,040 stars, indicating high community interest."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2502.09992?",
    "output": "arXiv Query: search_query=&id_list=2502.09992&start=0&max_results=10"
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2302.07459?",
    "output": "arXiv Query: search_query=&id_list=2302.07459&start=0&max_results=10"
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2504.11844&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2504.11844&start=0&max_results=10 is available at https://github.com/Crista23/goal_directedness_llms (Crista23/goal_directedness_llms)."
  },
  {
    "input": "Describe the meta-llama/llama repository that implements LLaMA: Open and Efficient Foundation Language Models.",
    "output": "Inference code for Llama models"
  },
  {
    "input": "Tell me the title of ArXiv paper 2303.04671.",
    "output": "arXiv Query: search_query=&id_list=2303.04671&start=0&max_results=10"
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2503.22678&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2503.22678&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at MAXNORM8650/MedAgentSim which medagentsim: self-evolving multi-agent simulations for realistic clinical interactions, miccai 2025 (oral and early accepted) It is written in Python. The repository has 94 stars, indicating high community interest."
  },
  {
    "input": "Describe the ruiqi-zhong/D5 repository that implements arXiv Query: search_query=&id_list=2303.01469&start=0&max_results=10.",
    "output": "The GitHub repo for Goal Driven Discovery of Distributional Differences via Language Descriptions"
  },
  {
    "input": "In what year did Beir: A Heterogeneous Benchmark for IR come out?",
    "output": "The Beir: A Heterogeneous Benchmark for IR paper was published in 2022."
  },
  {
    "input": "Summarize the Segment Anything in Medical Images paper.",
    "output": "Segment Anything in Medical Images is a 2023 paper in Computer Vision and Pattern Recognition. The implementation is available at bowang-lab/MedSAM which segment anything in medical images It is written in Jupyter Notebook. The repository has 3,930 stars, indicating high community interest."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2505.22954&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2505.22954&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "Where can I find the code for MRKL Systems: A modular approach to AGI?",
    "output": "The code for MRKL Systems: A modular approach to AGI is available at https://github.com/langchain-ai/langchain (langchain-ai/langchain)."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2301.04104&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2301.04104&start=0&max_results=10 is available at https://github.com/google-deepmind/tracr (google-deepmind/tracr)."
  },
  {
    "input": "Tell me the title of ArXiv paper 2106.09685.",
    "output": "LoRA: Low-Rank Adaptation of Large Language Models"
  },
  {
    "input": "Give me an overview of LLM.int8(): 8-bit Matrix Multiplication for Transformers.",
    "output": "LLM.int8(): 8-bit Matrix Multiplication for Transformers is a 2022 paper in Machine Learning. The implementation is available at bitsandbytes-foundation/bitsandbytes which accessible large language models via k-bit quantization for pytorch. It is written in Python. The repository has 7,771 stars, indicating high community interest."
  },
  {
    "input": "In what year did Whisper: Robust Speech Recognition via Large-Scale Weak Supervision come out?",
    "output": "The Whisper: Robust Speech Recognition via Large-Scale Weak Supervision paper was published in 2022."
  },
  {
    "input": "What is the ArXiv category of Segment Anything?",
    "output": "Segment Anything is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "Where can I find the code for Wav2Vec 2.0: A Framework for Self-Supervised Learning of Speech?",
    "output": "The code for Wav2Vec 2.0: A Framework for Self-Supervised Learning of Speech is available at https://github.com/facebookresearch/fairseq (facebookresearch/fairseq)."
  },
  {
    "input": "When was the EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification paper published?",
    "output": "The EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification paper was published in 2019."
  },
  {
    "input": "How popular is the FlashAttention: Fast and Memory-Efficient Exact Attention implementation?",
    "output": "The Dao-AILab/flash-attention repository has 20,719 stars and 2,160 forks on GitHub."
  },
  {
    "input": "Explain the huggingface/alignment-handbook implementation of Zephyr: Direct Distillation of LM Alignment.",
    "output": "Robust recipes to align language models with human and AI preferences"
  },
  {
    "input": "Tell me the title of ArXiv paper 1903.07293.",
    "output": "PyTorch Geometric: Library for Geometric Deep Learning"
  },
  {
    "input": "What is the GitHub repository for High-Resolution Image Synthesis with Latent Diffusion Models?",
    "output": "The code for High-Resolution Image Synthesis with Latent Diffusion Models is available at https://github.com/CompVis/latent-diffusion (CompVis/latent-diffusion)."
  },
  {
    "input": "What language is the princeton-nlp/SimCSE repository written in?",
    "output": "The SimCSE: Simple Contrastive Learning of Sentence Embeddings implementation is written in Python."
  },
  {
    "input": "Describe the lm-sys/FastChat repository that implements Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena.",
    "output": "An open platform for training, serving, and evaluating large language models. Release repo for Vicuna and Chatbot Arena."
  },
  {
    "input": "Give me an overview of CLIP: Learning Transferable Visual Models From Natural Language.",
    "output": "CLIP: Learning Transferable Visual Models From Natural Language is a 2021 paper in Computer Vision and Pattern Recognition. The implementation is available at openai/CLIP which clip (contrastive language-image pretraining),  predict the most relevant text snippet given an image It is written in Jupyter Notebook. The repository has 31,706 stars, indicating high community interest."
  },
  {
    "input": "Tell me the title of ArXiv paper 2001.04451.",
    "output": "Longformer: The Long-Document Transformer"
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2301.11316&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2301.11316&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Describe the GAIR-NLP/LIMO repository that implements arXiv Query: search_query=&id_list=2502.01061&start=0&max_results=10.",
    "output": "[COLM 2025] LIMO: Less is More for Reasoning"
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2101.00027?",
    "output": "SimCSE: Simple Contrastive Learning of Sentence Embeddings"
  },
  {
    "input": "What paper has ArXiv ID 2301.00774?",
    "output": "arXiv Query: search_query=&id_list=2301.00774&start=0&max_results=10"
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2302.05206&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2302.05206&start=0&max_results=10 is available at https://github.com/tianjunz/HIR (tianjunz/HIR)."
  },
  {
    "input": "What does the microsoft/DeBERTa repository do?",
    "output": "The implementation of DeBERTa"
  },
  {
    "input": "What is the GitHub repository for DreamFusion: Text-to-3D using 2D Diffusion?",
    "output": "The code for DreamFusion: Text-to-3D using 2D Diffusion is available at https://github.com/ashawkey/stable-dreamfusion (ashawkey/stable-dreamfusion)."
  },
  {
    "input": "What paper has ArXiv ID 2301.00808?",
    "output": "arXiv Query: search_query=&id_list=2301.00808&start=0&max_results=10"
  },
  {
    "input": "Summarize the Flamingo: A Visual Language Model for Few-Shot Learning paper.",
    "output": "Flamingo: A Visual Language Model for Few-Shot Learning is a 2022 paper in Computer Vision and Pattern Recognition. The implementation is available at lucidrains/flamingo-pytorch which implementation of \ud83e\udda9 flamingo, state-of-the-art few-shot visual question answering attention net out of deepmind, in pytorch It is written in Python. The repository has 1,270 stars, indicating high community interest."
  },
  {
    "input": "Explain the tensorflow/tensor2tensor implementation of Attention Is All You Need.",
    "output": "Library of deep learning models and datasets designed to make deep learning more accessible and accelerate ML research."
  },
  {
    "input": "What does the autonomousvision/stylegan-t repository do?",
    "output": "[ICML'23] StyleGAN-T: Unlocking the Power of GANs for Fast Large-Scale Text-to-Image Synthesis"
  },
  {
    "input": "What language is the meta-llama/codellama repository written in?",
    "output": "The Code Llama: Open Foundation Models for Code implementation is written in Python."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2406.12824&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2406.12824&start=0&max_results=10 is a 2024 paper in Machine Learning. The implementation is available at hpcaitech/Open-Sora which open-sora: democratizing efficient video production for all It is written in Python. The repository has 27,981 stars, indicating high community interest."
  },
  {
    "input": "What paper has ArXiv ID 2503.22708?",
    "output": "arXiv Query: search_query=&id_list=2503.22708&start=0&max_results=10"
  },
  {
    "input": "What is the GitHub repository for CLIP: Learning Transferable Visual Models From Natural Language?",
    "output": "The code for CLIP: Learning Transferable Visual Models From Natural Language is available at https://github.com/openai/CLIP (openai/CLIP)."
  },
  {
    "input": "What programming language is Segment Anything implemented in?",
    "output": "The Segment Anything implementation is written in Jupyter Notebook."
  },
  {
    "input": "What topics does the ELECTRA: Pre-training Text Encoders as Discriminators implementation cover?",
    "output": "The google-research/electra repository covers: nlp, deep-learning, tensorflow."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 1905.09418?",
    "output": "BERT Rediscovers the Classical NLP Pipeline"
  },
  {
    "input": "Which field does WizardCoder: Empowering Code LLMs with Evol-Instruct belong to?",
    "output": "WizardCoder: Empowering Code LLMs with Evol-Instruct is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Give me an overview of Grounding DINO: Marrying DINO with Grounded Pre-Training.",
    "output": "Grounding DINO: Marrying DINO with Grounded Pre-Training is a 2023 paper in Computer Vision and Pattern Recognition. The implementation is available at IDEA-Research/GroundingDINO which [eccv 2024] official implementation of the paper \"grounding dino: marrying dino with grounded pre-training for open-set object detection\" It is written in Python. The repository has 9,324 stars, indicating high community interest."
  },
  {
    "input": "What category is the Contriever: Unsupervised Dense Information Retrieval paper in?",
    "output": "Contriever: Unsupervised Dense Information Retrieval is in the Information Retrieval (cs.IR) category."
  },
  {
    "input": "Give me an overview of Decoupled Weight Decay Regularization.",
    "output": "Decoupled Weight Decay Regularization is a 2017 paper in Machine Learning. The implementation is available at pytorch/pytorch which tensors and dynamic neural networks in python with strong gpu acceleration It is written in Python. The repository has 95,348 stars, indicating high community interest."
  },
  {
    "input": "Explain the AudioPaLM: A Large Language Model for Speech Understanding paper.",
    "output": "AudioPaLM: A Large Language Model for Speech Understanding is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at lucidrains/audiolm-pytorch which implementation of audiolm, a sota language modeling approach to audio generation out of google research, in pytorch It is written in Python. The repository has 2,607 stars, indicating high community interest."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2301.13688&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2301.13688&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at amazon-science/mm-cot which official implementation for \"multimodal chain-of-thought reasoning in language models\" (stay tuned and more will be updated) It is written in Python. The repository has 3,982 stars, indicating high community interest."
  },
  {
    "input": "Explain the SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding paper.",
    "output": "SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding is a 2019 paper in Computation and Language (Natural Language Processing). The implementation is available at nyu-mll/jiant which jiant is an nlp toolkit It is written in Python. The repository has 1,672 stars, indicating high community interest."
  },
  {
    "input": "Describe the facebookresearch/ConvNeXt-V2 repository that implements arXiv Query: search_query=&id_list=2301.00808&start=0&max_results=10.",
    "output": "Code release for ConvNeXt V2 model"
  },
  {
    "input": "In what year did Billion-scale similarity search with GPUs come out?",
    "output": "The Billion-scale similarity search with GPUs paper was published in 2019."
  },
  {
    "input": "How many stars does the Longformer: The Long-Document Transformer repository have?",
    "output": "The allenai/longformer repository has 2,175 stars and 288 forks on GitHub."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2502.14815&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2502.14815&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2303.06865&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2303.06865&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at FMInference/FlexLLMGen which running large language models on a single gpu for throughput-oriented scenarios. It is written in Python. The repository has 9,379 stars, indicating high community interest."
  },
  {
    "input": "What is the GitHub repository for Beir: A Heterogeneous Benchmark for IR?",
    "output": "The code for Beir: A Heterogeneous Benchmark for IR is available at https://github.com/beir-cellar/beir (beir-cellar/beir)."
  },
  {
    "input": "What's the implementation repository for Multi-Task Deep Neural Networks for Natural Language Understanding?",
    "output": "The code for Multi-Task Deep Neural Networks for Natural Language Understanding is available at https://github.com/namisan/mt-dnn (namisan/mt-dnn)."
  },
  {
    "input": "What category is the ViT: An Image is Worth 16x16 Words paper in?",
    "output": "ViT: An Image is Worth 16x16 Words is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "What paper has ArXiv ID 1902.05188?",
    "output": "JAX: Composable transformations of Python+NumPy programs"
  },
  {
    "input": "When was the Enriching Word Vectors with Subword Information paper published?",
    "output": "The Enriching Word Vectors with Subword Information paper was published in 2016."
  },
  {
    "input": "Summarize the PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel paper.",
    "output": "PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel is a 2022 paper in Distributed, Parallel, and Cluster Computing. The implementation is available at pytorch/pytorch which tensors and dynamic neural networks in python with strong gpu acceleration It is written in Python. The repository has 95,348 stars, indicating high community interest."
  },
  {
    "input": "What does the google-research/albert repository do?",
    "output": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"
  },
  {
    "input": "What category is the LoRA: Low-Rank Adaptation of Large Language Models paper in?",
    "output": "LoRA: Low-Rank Adaptation of Large Language Models is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Give me an overview of RoFormer: Enhanced Transformer with Rotary Position Embedding.",
    "output": "RoFormer: Enhanced Transformer with Rotary Position Embedding is a 2021 paper in Computation and Language (Natural Language Processing). The implementation is available at ZhuiyiTechnology/roformer which rotary transformer It is written in Python. The repository has 1,051 stars, indicating high community interest."
  },
  {
    "input": "What's the implementation repository for Mamba: Linear-Time Sequence Modeling with Selective State Spaces?",
    "output": "The code for Mamba: Linear-Time Sequence Modeling with Selective State Spaces is available at https://github.com/state-spaces/mamba (state-spaces/mamba)."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2301.13688&start=0&max_results=10 implementation?",
    "output": "The amazon-science/mm-cot repository has 3,982 stars and 333 forks on GitHub."
  },
  {
    "input": "What programming language is Neural Message Passing for Quantum Chemistry implemented in?",
    "output": "The Neural Message Passing for Quantum Chemistry implementation is written in Python."
  },
  {
    "input": "What language is the tatsu-lab/alpaca_eval repository written in?",
    "output": "The AlpacaEval: An Automatic Evaluator for Instruction-following implementation is written in Jupyter Notebook."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2502.12115&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2502.12115&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2204.02311?",
    "output": "PaLM: Scaling Language Modeling with Pathways"
  },
  {
    "input": "What topics does the Graph Attention Networks implementation cover?",
    "output": "The PetarV-/GAT repository covers: graph-attention-networks, attention-mechanism, self-attention, tensorflow, neural-networks, python."
  },
  {
    "input": "Explain the PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel paper.",
    "output": "PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel is a 2022 paper in Distributed, Parallel, and Cluster Computing. The implementation is available at pytorch/pytorch which tensors and dynamic neural networks in python with strong gpu acceleration It is written in Python. The repository has 95,348 stars, indicating high community interest."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2109.03264&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2109.03264&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Explain the Analyzing the Structure of Attention in a Transformer Language Model paper.",
    "output": "Analyzing the Structure of Attention in a Transformer Language Model is a 2020 paper in Computation and Language (Natural Language Processing). The implementation is available at clarkkev/attention-analysis It is written in Jupyter Notebook. The repository has 469 stars, indicating high community interest."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2406.09308&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2406.09308&start=0&max_results=10 is a 2024 paper in Machine Learning. The implementation is available at lamini-ai/Lamini-Memory-Tuning which banishing llm hallucinations requires rethinking generalization The repository has 275 stars, indicating high community interest."
  },
  {
    "input": "What is the ArXiv category of Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena?",
    "output": "Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What paper has ArXiv ID 2305.18290?",
    "output": "Direct Preference Optimization: Your Language Model is a Reward Model"
  },
  {
    "input": "What's the implementation repository for FlashAttention: Fast and Memory-Efficient Exact Attention?",
    "output": "The code for FlashAttention: Fast and Memory-Efficient Exact Attention is available at https://github.com/Dao-AILab/flash-attention (Dao-AILab/flash-attention)."
  },
  {
    "input": "How many stars does the Direct Preference Optimization: Your Language Model is a Reward Model repository have?",
    "output": "The eric-mitchell/direct-preference-optimization repository has 2,789 stars and 231 forks on GitHub."
  },
  {
    "input": "Tell me the title of ArXiv paper 2004.13637.",
    "output": "arXiv Query: search_query=&id_list=2004.13637&start=0&max_results=10"
  },
  {
    "input": "What is LoRA: Low-Rank Adaptation of Large Language Models about?",
    "output": "LoRA: Low-Rank Adaptation of Large Language Models is a 2021 paper in Computation and Language (Natural Language Processing). The implementation is available at microsoft/LoRA which code for loralib, an implementation of \"lora: low-rank adaptation of large language models\" It is written in Python. The repository has 12,979 stars, indicating high community interest."
  },
  {
    "input": "What's the implementation repository for Synthetic Data Generation with Large Language Models?",
    "output": "The code for Synthetic Data Generation with Large Language Models is available at https://github.com/google-research/google-research (google-research/google-research)."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2410.18050&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2410.18050&start=0&max_results=10 is a 2024 paper in Machine Learning. The implementation is available at ibm-granite/granite-3.0-language-models The repository has 267 stars, indicating high community interest."
  },
  {
    "input": "Explain the facebookresearch/fairseq implementation of arXiv Query: search_query=&id_list=2102.01192&start=0&max_results=10.",
    "output": "Facebook AI Research Sequence-to-Sequence Toolkit written in Python."
  },
  {
    "input": "Explain the hpcaitech/Open-Sora implementation of arXiv Query: search_query=&id_list=2406.12824&start=0&max_results=10.",
    "output": "Open-Sora: Democratizing Efficient Video Production for All"
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2302.04023&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2302.04023&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at mit-han-lab/offsite-tuning which offsite-tuning: transfer learning without full model It is written in Python. The repository has 382 stars, indicating high community interest."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2302.08242&start=0&max_results=10 repository have?",
    "output": "The haoliuhl/language-quantized-autoencoders repository has 111 stars and 5 forks on GitHub."
  },
  {
    "input": "Which field does Direct Preference Optimization: Your Language Model is a Reward Model belong to?",
    "output": "Direct Preference Optimization: Your Language Model is a Reward Model is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2406.09403&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2406.09403&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "How many stars does the Neural Machine Translation by Jointly Learning to Align and Translate repository have?",
    "output": "The lisa-groundhog/GroundHog repository has 613 stars and 230 forks on GitHub."
  },
  {
    "input": "What paper has ArXiv ID 2302.04023?",
    "output": "arXiv Query: search_query=&id_list=2302.04023&start=0&max_results=10"
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2405.03520&start=0&max_results=10 implementation?",
    "output": "The ibm-granite/granite-code-models repository has 1,240 stars and 88 forks on GitHub."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2212.04356?",
    "output": "Whisper: Robust Speech Recognition via Large-Scale Weak Supervision"
  },
  {
    "input": "In what year did MetaGPT: Meta Programming for Multi-Agent Systems come out?",
    "output": "The MetaGPT: Meta Programming for Multi-Agent Systems paper was published in 2023."
  },
  {
    "input": "What's the implementation repository for GPT-4 Technical Report?",
    "output": "The code for GPT-4 Technical Report is available at https://github.com/openai/evals (openai/evals)."
  },
  {
    "input": "What does the LTH14/fractalgen repository do?",
    "output": "PyTorch implementation of FractalGen https://arxiv.org/abs/2502.17437"
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2404.03592&start=0&max_results=10 repository have?",
    "output": "The OpenBMB/Eurus repository has 320 stars and 14 forks on GitHub."
  },
  {
    "input": "How many stars does the Very Deep Convolutional Networks for Large-Scale Image Recognition repository have?",
    "output": "The machrisaa/tensorflow-vgg repository has 2,239 stars and 1,073 forks on GitHub."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2312.02783&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2312.02783&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2302.04870&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2302.04870&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "Explain the HazyResearch/safari implementation of arXiv Query: search_query=&id_list=2303.03846&start=0&max_results=10.",
    "output": "Convolutions for Sequence Modeling"
  },
  {
    "input": "Where can I find the code for MiniGPT-4: Enhancing Vision-Language Understanding?",
    "output": "The code for MiniGPT-4: Enhancing Vision-Language Understanding is available at https://github.com/Vision-CAIR/MiniGPT-4 (Vision-CAIR/MiniGPT-4)."
  },
  {
    "input": "How popular is the Denoising Diffusion Probabilistic Models implementation?",
    "output": "The hojonathanho/diffusion repository has 4,860 stars and 455 forks on GitHub."
  },
  {
    "input": "Tell me the title of ArXiv paper 1312.5602.",
    "output": "Playing Atari with Deep Reinforcement Learning"
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2501.11067&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2501.11067&start=0&max_results=10 is available at https://github.com/plurai-ai/intellagent (plurai-ai/intellagent)."
  },
  {
    "input": "Summarize the Training language models to follow instructions with human feedback paper.",
    "output": "Training language models to follow instructions with human feedback is a 2022 paper in Computation and Language (Natural Language Processing). The implementation is available at openai/following-instructions-human-feedback The repository has 1,249 stars, indicating high community interest."
  },
  {
    "input": "Explain the microsoft/unilm implementation of Text Embeddings by Weakly-Supervised Contrastive Pre-training.",
    "output": "Large-scale Self-supervised Pre-training Across Tasks, Languages, and Modalities"
  },
  {
    "input": "What does the lamini-ai/Lamini-Memory-Tuning repository do?",
    "output": "Banishing LLM Hallucinations Requires Rethinking Generalization"
  },
  {
    "input": "Explain the SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities paper.",
    "output": "SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at 0nutation/SpeechGPT which speechgpt series: speech large language models It is written in Python. The repository has 1,396 stars, indicating high community interest."
  },
  {
    "input": "What does the GAIR-NLP/LIMO repository do?",
    "output": "[COLM 2025] LIMO: Less is More for Reasoning"
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2502.12115&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2502.12115&start=0&max_results=10 is available at https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero (Open-Reasoner-Zero/Open-Reasoner-Zero)."
  },
  {
    "input": "Explain the facebookresearch/faiss implementation of Billion-scale similarity search with GPUs.",
    "output": "A library for efficient similarity search and clustering of dense vectors."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2505.22101&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2505.22101&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at ruixin31/Spurious_Rewards It is written in Python. The repository has 342 stars, indicating high community interest."
  },
  {
    "input": "What is the GitHub repository for Segment Anything?",
    "output": "The code for Segment Anything is available at https://github.com/facebookresearch/segment-anything (facebookresearch/segment-anything)."
  },
  {
    "input": "Which field does PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel belong to?",
    "output": "PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel is in the Distributed, Parallel, and Cluster Computing (cs.DC) category."
  },
  {
    "input": "When was the ColBERT: Efficient Passage Search via Contextualized Late Interaction paper published?",
    "output": "The ColBERT: Efficient Passage Search via Contextualized Late Interaction paper was published in 2021."
  },
  {
    "input": "Tell me the title of ArXiv paper 2210.03629.",
    "output": "ReAct: Synergizing Reasoning and Acting in Language Models"
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2505.23006&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2505.23006&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2405.04324&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2405.04324&start=0&max_results=10 is a 2024 paper in Machine Learning. The implementation is available at ibm-granite/granite-code-models which granite code models: a family of open foundation models for code intelligence The repository has 1,240 stars, indicating high community interest."
  },
  {
    "input": "What does the meta-llama/llama repository do?",
    "output": "Inference code for Llama models"
  },
  {
    "input": "Tell me the title of ArXiv paper 2301.11316.",
    "output": "arXiv Query: search_query=&id_list=2301.11316&start=0&max_results=10"
  },
  {
    "input": "What programming language is Med-PaLM 2: Towards Expert-Level Medical Question Answering implemented in?",
    "output": "The Med-PaLM 2: Towards Expert-Level Medical Question Answering implementation is written in Jupyter Notebook."
  },
  {
    "input": "Explain the SQuAD: 100,000+ Questions for Machine Comprehension of Text paper.",
    "output": "SQuAD: 100,000+ Questions for Machine Comprehension of Text is a 2016 paper in Computation and Language (Natural Language Processing). The implementation is available at rajpurkar/SQuAD-explorer which visually explore the stanford question answering dataset It is written in JavaScript. The repository has 570 stars, indicating high community interest."
  },
  {
    "input": "Give me an overview of Faster R-CNN: Towards Real-Time Object Detection.",
    "output": "Faster R-CNN: Towards Real-Time Object Detection is a 2015 paper in Computer Vision and Pattern Recognition. The implementation is available at rbgirshick/py-faster-rcnn which faster r-cnn (python implementation) -- see https://github.com/shaoqingren/faster_rcnn for the official matlab version It is written in Python. The repository has 8,269 stars, indicating high community interest."
  },
  {
    "input": "What year was XLNet: Generalized Autoregressive Pretraining released?",
    "output": "The XLNet: Generalized Autoregressive Pretraining paper was published in 2019."
  },
  {
    "input": "What is the GitHub repository for Know What You Don't Know: Unanswerable Questions for SQuAD?",
    "output": "The code for Know What You Don't Know: Unanswerable Questions for SQuAD is available at https://github.com/rajpurkar/SQuAD-explorer (rajpurkar/SQuAD-explorer)."
  },
  {
    "input": "What language is the google-deepmind/code_contests repository written in?",
    "output": "The Competition-Level Code Generation with AlphaCode implementation is written in C++."
  },
  {
    "input": "Tell me the title of ArXiv paper 1706.03059.",
    "output": "Depthwise Separable Convolutions for Neural Machine Translation"
  },
  {
    "input": "Describe the ruiqi-zhong/D5 repository that implements arXiv Query: search_query=&id_list=2302.14838&start=0&max_results=10.",
    "output": "The GitHub repo for Goal Driven Discovery of Distributional Differences via Language Descriptions"
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2302.04023&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2302.04023&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What category is the Densely Connected Convolutional Networks paper in?",
    "output": "Densely Connected Convolutional Networks is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "What is Text Embeddings by Weakly-Supervised Contrastive Pre-training about?",
    "output": "Text Embeddings by Weakly-Supervised Contrastive Pre-training is a 2022 paper in Computation and Language (Natural Language Processing). The implementation is available at microsoft/unilm which large-scale self-supervised pre-training across tasks, languages, and modalities It is written in Python. The repository has 21,848 stars, indicating high community interest."
  },
  {
    "input": "What topics does the DistilBERT: a distilled version of BERT implementation cover?",
    "output": "The huggingface/transformers repository covers: nlp, natural-language-processing, pytorch, pytorch-transformers, transformer, model-hub, pretrained-models, speech-recognition, hacktoberfest, python, machine-learning, deep-learning, audio, deepseek, gemma, glm, llm, qwen, vlm."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2102.01192&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2102.01192&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2503.01141&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2503.01141&start=0&max_results=10 is available at https://github.com/LTH14/fractalgen (LTH14/fractalgen)."
  },
  {
    "input": "How many stars does the GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers repository have?",
    "output": "The IST-DASLab/gptq repository has 2,221 stars and 186 forks on GitHub."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2301.13188&start=0&max_results=10 repository have?",
    "output": "The amazon-science/mm-cot repository has 3,982 stars and 333 forks on GitHub."
  },
  {
    "input": "In what year did Synthetic Data Generation with Large Language Models come out?",
    "output": "The Synthetic Data Generation with Large Language Models paper was published in 2023."
  },
  {
    "input": "What is the ArXiv category of Enriching Word Vectors with Subword Information?",
    "output": "Enriching Word Vectors with Subword Information is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2303.02913&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2303.02913&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at Shark-NLP/OpenICL which openicl is an open-source framework to facilitate research, development, and prototyping of in-context learning. It is written in Python. The repository has 579 stars, indicating high community interest."
  },
  {
    "input": "Explain the mit-han-lab/offsite-tuning implementation of arXiv Query: search_query=&id_list=2302.04870&start=0&max_results=10.",
    "output": "Offsite-Tuning: Transfer Learning without Full Model"
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2503.22678&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2503.22678&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at MAXNORM8650/MedAgentSim which medagentsim: self-evolving multi-agent simulations for realistic clinical interactions, miccai 2025 (oral and early accepted) It is written in Python. The repository has 94 stars, indicating high community interest."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2301.12652&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2301.12652&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2403.17887&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2403.17887&start=0&max_results=10 is a 2024 paper in Machine Learning. The implementation is available at OpenBMB/Eurus It is written in Python. The repository has 320 stars, indicating high community interest."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2303.02506&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2303.02506&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at NVlabs/prismer which the implementation of \"prismer: a vision-language model with multi-task experts\". It is written in Python. The repository has 1,307 stars, indicating high community interest."
  },
  {
    "input": "Tell me the title of ArXiv paper 2107.03374.",
    "output": "Codex: Evaluating Large Language Models Trained on Code"
  },
  {
    "input": "What programming language is Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks implemented in?",
    "output": "The Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks implementation is written in Python."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2006.03511&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2006.03511&start=0&max_results=10 is a 2020 paper in Machine Learning. The implementation is available at facebookresearch/TransCoder which public release of the transcoder research project https://arxiv.org/pdf/2006.03511.pdf It is written in Python. The repository has 1,722 stars, indicating high community interest."
  },
  {
    "input": "What is MMLU: Measuring Massive Multitask Language Understanding about?",
    "output": "MMLU: Measuring Massive Multitask Language Understanding is a 2021 paper in cs.CY. The implementation is available at hendrycks/test which measuring massive multitask language understanding | iclr 2021 It is written in Python. The repository has 1,523 stars, indicating high community interest."
  },
  {
    "input": "What is the GitHub repository for BLIP: Bootstrapping Language-Image Pre-training?",
    "output": "The code for BLIP: Bootstrapping Language-Image Pre-training is available at https://github.com/salesforce/BLIP (salesforce/BLIP)."
  },
  {
    "input": "Summarize the SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding paper.",
    "output": "SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding is a 2019 paper in Computation and Language (Natural Language Processing). The implementation is available at nyu-mll/jiant which jiant is an nlp toolkit It is written in Python. The repository has 1,672 stars, indicating high community interest."
  },
  {
    "input": "Give me an overview of ELECTRA: Pre-training Text Encoders as Discriminators.",
    "output": "ELECTRA: Pre-training Text Encoders as Discriminators is a 2019 paper in Computation and Language (Natural Language Processing). The implementation is available at google-research/electra which electra: pre-training text encoders as discriminators rather than generators It is written in Python. The repository has 2,365 stars, indicating high community interest."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2107.07346&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2107.07346&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What is Self-RAG: Learning to Retrieve, Generate, and Critique about?",
    "output": "Self-RAG: Learning to Retrieve, Generate, and Critique is a 2022 paper in Computation and Language (Natural Language Processing). The implementation is available at AkariAsai/self-rag which this includes the original implementation of self-rag: learning to retrieve, generate and critique through self-reflection by akari asai, zeqiu wu, yizhong wang, avirup sil, and hannaneh hajishirzi. It is written in Python. The repository has 2,252 stars, indicating high community interest."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2406.06326?",
    "output": "arXiv Query: search_query=&id_list=2406.06326&start=0&max_results=10"
  },
  {
    "input": "What are the key topics in the Megvii-BaseDetection/YOLOX repository?",
    "output": "The Megvii-BaseDetection/YOLOX repository covers: yolox, yolov3, onnx, tensorrt, ncnn, openvino, pytorch, megengine, object-detection, yolo, deep-learning."
  },
  {
    "input": "How popular is the CLIP: Learning Transferable Visual Models From Natural Language implementation?",
    "output": "The openai/CLIP repository has 31,706 stars and 3,848 forks on GitHub."
  },
  {
    "input": "What language is the google-deepmind/dqn repository written in?",
    "output": "The Playing Atari with Deep Reinforcement Learning implementation is written in Lua."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2406.06326&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2406.06326&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Explain the noahshinn/reflexion implementation of Reflexion: Language Agents with Verbal Reinforcement Learning.",
    "output": "[NeurIPS 2023] Reflexion: Language Agents with Verbal Reinforcement Learning"
  },
  {
    "input": "What is the ArXiv category of Neural Collaborative Filtering?",
    "output": "Neural Collaborative Filtering is in the Information Retrieval (cs.IR) category."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2301.00704&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2301.00704&start=0&max_results=10 is available at https://github.com/lucidrains/muse-maskgit-pytorch (lucidrains/muse-maskgit-pytorch)."
  },
  {
    "input": "What is the GitHub repository for MetaGPT: Meta Programming for Multi-Agent Systems?",
    "output": "The code for MetaGPT: Meta Programming for Multi-Agent Systems is available at https://github.com/FoundationAgents/MetaGPT (FoundationAgents/MetaGPT)."
  },
  {
    "input": "What paper has ArXiv ID 2001.04451?",
    "output": "Longformer: The Long-Document Transformer"
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2505.23006&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2505.23006&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "When was the Very Deep Convolutional Networks for Large-Scale Image Recognition paper published?",
    "output": "The Very Deep Convolutional Networks for Large-Scale Image Recognition paper was published in 2014."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2502.17437&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2502.17437&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at LTH14/fractalgen which pytorch implementation of fractalgen https://arxiv.org/abs/2502.17437 It is written in Python. The repository has 1,196 stars, indicating high community interest."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2002.04745?",
    "output": "Lookahead Optimizer: k steps forward, 1 step back"
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2307.01952?",
    "output": "Animate Anyone: Consistent and Controllable Image-to-Video Synthesis"
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2102.01192&start=0&max_results=10 repository have?",
    "output": "The facebookresearch/fairseq repository has 31,988 stars and 6,630 forks on GitHub."
  },
  {
    "input": "Describe the plurai-ai/intellagent repository that implements arXiv Query: search_query=&id_list=2501.13545&start=0&max_results=10.",
    "output": "A framework for comprehensive diagnosis and optimization of agents using simulated, realistic synthetic interactions"
  },
  {
    "input": "What paper has ArXiv ID 2212.10496?",
    "output": "Self-RAG: Learning to Retrieve, Generate, and Critique"
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2302.07459&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2302.07459&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2307.00109?",
    "output": "Med-PaLM 2: Towards Expert-Level Medical Question Answering"
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2303.02913&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2303.02913&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Explain the facebookresearch/CovidPrognosis implementation of arXiv Query: search_query=&id_list=2012.06009&start=0&max_results=10.",
    "output": "COVID deterioration prediction based on chest X-ray radiographs via MoCo-trained image representations"
  },
  {
    "input": "What's the implementation repository for Neural Message Passing for Quantum Chemistry?",
    "output": "The code for Neural Message Passing for Quantum Chemistry is available at https://github.com/priba/nmp_qc (priba/nmp_qc)."
  },
  {
    "input": "What topics does the arXiv Query: search_query=&id_list=2101.04909&start=0&max_results=10 implementation cover?",
    "output": "The facebookresearch/CovidPrognosis repository covers: medical-imaging, deep-learning, radiography, x-ray, pytorch, covid-19, medical-image-analysis."
  },
  {
    "input": "What topics does the The Power of Scale for Parameter-Efficient Prompt Tuning implementation cover?",
    "output": "The google-research/prompt-tuning repository covers: prompt-tuning, nlp, machine-learning, jax, flax, language-model."
  },
  {
    "input": "What category is the An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale paper in?",
    "output": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "How many stars does the PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel repository have?",
    "output": "The pytorch/pytorch repository has 95,348 stars and 26,003 forks on GitHub."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2504.19413?",
    "output": "arXiv Query: search_query=&id_list=2504.19413&start=0&max_results=10"
  },
  {
    "input": "What's the implementation repository for RetNet: Retentive Network: A Successor to Transformer?",
    "output": "The code for RetNet: Retentive Network: A Successor to Transformer is available at https://github.com/microsoft/torchscale (microsoft/torchscale)."
  },
  {
    "input": "Explain the hendrycks/test implementation of MMLU: Measuring Massive Multitask Language Understanding.",
    "output": "Measuring Massive Multitask Language Understanding | ICLR 2021"
  },
  {
    "input": "Describe the allenai/longformer repository that implements Longformer: The Long-Document Transformer.",
    "output": "Longformer: The Long-Document Transformer"
  },
  {
    "input": "When was the GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers paper published?",
    "output": "The GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers paper was published in 2022."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2501.13545?",
    "output": "arXiv Query: search_query=&id_list=2501.13545&start=0&max_results=10"
  },
  {
    "input": "What year was RetNet: Retentive Network: A Successor to Transformer released?",
    "output": "The RetNet: Retentive Network: A Successor to Transformer paper was published in 2022."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2302.12246&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2302.12246&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is the ArXiv category of Wide & Deep Learning for Recommender Systems?",
    "output": "Wide & Deep Learning for Recommender Systems is in the Information Retrieval (cs.IR) category."
  },
  {
    "input": "What is S4: Efficiently Modeling Long Sequences about?",
    "output": "S4: Efficiently Modeling Long Sequences is a 2021 paper in Machine Learning. The implementation is available at state-spaces/s4 which structured state space sequence models It is written in Jupyter Notebook. The repository has 2,783 stars, indicating high community interest."
  },
  {
    "input": "When was the TensorFlow: A System for Large-Scale Machine Learning paper published?",
    "output": "The TensorFlow: A System for Large-Scale Machine Learning paper was published in 2016."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2505.22954&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2505.22954&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "Describe the ali-vilab/composer repository that implements arXiv Query: search_query=&id_list=2302.09778&start=0&max_results=10.",
    "output": "Official implementation of \"Composer: Creative and Controllable Image Synthesis with Composable Conditions\""
  },
  {
    "input": "In what year did CLIP: Learning Transferable Visual Models From Natural Language come out?",
    "output": "The CLIP: Learning Transferable Visual Models From Natural Language paper was published in 2021."
  },
  {
    "input": "What year was Distilling the Knowledge in a Neural Network released?",
    "output": "The Distilling the Knowledge in a Neural Network paper was published in 2015."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2104.00355&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2104.00355&start=0&max_results=10 paper was published in 2021."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 1606.07792?",
    "output": "Wide & Deep Learning for Recommender Systems"
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2503.22678&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2503.22678&start=0&max_results=10 is available at https://github.com/MAXNORM8650/MedAgentSim (MAXNORM8650/MedAgentSim)."
  },
  {
    "input": "What language is the google-research/vision_transformer repository written in?",
    "output": "The ViT: An Image is Worth 16x16 Words implementation is written in Jupyter Notebook."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2006.03511&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2006.03511&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Tell me the title of ArXiv paper 2303.11366.",
    "output": "Reflexion: Language Agents with Verbal Reinforcement Learning"
  },
  {
    "input": "Summarize the LLM.int8(): 8-bit Matrix Multiplication for Transformers paper.",
    "output": "LLM.int8(): 8-bit Matrix Multiplication for Transformers is a 2022 paper in Machine Learning. The implementation is available at bitsandbytes-foundation/bitsandbytes which accessible large language models via k-bit quantization for pytorch. It is written in Python. The repository has 7,771 stars, indicating high community interest."
  },
  {
    "input": "What paper has ArXiv ID 2405.03548?",
    "output": "arXiv Query: search_query=&id_list=2405.03548&start=0&max_results=10"
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2504.20734&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2504.20734&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "Which field does StarCoder: May the source be with you! belong to?",
    "output": "StarCoder: May the source be with you! is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "When was the DreamFusion: Text-to-3D using 2D Diffusion paper published?",
    "output": "The DreamFusion: Text-to-3D using 2D Diffusion paper was published in 2022."
  },
  {
    "input": "Explain the nlpxucan/WizardLM implementation of WizardCoder: Empowering Code LLMs with Evol-Instruct.",
    "output": "LLMs build upon Evol Insturct: WizardLM, WizardCoder, WizardMath"
  },
  {
    "input": "What category is the ColBERT: Efficient Passage Search via Contextualized Late Interaction paper in?",
    "output": "ColBERT: Efficient Passage Search via Contextualized Late Interaction is in the Information Retrieval (cs.IR) category."
  },
  {
    "input": "What language is the microsoft/DeBERTa repository written in?",
    "output": "The DeBERTa: Decoding-enhanced BERT with Disentangled Attention implementation is written in Python."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2301.00704&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2301.00704&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at lucidrains/muse-maskgit-pytorch which implementation of muse: text-to-image generation via masked generative transformers, in pytorch It is written in Python. The repository has 916 stars, indicating high community interest."
  },
  {
    "input": "What does the pyg-team/pytorch_geometric repository do?",
    "output": "Graph Neural Network Library for PyTorch"
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2504.21801&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2504.21801&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What programming language is Mamba: Linear-Time Sequence Modeling with Selective State Spaces implemented in?",
    "output": "The Mamba: Linear-Time Sequence Modeling with Selective State Spaces implementation is written in Python."
  },
  {
    "input": "What language is the openai/guided-diffusion repository written in?",
    "output": "The Diffusion Models Beat GANs on Image Synthesis implementation is written in Python."
  },
  {
    "input": "Explain the LTH14/fractalgen implementation of arXiv Query: search_query=&id_list=2503.00735&start=0&max_results=10.",
    "output": "PyTorch implementation of FractalGen https://arxiv.org/abs/2502.17437"
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2502.19328&start=0&max_results=10 repository have?",
    "output": "The LTH14/fractalgen repository has 1,196 stars and 65 forks on GitHub."
  },
  {
    "input": "Tell me the title of ArXiv paper 2501.13824.",
    "output": "arXiv Query: search_query=&id_list=2501.13824&start=0&max_results=10"
  },
  {
    "input": "Tell me the title of ArXiv paper 2406.12824.",
    "output": "arXiv Query: search_query=&id_list=2406.12824&start=0&max_results=10"
  },
  {
    "input": "Explain the Finetuned Language Models Are Zero-Shot Learners paper.",
    "output": "Finetuned Language Models Are Zero-Shot Learners is a 2021 paper in Computation and Language (Natural Language Processing). The implementation is available at google-research/FLAN It is written in Python. The repository has 1,554 stars, indicating high community interest."
  },
  {
    "input": "Which field does Deep Residual Learning for Image Recognition belong to?",
    "output": "Deep Residual Learning for Image Recognition is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "Explain the amazon-science/mm-cot implementation of arXiv Query: search_query=&id_list=2301.13188&start=0&max_results=10.",
    "output": "Official implementation for \"Multimodal Chain-of-Thought Reasoning in Language Models\" (stay tuned and more will be updated)"
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2303.06865&start=0&max_results=10 repository have?",
    "output": "The FMInference/FlexLLMGen repository has 9,379 stars and 584 forks on GitHub."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2405.03520&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2405.03520&start=0&max_results=10 is a 2024 paper in Machine Learning. The implementation is available at ibm-granite/granite-code-models which granite code models: a family of open foundation models for code intelligence The repository has 1,240 stars, indicating high community interest."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2406.09403&start=0&max_results=10 implementation?",
    "output": "The lamini-ai/Lamini-Memory-Tuning repository has 275 stars and 12 forks on GitHub."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2302.00923&start=0&max_results=10 implementation?",
    "output": "The amazon-science/mm-cot repository has 3,982 stars and 333 forks on GitHub."
  },
  {
    "input": "Tell me the title of ArXiv paper 2307.01952.",
    "output": "Animate Anyone: Consistent and Controllable Image-to-Video Synthesis"
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2403.17887&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2403.17887&start=0&max_results=10 is available at https://github.com/OpenBMB/Eurus (OpenBMB/Eurus)."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2302.03011&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2302.03011&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What language is the tatp22/linformer-pytorch repository written in?",
    "output": "The Linformer: Self-Attention with Linear Complexity implementation is written in Python."
  },
  {
    "input": "Explain the ChemCrow: Augmenting large-language models with chemistry tools paper.",
    "output": "ChemCrow: Augmenting large-language models with chemistry tools is a 2023 paper in Artificial Intelligence. The implementation is available at ur-whitelab/chemcrow-public which chemcrow It is written in Python. The repository has 838 stars, indicating high community interest."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2302.14838&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2302.14838&start=0&max_results=10 is available at https://github.com/ruiqi-zhong/D5 (ruiqi-zhong/D5)."
  },
  {
    "input": "Tell me the title of ArXiv paper 2112.11446.",
    "output": "FLAN: Finetuned Language Models Are Zero-Shot Learners"
  },
  {
    "input": "What is the ArXiv category of Universal Language Model Fine-tuning for Text Classification?",
    "output": "Universal Language Model Fine-tuning for Text Classification is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What are the key topics in the langchain-ai/langchain repository?",
    "output": "The langchain-ai/langchain repository covers: ai, anthropic, gemini, langchain, llm, openai, python, agents, ai-agents, ai-agents-framework, chatgpt, enterprise, framework, generative-ai, multiagent, open-source, pydantic, rag, aiagentframework."
  },
  {
    "input": "What is the GitHub repository for Mistral 7B?",
    "output": "The code for Mistral 7B is available at https://github.com/mistralai/mistral-inference (mistralai/mistral-inference)."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2503.00735&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2503.00735&start=0&max_results=10 is available at https://github.com/LTH14/fractalgen (LTH14/fractalgen)."
  },
  {
    "input": "How many stars does the PaLM: Scaling Language Modeling with Pathways repository have?",
    "output": "The lucidrains/PaLM-pytorch repository has 826 stars and 82 forks on GitHub."
  },
  {
    "input": "How popular is the AudioPaLM: A Large Language Model for Speech Understanding implementation?",
    "output": "The lucidrains/audiolm-pytorch repository has 2,607 stars and 280 forks on GitHub."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2104.05556?",
    "output": "Multi-Task Deep Neural Networks for Natural Language Understanding"
  },
  {
    "input": "Summarize the Universal Language Model Fine-tuning for Text Classification paper.",
    "output": "Universal Language Model Fine-tuning for Text Classification is a 2018 paper in Computation and Language (Natural Language Processing). The implementation is available at fastai/fastai which the fastai deep learning library It is written in Jupyter Notebook. The repository has 27,611 stars, indicating high community interest."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2004.08492&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2004.08492&start=0&max_results=10 is a 2020 paper in Machine Learning. The implementation is available at uber/orbit which a python package for bayesian forecasting with object-oriented design and probabilistic models under the hood. It is written in Python. The repository has 2,019 stars, indicating high community interest."
  },
  {
    "input": "In what year did AlpacaEval: An Automatic Evaluator for Instruction-following come out?",
    "output": "The AlpacaEval: An Automatic Evaluator for Instruction-following paper was published in 2023."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2504.02732&start=0&max_results=10 implementation?",
    "output": "The MAXNORM8650/MedAgentSim repository has 94 stars and 11 forks on GitHub."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2107.07346&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2107.07346&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2406.10209&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2406.10209&start=0&max_results=10 is available at https://github.com/hpcaitech/Open-Sora (hpcaitech/Open-Sora)."
  },
  {
    "input": "Explain the facebookresearch/fairseq implementation of arXiv Query: search_query=&id_list=2104.00355&start=0&max_results=10.",
    "output": "Facebook AI Research Sequence-to-Sequence Toolkit written in Python."
  },
  {
    "input": "What programming language is PaLM: Scaling Language Modeling with Pathways implemented in?",
    "output": "The PaLM: Scaling Language Modeling with Pathways implementation is written in Python."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2301.01947&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2301.01947&start=0&max_results=10 is available at https://github.com/oughtinc/ice (oughtinc/ice)."
  },
  {
    "input": "What are the key topics in the ysymyth/ReAct repository?",
    "output": "The ysymyth/ReAct repository covers: decision-making, large-language-models, llm, prompting, reasoning."
  },
  {
    "input": "Explain the GPT-4 Technical Report paper.",
    "output": "GPT-4 Technical Report is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at openai/evals which evals is a framework for evaluating llms and llm systems, and an open-source registry of benchmarks. It is written in Python. The repository has 17,349 stars, indicating high community interest."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2301.02828&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2301.02828&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2504.20734&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2504.20734&start=0&max_results=10 is available at https://github.com/MoonshotAI/Kimi-Audio (MoonshotAI/Kimi-Audio)."
  },
  {
    "input": "What language is the mistralai/mistral-inference repository written in?",
    "output": "The Mistral 7B implementation is written in Jupyter Notebook."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2410.12896&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2410.12896&start=0&max_results=10 is a 2024 paper in Machine Learning. The implementation is available at ibm-granite/granite-3.0-language-models The repository has 267 stars, indicating high community interest."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2302.14233&start=0&max_results=10 repository have?",
    "output": "The ruiqi-zhong/D5 repository has 71 stars and 13 forks on GitHub."
  },
  {
    "input": "Tell me the title of ArXiv paper 2302.14233.",
    "output": "arXiv Query: search_query=&id_list=2302.14233&start=0&max_results=10"
  },
  {
    "input": "Explain the facebookresearch/fairseq implementation of Wav2Vec 2.0: A Framework for Self-Supervised Learning of Speech.",
    "output": "Facebook AI Research Sequence-to-Sequence Toolkit written in Python."
  },
  {
    "input": "Which field does Synthetic Data Generation with Large Language Models belong to?",
    "output": "Synthetic Data Generation with Large Language Models is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What year was Lookahead Optimizer: k steps forward, 1 step back released?",
    "output": "The Lookahead Optimizer: k steps forward, 1 step back paper was published in 2020."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2301.13688&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2301.13688&start=0&max_results=10 is available at https://github.com/amazon-science/mm-cot (amazon-science/mm-cot)."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2503.22678&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2503.22678&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "What is the GitHub repository for Diffusion Models Beat GANs on Image Synthesis?",
    "output": "The code for Diffusion Models Beat GANs on Image Synthesis is available at https://github.com/openai/guided-diffusion (openai/guided-diffusion)."
  },
  {
    "input": "What language is the haoliuhl/language-quantized-autoencoders repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2302.00902&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2504.16084&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2504.16084&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2503.22708&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2503.22708&start=0&max_results=10 is available at https://github.com/allenai/codescientist (allenai/codescientist)."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2301.00808&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2301.00808&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at facebookresearch/ConvNeXt-V2 which code release for convnext v2 model It is written in Python. The repository has 1,890 stars, indicating high community interest."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2301.05062&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2301.05062&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What language is the meta-llama/llama repository written in?",
    "output": "The LLaMA: Open and Efficient Foundation Language Models implementation is written in Python."
  },
  {
    "input": "What year was HuggingGPT: Solving AI Tasks with ChatGPT released?",
    "output": "The HuggingGPT: Solving AI Tasks with ChatGPT paper was published in 2023."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2004.13637&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2004.13637&start=0&max_results=10 paper was published in 2020."
  },
  {
    "input": "What topics does the Densely Connected Convolutional Networks implementation cover?",
    "output": "The liuzhuang13/DenseNet repository covers: deep-learning."
  },
  {
    "input": "Explain the ruiqi-zhong/D5 implementation of arXiv Query: search_query=&id_list=2302.14838&start=0&max_results=10.",
    "output": "The GitHub repo for Goal Driven Discovery of Distributional Differences via Language Descriptions"
  },
  {
    "input": "Explain the Contriever: Unsupervised Dense Information Retrieval paper.",
    "output": "Contriever: Unsupervised Dense Information Retrieval is a 2022 paper in Information Retrieval. The implementation is available at facebookresearch/contriever which contriever: unsupervised dense information retrieval with contrastive learning It is written in Python. The repository has 763 stars, indicating high community interest."
  },
  {
    "input": "What category is the WizardCoder: Empowering Code LLMs with Evol-Instruct paper in?",
    "output": "WizardCoder: Empowering Code LLMs with Evol-Instruct is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What is MetaGPT: Meta Programming for Multi-Agent Systems about?",
    "output": "MetaGPT: Meta Programming for Multi-Agent Systems is a 2023 paper in Artificial Intelligence. The implementation is available at FoundationAgents/MetaGPT which \ud83c\udf1f the multi-agent framework: first ai software company, towards natural language programming It is written in Python. The repository has 59,669 stars, indicating high community interest."
  },
  {
    "input": "What are the key topics in the haoliuhl/language-quantized-autoencoders repository?",
    "output": "The haoliuhl/language-quantized-autoencoders repository covers: autoencoders, bert, large-language-models, multimodal, roberta, vq, vqvae."
  },
  {
    "input": "In what year did ReAct: Synergizing Reasoning and Acting in Language Models come out?",
    "output": "The ReAct: Synergizing Reasoning and Acting in Language Models paper was published in 2022."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2405.02803?",
    "output": "arXiv Query: search_query=&id_list=2405.02803&start=0&max_results=10"
  },
  {
    "input": "What's the implementation repository for DistilBERT: a distilled version of BERT?",
    "output": "The code for DistilBERT: a distilled version of BERT is available at https://github.com/huggingface/transformers (huggingface/transformers)."
  },
  {
    "input": "Give me an overview of AlpacaEval: An Automatic Evaluator for Instruction-following.",
    "output": "AlpacaEval: An Automatic Evaluator for Instruction-following is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at tatsu-lab/alpaca_eval which an automatic evaluator for instruction-following language models. human-validated, high-quality, cheap, and fast. It is written in Jupyter Notebook. The repository has 1,907 stars, indicating high community interest."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2502.03387&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2502.03387&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "In what year did Lookahead Optimizer: k steps forward, 1 step back come out?",
    "output": "The Lookahead Optimizer: k steps forward, 1 step back paper was published in 2020."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2303.09431&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2303.09431&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "When was the Longformer: The Long-Document Transformer paper published?",
    "output": "The Longformer: The Long-Document Transformer paper was published in 2020."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2410.12896&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2410.12896&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Explain the An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale paper.",
    "output": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale is a 2020 paper in Computer Vision and Pattern Recognition. The implementation is available at google-research/vision_transformer It is written in Jupyter Notebook. The repository has 12,040 stars, indicating high community interest."
  },
  {
    "input": "Explain the facebookresearch/segment-anything implementation of Segment Anything.",
    "output": "The repository provides code for running inference with the SegmentAnything Model (SAM), links for downloading the trained model checkpoints, and example notebooks that show how to use the model."
  },
  {
    "input": "What paper has ArXiv ID 2301.10343?",
    "output": "arXiv Query: search_query=&id_list=2301.10343&start=0&max_results=10"
  },
  {
    "input": "Where can I find the code for Learning to summarize from human feedback?",
    "output": "The code for Learning to summarize from human feedback is available at https://github.com/openai/summarize-from-feedback (openai/summarize-from-feedback)."
  },
  {
    "input": "When was the Llama 2: Open Foundation and Fine-Tuned Chat Models paper published?",
    "output": "The Llama 2: Open Foundation and Fine-Tuned Chat Models paper was published in 2023."
  },
  {
    "input": "What is the ArXiv category of LLaVA-1.5: Improved Baselines with Visual Instruction Tuning?",
    "output": "LLaVA-1.5: Improved Baselines with Visual Instruction Tuning is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "In what year did MMLU: Measuring Massive Multitask Language Understanding come out?",
    "output": "The MMLU: Measuring Massive Multitask Language Understanding paper was published in 2021."
  },
  {
    "input": "What paper has ArXiv ID 2204.02311?",
    "output": "PaLM: Scaling Language Modeling with Pathways"
  },
  {
    "input": "In what year did Reformer: The Efficient Transformer come out?",
    "output": "The Reformer: The Efficient Transformer paper was published in 2020."
  },
  {
    "input": "Give me an overview of Direct Preference Optimization: Your Language Model is a Reward Model.",
    "output": "Direct Preference Optimization: Your Language Model is a Reward Model is a 2023 paper in Machine Learning. The implementation is available at eric-mitchell/direct-preference-optimization which reference implementation for dpo (direct preference optimization) It is written in Python. The repository has 2,789 stars, indicating high community interest."
  },
  {
    "input": "What paper has ArXiv ID 2004.08492?",
    "output": "arXiv Query: search_query=&id_list=2004.08492&start=0&max_results=10"
  },
  {
    "input": "What is the GitHub repository for Llama 2: Open Foundation and Fine-Tuned Chat Models?",
    "output": "The code for Llama 2: Open Foundation and Fine-Tuned Chat Models is available at https://github.com/meta-llama/llama (meta-llama/llama)."
  },
  {
    "input": "What language is the jasonwei20/eda_nlp repository written in?",
    "output": "The EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification implementation is written in Python."
  },
  {
    "input": "Summarize the DistilBERT: a distilled version of BERT paper.",
    "output": "DistilBERT: a distilled version of BERT is a 2019 paper in Computation and Language (Natural Language Processing). The implementation is available at huggingface/transformers which \ud83e\udd17 transformers: the model-definition framework for state-of-the-art machine learning models in text, vision, audio, and multimodal models, for both inference and training.  It is written in Python. The repository has 152,994 stars, indicating high community interest."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2406.07394&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2406.07394&start=0&max_results=10 is a 2024 paper in Machine Learning. The implementation is available at hpcaitech/Open-Sora which open-sora: democratizing efficient video production for all It is written in Python. The repository has 27,981 stars, indicating high community interest."
  },
  {
    "input": "Tell me the title of ArXiv paper 2203.08913.",
    "output": "RetNet: Retentive Network: A Successor to Transformer"
  },
  {
    "input": "What paper has ArXiv ID 1808.07042?",
    "output": "Know What You Don't Know: Unanswerable Questions for SQuAD"
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2410.12896&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2410.12896&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Describe the facebookresearch/fairseq repository that implements Wav2Vec 2.0: A Framework for Self-Supervised Learning of Speech.",
    "output": "Facebook AI Research Sequence-to-Sequence Toolkit written in Python."
  },
  {
    "input": "Tell me the title of ArXiv paper 2210.17323.",
    "output": "GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers"
  },
  {
    "input": "Describe the salesforce/LAVIS repository that implements InstructBLIP: Towards General-purpose Vision-Language Models.",
    "output": "LAVIS - A One-stop Library for Language-Vision Intelligence"
  },
  {
    "input": "What is the ArXiv category of PyTorch: An Imperative Style, High-Performance Deep Learning Library?",
    "output": "PyTorch: An Imperative Style, High-Performance Deep Learning Library is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is the GitHub repository for EfficientDet: Scalable and Efficient Object Detection?",
    "output": "The code for EfficientDet: Scalable and Efficient Object Detection is available at https://github.com/google/automl (google/automl)."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2504.20734&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2504.20734&start=0&max_results=10 is available at https://github.com/MoonshotAI/Kimi-Audio (MoonshotAI/Kimi-Audio)."
  },
  {
    "input": "What is the ArXiv category of Llama 2: Open Foundation and Fine-Tuned Chat Models?",
    "output": "Llama 2: Open Foundation and Fine-Tuned Chat Models is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2303.03846&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2303.03846&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "Explain the MoonshotAI/Kimi-Audio implementation of arXiv Query: search_query=&id_list=2504.21801&start=0&max_results=10.",
    "output": "Kimi-Audio, an open-source audio foundation model excelling in audio understanding, generation, and conversation"
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2009.06367&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2009.06367&start=0&max_results=10 is available at https://github.com/salesforce/GeDi (salesforce/GeDi)."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2301.11316&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2301.11316&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2504.19413&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2504.19413&start=0&max_results=10 is available at https://github.com/MoonshotAI/Kimi-Audio (MoonshotAI/Kimi-Audio)."
  },
  {
    "input": "When was the FlashAttention-2: Faster Attention with Better Parallelism paper published?",
    "output": "The FlashAttention-2: Faster Attention with Better Parallelism paper was published in 2023."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2505.23006&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2505.23006&start=0&max_results=10 is available at https://github.com/ruixin31/Spurious_Rewards (ruixin31/Spurious_Rewards)."
  },
  {
    "input": "What language is the plurai-ai/intellagent repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2501.13824&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "In what year did Enriching Word Vectors with Subword Information come out?",
    "output": "The Enriching Word Vectors with Subword Information paper was published in 2016."
  },
  {
    "input": "Explain the MAXNORM8650/MedAgentSim implementation of arXiv Query: search_query=&id_list=2504.02732&start=0&max_results=10.",
    "output": "MedAgentSim: Self-Evolving Multi-Agent Simulations for Realistic Clinical Interactions, MICCAI 2025 (oral and early accepted)"
  },
  {
    "input": "Explain the google-research/google-research implementation of Med-PaLM 2: Towards Expert-Level Medical Question Answering.",
    "output": "Google Research"
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2004.08492&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2004.08492&start=0&max_results=10 is a 2020 paper in Machine Learning. The implementation is available at uber/orbit which a python package for bayesian forecasting with object-oriented design and probabilistic models under the hood. It is written in Python. The repository has 2,019 stars, indicating high community interest."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 1904.12848?",
    "output": "EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification"
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2303.01469&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2303.01469&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What programming language is RoFormer: Enhanced Transformer with Rotary Position Embedding implemented in?",
    "output": "The RoFormer: Enhanced Transformer with Rotary Position Embedding implementation is written in Python."
  },
  {
    "input": "What language is the facebookresearch/fairseq repository written in?",
    "output": "The XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale implementation is written in Python."
  },
  {
    "input": "Explain the PetarV-/GAT implementation of Graph Attention Networks.",
    "output": "Graph Attention Networks (https://arxiv.org/abs/1710.10903)"
  },
  {
    "input": "What is Question and Answer Test-Train Overlap in Open-Domain QA Datasets about?",
    "output": "Question and Answer Test-Train Overlap in Open-Domain QA Datasets is a 2020 paper in Computation and Language (Natural Language Processing). The implementation is available at google-research-datasets/natural-questions which natural questions (nq) contains real user questions issued to google search, and answers found from wikipedia by annotators. nq is designed for the training and evaluation of automatic question answering systems. It is written in Python. The repository has 1,074 stars, indicating high community interest."
  },
  {
    "input": "What is the ArXiv category of Synthetic Data Generation with Large Language Models?",
    "output": "Synthetic Data Generation with Large Language Models is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What language is the salesforce/BLIP repository written in?",
    "output": "The BLIP: Bootstrapping Language-Image Pre-training implementation is written in Jupyter Notebook."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2302.14838&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2302.14838&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2303.06865&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2303.06865&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "Where can I find the code for JAX: Composable transformations of Python+NumPy programs?",
    "output": "The code for JAX: Composable transformations of Python+NumPy programs is available at https://github.com/jax-ml/jax (jax-ml/jax)."
  },
  {
    "input": "What is Feature Pyramid Networks for Object Detection about?",
    "output": "Feature Pyramid Networks for Object Detection is a 2016 paper in Computer Vision and Pattern Recognition. The implementation is available at facebookresearch/Detectron which fair's research platform for object detection research, implementing popular algorithms like mask r-cnn and retinanet. It is written in Python. The repository has 26,396 stars, indicating high community interest."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2406.12824&start=0&max_results=10 repository have?",
    "output": "The hpcaitech/Open-Sora repository has 27,981 stars and 2,781 forks on GitHub."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2504.19413&start=0&max_results=10 implementation?",
    "output": "The MoonshotAI/Kimi-Audio repository has 4,367 stars and 316 forks on GitHub."
  },
  {
    "input": "How many stars does the ZeRO: Memory Optimizations Toward Training Trillion Parameter Models repository have?",
    "output": "The deepspeedai/DeepSpeed repository has 40,796 stars and 4,646 forks on GitHub."
  },
  {
    "input": "What's the implementation repository for WizardCoder: Empowering Code LLMs with Evol-Instruct?",
    "output": "The code for WizardCoder: Empowering Code LLMs with Evol-Instruct is available at https://github.com/nlpxucan/WizardLM (nlpxucan/WizardLM)."
  },
  {
    "input": "What language is the lisa-groundhog/GroundHog repository written in?",
    "output": "The Neural Machine Translation by Jointly Learning to Align and Translate implementation is written in Python."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2303.02913&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2303.02913&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at Shark-NLP/OpenICL which openicl is an open-source framework to facilitate research, development, and prototyping of in-context learning. It is written in Python. The repository has 579 stars, indicating high community interest."
  },
  {
    "input": "Tell me the title of ArXiv paper 2503.22678.",
    "output": "arXiv Query: search_query=&id_list=2503.22678&start=0&max_results=10"
  },
  {
    "input": "Tell me the title of ArXiv paper 2206.01861.",
    "output": "LLM.int8(): 8-bit Matrix Multiplication for Transformers"
  },
  {
    "input": "What is the GitHub repository for BERT: Pre-training of Deep Bidirectional Transformers?",
    "output": "The code for BERT: Pre-training of Deep Bidirectional Transformers is available at https://github.com/google-research/bert (google-research/bert)."
  },
  {
    "input": "Describe the alphadl/lookahead.pytorch repository that implements Lookahead Optimizer: k steps forward, 1 step back.",
    "output": "lookahead optimizer (Lookahead Optimizer: k steps forward, 1 step back) for pytorch "
  },
  {
    "input": "What's the implementation repository for Reformer: The Efficient Transformer?",
    "output": "The code for Reformer: The Efficient Transformer is available at https://github.com/google/trax (google/trax)."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2301.00774&start=0&max_results=10 repository have?",
    "output": "The facebookresearch/ConvNeXt-V2 repository has 1,890 stars and 155 forks on GitHub."
  },
  {
    "input": "What language is the ruiqi-zhong/D5 repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2302.14838&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2301.01947&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2301.01947&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What's the implementation repository for Wide & Deep Learning for Recommender Systems?",
    "output": "The code for Wide & Deep Learning for Recommender Systems is available at https://github.com/tensorflow/models (tensorflow/models)."
  },
  {
    "input": "When was the Faster R-CNN: Towards Real-Time Object Detection paper published?",
    "output": "The Faster R-CNN: Towards Real-Time Object Detection paper was published in 2015."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2503.20201&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2503.20201&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "Which field does TensorFlow: A System for Large-Scale Machine Learning belong to?",
    "output": "TensorFlow: A System for Large-Scale Machine Learning is in the Distributed, Parallel, and Cluster Computing (cs.DC) category."
  },
  {
    "input": "What programming language is PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel implemented in?",
    "output": "The PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel implementation is written in Python."
  },
  {
    "input": "What is the GitHub repository for Faster R-CNN: Towards Real-Time Object Detection?",
    "output": "The code for Faster R-CNN: Towards Real-Time Object Detection is available at https://github.com/rbgirshick/py-faster-rcnn (rbgirshick/py-faster-rcnn)."
  },
  {
    "input": "How many stars does the Llama 2: Open Foundation and Fine-Tuned Chat Models repository have?",
    "output": "The meta-llama/llama repository has 58,946 stars and 9,817 forks on GitHub."
  },
  {
    "input": "How popular is the Analyzing the Structure of Attention in a Transformer Language Model implementation?",
    "output": "The clarkkev/attention-analysis repository has 469 stars and 82 forks on GitHub."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2301.01751&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2301.01751&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at oughtinc/ice which interactive composition explorer: a debugger for compositional language model programs It is written in Python. The repository has 563 stars, indicating high community interest."
  },
  {
    "input": "What is the ArXiv category of LLaVA: Large Language and Vision Assistant?",
    "output": "LLaVA: Large Language and Vision Assistant is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "Tell me the title of ArXiv paper 2305.07185.",
    "output": "LIMA: Less Is More for Alignment"
  },
  {
    "input": "When was the ViT: An Image is Worth 16x16 Words paper published?",
    "output": "The ViT: An Image is Worth 16x16 Words paper was published in 2021."
  },
  {
    "input": "When was the Question and Answer Test-Train Overlap in Open-Domain QA Datasets paper published?",
    "output": "The Question and Answer Test-Train Overlap in Open-Domain QA Datasets paper was published in 2020."
  },
  {
    "input": "How many stars does the Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena repository have?",
    "output": "The lm-sys/FastChat repository has 39,271 stars and 4,772 forks on GitHub."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2503.23513&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2503.23513&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at MAXNORM8650/MedAgentSim which medagentsim: self-evolving multi-agent simulations for realistic clinical interactions, miccai 2025 (oral and early accepted) It is written in Python. The repository has 94 stars, indicating high community interest."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2302.10866&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2302.10866&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What language is the facebookresearch/CovidPrognosis repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2012.06009&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2302.08575?",
    "output": "SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities"
  },
  {
    "input": "What is Diffusion Models Beat GANs on Image Synthesis about?",
    "output": "Diffusion Models Beat GANs on Image Synthesis is a 2021 paper in Computer Vision and Pattern Recognition. The implementation is available at openai/guided-diffusion It is written in Python. The repository has 7,161 stars, indicating high community interest."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2312.02120&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2312.02120&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at ContextualAI/HALOs which a library with extensible implementations of dpo, kto, ppo, orpo, and other human-aware loss functions (halos). It is written in Python. The repository has 893 stars, indicating high community interest."
  },
  {
    "input": "Explain the ContextualAI/HALOs implementation of arXiv Query: search_query=&id_list=2312.02783&start=0&max_results=10.",
    "output": "A library with extensible implementations of DPO, KTO, PPO, ORPO, and other human-aware loss functions (HALOs)."
  },
  {
    "input": "What is the GitHub repository for Zephyr: Direct Distillation of LM Alignment?",
    "output": "The code for Zephyr: Direct Distillation of LM Alignment is available at https://github.com/huggingface/alignment-handbook (huggingface/alignment-handbook)."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2301.04856&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2301.04856&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "Where can I find the code for InPars: Data Augmentation for Information Retrieval?",
    "output": "The code for InPars: Data Augmentation for Information Retrieval is available at https://github.com/zetaalphavector/InPars (zetaalphavector/InPars)."
  },
  {
    "input": "When was the Dream to Control: Learning Behaviors by Latent Imagination paper published?",
    "output": "The Dream to Control: Learning Behaviors by Latent Imagination paper was published in 2019."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2303.10512?",
    "output": "LLaMA-Adapter: Efficient Fine-tuning of Language Models"
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2301.10343?",
    "output": "arXiv Query: search_query=&id_list=2301.10343&start=0&max_results=10"
  },
  {
    "input": "What's the implementation repository for FLAN: Finetuned Language Models Are Zero-Shot Learners?",
    "output": "The code for FLAN: Finetuned Language Models Are Zero-Shot Learners is available at https://github.com/google-research/FLAN (google-research/FLAN)."
  },
  {
    "input": "Where can I find the code for Training language models to follow instructions with human feedback?",
    "output": "The code for Training language models to follow instructions with human feedback is available at https://github.com/openai/following-instructions-human-feedback (openai/following-instructions-human-feedback)."
  },
  {
    "input": "Which field does Self-RAG: Learning to Retrieve, Generate, and Critique belong to?",
    "output": "Self-RAG: Learning to Retrieve, Generate, and Critique is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Tell me the title of ArXiv paper 2502.01061.",
    "output": "arXiv Query: search_query=&id_list=2502.01061&start=0&max_results=10"
  },
  {
    "input": "What are the key topics in the microsoft/unilm repository?",
    "output": "The microsoft/unilm repository covers: nlp, pre-trained-model, unilm, minilm, layoutlm, layoutxlm, beit, document-ai, trocr, beit-3, foundation-models, xlm-e, deepnet, llm, multimodal, mllm, kosmos, kosmos-1, textdiffuser, bitnet."
  },
  {
    "input": "In what year did LIMA: Less Is More for Alignment come out?",
    "output": "The LIMA: Less Is More for Alignment paper was published in 2023."
  },
  {
    "input": "Tell me the title of ArXiv paper 2301.00774.",
    "output": "arXiv Query: search_query=&id_list=2301.00774&start=0&max_results=10"
  },
  {
    "input": "What language is the facebookresearch/fairseq repository written in?",
    "output": "The RoBERTa: A Robustly Optimized BERT Pretraining Approach implementation is written in Python."
  },
  {
    "input": "What year was ControlNet: Adding Conditional Control to Text-to-Image Diffusion released?",
    "output": "The ControlNet: Adding Conditional Control to Text-to-Image Diffusion paper was published in 2023."
  },
  {
    "input": "What category is the SQuAD: 100,000+ Questions for Machine Comprehension of Text paper in?",
    "output": "SQuAD: 100,000+ Questions for Machine Comprehension of Text is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "How popular is the Denoising Diffusion Implicit Models implementation?",
    "output": "The ermongroup/ddim repository has 1,743 stars and 227 forks on GitHub."
  },
  {
    "input": "Explain the SmoothQuant: Accurate and Efficient Post-Training Quantization paper.",
    "output": "SmoothQuant: Accurate and Efficient Post-Training Quantization is a 2022 paper in Machine Learning. The implementation is available at mit-han-lab/smoothquant which [icml 2023] smoothquant: accurate and efficient post-training quantization for large language models It is written in Python. The repository has 1,561 stars, indicating high community interest."
  },
  {
    "input": "What language is the linkedin/gdmix repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2007.16122&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2302.03917&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2302.03917&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at mit-han-lab/offsite-tuning which offsite-tuning: transfer learning without full model It is written in Python. The repository has 382 stars, indicating high community interest."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2302.00923&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2302.00923&start=0&max_results=10 is available at https://github.com/amazon-science/mm-cot (amazon-science/mm-cot)."
  },
  {
    "input": "Explain the Codex: Evaluating Large Language Models Trained on Code paper.",
    "output": "Codex: Evaluating Large Language Models Trained on Code is a 2021 paper in Machine Learning. The implementation is available at openai/human-eval which code for the paper \"evaluating large language models trained on code\" It is written in Python. The repository has 3,030 stars, indicating high community interest."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2503.00735&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2503.00735&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Explain the The Flan Collection: Designing Data and Methods for Effective Instruction Tuning paper.",
    "output": "The Flan Collection: Designing Data and Methods for Effective Instruction Tuning is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at google-research/FLAN It is written in Python. The repository has 1,554 stars, indicating high community interest."
  },
  {
    "input": "What year was Feature Pyramid Networks for Object Detection released?",
    "output": "The Feature Pyramid Networks for Object Detection paper was published in 2016."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2502.12115&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2502.12115&start=0&max_results=10 is available at https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero (Open-Reasoner-Zero/Open-Reasoner-Zero)."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2301.11305&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2301.11305&start=0&max_results=10 is available at https://github.com/autonomousvision/stylegan-t (autonomousvision/stylegan-t)."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2406.12824&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2406.12824&start=0&max_results=10 is available at https://github.com/hpcaitech/Open-Sora (hpcaitech/Open-Sora)."
  },
  {
    "input": "Tell me the title of ArXiv paper 2005.00687.",
    "output": "Deep Graph Library: A Graph-Centric, Highly-Performant Package"
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2004.08492&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2004.08492&start=0&max_results=10 paper was published in 2020."
  },
  {
    "input": "Explain the princeton-nlp/SimCSE implementation of SimCSE: Simple Contrastive Learning of Sentence Embeddings.",
    "output": "[EMNLP 2021] SimCSE: Simple Contrastive Learning of Sentence Embeddings https://arxiv.org/abs/2104.08821"
  },
  {
    "input": "What language is the GAIR-NLP/LIMO repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2502.01061&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2406.07394&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2406.07394&start=0&max_results=10 is available at https://github.com/hpcaitech/Open-Sora (hpcaitech/Open-Sora)."
  },
  {
    "input": "Which field does DeBERTa: Decoding-enhanced BERT with Disentangled Attention belong to?",
    "output": "DeBERTa: Decoding-enhanced BERT with Disentangled Attention is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What paper has ArXiv ID 2504.16084?",
    "output": "arXiv Query: search_query=&id_list=2504.16084&start=0&max_results=10"
  },
  {
    "input": "Summarize the Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena paper.",
    "output": "Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at lm-sys/FastChat which an open platform for training, serving, and evaluating large language models. release repo for vicuna and chatbot arena. It is written in Python. The repository has 39,271 stars, indicating high community interest."
  },
  {
    "input": "How popular is the Sequence to Sequence Learning with Neural Networks implementation?",
    "output": "The farizrahman4u/seq2seq repository has 3,173 stars and 838 forks on GitHub."
  },
  {
    "input": "Explain the uber/orbit implementation of arXiv Query: search_query=&id_list=2004.08492&start=0&max_results=10.",
    "output": "A Python package for Bayesian forecasting with object-oriented design and probabilistic models under the hood."
  },
  {
    "input": "Which field does Grounding DINO: Marrying DINO with Grounded Pre-Training belong to?",
    "output": "Grounding DINO: Marrying DINO with Grounded Pre-Training is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "Summarize the LLaMA-Adapter: Efficient Fine-tuning of Language Models paper.",
    "output": "LLaMA-Adapter: Efficient Fine-tuning of Language Models is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at OpenGVLab/LLaMA-Adapter which [iclr 2024] fine-tuning llama to follow instructions within 1 hour and 1.2m parameters It is written in Python. The repository has 5,923 stars, indicating high community interest."
  },
  {
    "input": "Tell me the title of ArXiv paper 2312.02120.",
    "output": "arXiv Query: search_query=&id_list=2312.02120&start=0&max_results=10"
  },
  {
    "input": "Where can I find the code for Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks?",
    "output": "The code for Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks is available at https://github.com/huggingface/sentence-transformers (huggingface/sentence-transformers)."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2301.04856&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2301.04856&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "Describe the pytorch/pytorch repository that implements Decoupled Weight Decay Regularization.",
    "output": "Tensors and Dynamic neural networks in Python with strong GPU acceleration"
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2504.01848&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2504.01848&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "Describe the openai/human-eval repository that implements Codex: Evaluating Large Language Models Trained on Code.",
    "output": "Code for the paper \"Evaluating Large Language Models Trained on Code\""
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2009.06367&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2009.06367&start=0&max_results=10 is a 2020 paper in Machine Learning. The implementation is available at salesforce/GeDi which gedi: generative discriminator guided sequence generation It is written in Python. The repository has 209 stars, indicating high community interest."
  },
  {
    "input": "What year was Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context released?",
    "output": "The Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context paper was published in 2019."
  },
  {
    "input": "How many stars does the SimCSE: Simple Contrastive Learning of Sentence Embeddings repository have?",
    "output": "The princeton-nlp/SimCSE repository has 3,615 stars and 531 forks on GitHub."
  },
  {
    "input": "What does the NVlabs/prismer repository do?",
    "output": "The implementation of \"Prismer: A Vision-Language Model with Multi-Task Experts\"."
  },
  {
    "input": "What's the implementation repository for T5: Exploring the Limits of Transfer Learning?",
    "output": "The code for T5: Exploring the Limits of Transfer Learning is available at https://github.com/google-research/text-to-text-transfer-transformer (google-research/text-to-text-transfer-transformer)."
  },
  {
    "input": "When was the Zephyr: Direct Distillation of LM Alignment paper published?",
    "output": "The Zephyr: Direct Distillation of LM Alignment paper was published in 2023."
  },
  {
    "input": "What paper has ArXiv ID 2006.04768?",
    "output": "DeBERTa: Decoding-enhanced BERT with Disentangled Attention"
  },
  {
    "input": "Tell me the title of ArXiv paper 1703.04691.",
    "output": "Neural Message Passing for Quantum Chemistry"
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2502.19328&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2502.19328&start=0&max_results=10 is available at https://github.com/LTH14/fractalgen (LTH14/fractalgen)."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2303.02913&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2303.02913&start=0&max_results=10 is available at https://github.com/Shark-NLP/OpenICL (Shark-NLP/OpenICL)."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2301.12652&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2301.12652&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2403.17887&start=0&max_results=10 repository have?",
    "output": "The OpenBMB/Eurus repository has 320 stars and 14 forks on GitHub."
  },
  {
    "input": "When was the SmoothQuant: Accurate and Efficient Post-Training Quantization paper published?",
    "output": "The SmoothQuant: Accurate and Efficient Post-Training Quantization paper was published in 2022."
  },
  {
    "input": "What language is the ContextualAI/HALOs repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2312.02120&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What's the implementation repository for ColBERT: Efficient Passage Search via Contextualized Late Interaction?",
    "output": "The code for ColBERT: Efficient Passage Search via Contextualized Late Interaction is available at https://github.com/stanford-futuredata/ColBERT (stanford-futuredata/ColBERT)."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2302.03917&start=0&max_results=10 repository have?",
    "output": "The mit-han-lab/offsite-tuning repository has 382 stars and 39 forks on GitHub."
  },
  {
    "input": "What category is the DistilBERT: a distilled version of BERT paper in?",
    "output": "DistilBERT: a distilled version of BERT is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Explain the ColBERT: Efficient Passage Search via Contextualized Late Interaction paper.",
    "output": "ColBERT: Efficient Passage Search via Contextualized Late Interaction is a 2021 paper in Information Retrieval. The implementation is available at stanford-futuredata/ColBERT which colbert: state-of-the-art neural search (sigir'20, tacl'21, neurips'21, naacl'22, cikm'22, acl'23, emnlp'23) It is written in Python. The repository has 3,707 stars, indicating high community interest."
  },
  {
    "input": "What category is the SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding paper in?",
    "output": "SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What is the ArXiv category of PyTorch Geometric: Library for Geometric Deep Learning?",
    "output": "PyTorch Geometric: Library for Geometric Deep Learning is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Explain the ZeRO: Memory Optimizations Toward Training Trillion Parameter Models paper.",
    "output": "ZeRO: Memory Optimizations Toward Training Trillion Parameter Models is a 2019 paper in Machine Learning. The implementation is available at deepspeedai/DeepSpeed which deepspeed is a deep learning optimization library that makes distributed training and inference easy, efficient, and effective. It is written in Python. The repository has 40,796 stars, indicating high community interest."
  },
  {
    "input": "Describe the ruiqi-zhong/D5 repository that implements arXiv Query: search_query=&id_list=2302.14233&start=0&max_results=10.",
    "output": "The GitHub repo for Goal Driven Discovery of Distributional Differences via Language Descriptions"
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2301.00808&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2301.00808&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at facebookresearch/ConvNeXt-V2 which code release for convnext v2 model It is written in Python. The repository has 1,890 stars, indicating high community interest."
  },
  {
    "input": "Describe the hojonathanho/diffusion repository that implements Denoising Diffusion Probabilistic Models.",
    "output": "Denoising Diffusion Probabilistic Models"
  },
  {
    "input": "What topics does the arXiv Query: search_query=&id_list=2504.02732&start=0&max_results=10 implementation cover?",
    "output": "The MAXNORM8650/MedAgentSim repository covers: multi-agent-simulations."
  },
  {
    "input": "What paper has ArXiv ID 2406.10209?",
    "output": "arXiv Query: search_query=&id_list=2406.10209&start=0&max_results=10"
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2301.13688?",
    "output": "arXiv Query: search_query=&id_list=2301.13688&start=0&max_results=10"
  },
  {
    "input": "What does the tensorflow/tensor2tensor repository do?",
    "output": "Library of deep learning models and datasets designed to make deep learning more accessible and accelerate ML research."
  },
  {
    "input": "Explain the lisa-groundhog/GroundHog implementation of Neural Machine Translation by Jointly Learning to Align and Translate.",
    "output": "Library for implementing RNNs with Theano"
  },
  {
    "input": "What is the GitHub repository for ViT: An Image is Worth 16x16 Words?",
    "output": "The code for ViT: An Image is Worth 16x16 Words is available at https://github.com/google-research/vision_transformer (google-research/vision_transformer)."
  },
  {
    "input": "How many stars does the Question and Answer Test-Train Overlap in Open-Domain QA Datasets repository have?",
    "output": "The google-research-datasets/natural-questions repository has 1,074 stars and 158 forks on GitHub."
  },
  {
    "input": "What does the google-research/google-research repository do?",
    "output": "Google Research"
  },
  {
    "input": "How many stars does the LLaVA: Large Language and Vision Assistant repository have?",
    "output": "The haotian-liu/LLaVA repository has 24,034 stars and 2,665 forks on GitHub."
  },
  {
    "input": "Explain the allenai/longformer implementation of Longformer: The Long-Document Transformer.",
    "output": "Longformer: The Long-Document Transformer"
  },
  {
    "input": "Which field does CLIP: Learning Transferable Visual Models From Natural Language belong to?",
    "output": "CLIP: Learning Transferable Visual Models From Natural Language is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "What's the implementation repository for LLaMA-Adapter: Efficient Fine-tuning of Language Models?",
    "output": "The code for LLaMA-Adapter: Efficient Fine-tuning of Language Models is available at https://github.com/OpenGVLab/LLaMA-Adapter (OpenGVLab/LLaMA-Adapter)."
  },
  {
    "input": "Describe the lamini-ai/Lamini-Memory-Tuning repository that implements arXiv Query: search_query=&id_list=2406.09403&start=0&max_results=10.",
    "output": "Banishing LLM Hallucinations Requires Rethinking Generalization"
  },
  {
    "input": "Where can I find the code for Competition-Level Code Generation with AlphaCode?",
    "output": "The code for Competition-Level Code Generation with AlphaCode is available at https://github.com/google-deepmind/code_contests (google-deepmind/code_contests)."
  },
  {
    "input": "Tell me the title of ArXiv paper 2211.10438.",
    "output": "SmoothQuant: Accurate and Efficient Post-Training Quantization"
  },
  {
    "input": "What does the machrisaa/tensorflow-vgg repository do?",
    "output": "VGG19 and VGG16 on Tensorflow"
  },
  {
    "input": "Where can I find the code for Densely Connected Convolutional Networks?",
    "output": "The code for Densely Connected Convolutional Networks is available at https://github.com/liuzhuang13/DenseNet (liuzhuang13/DenseNet)."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2504.16084&start=0&max_results=10 implementation?",
    "output": "The Crista23/goal_directedness_llms repository has 12 stars and 2 forks on GitHub."
  },
  {
    "input": "When was the Feature Pyramid Networks for Object Detection paper published?",
    "output": "The Feature Pyramid Networks for Object Detection paper was published in 2016."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2303.04226&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2303.04226&start=0&max_results=10 is available at https://github.com/HazyResearch/safari (HazyResearch/safari)."
  },
  {
    "input": "Explain the AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning paper.",
    "output": "AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning is a 2022 paper in Machine Learning. The implementation is available at QingruZhang/AdaLoRA which adalora: adaptive budget allocation for parameter-efficient fine-tuning (iclr 2023).  It is written in Python. The repository has 361 stars, indicating high community interest."
  },
  {
    "input": "Describe the huawei-noah/Pretrained-Language-Model repository that implements TinyBERT: Distilling BERT for Natural Language Understanding.",
    "output": "Pretrained language model and its related optimization techniques developed by Huawei Noah's Ark Lab."
  },
  {
    "input": "Explain the MiniGPT-4: Enhancing Vision-Language Understanding paper.",
    "output": "MiniGPT-4: Enhancing Vision-Language Understanding is a 2023 paper in Computer Vision and Pattern Recognition. The implementation is available at Vision-CAIR/MiniGPT-4 which open-sourced codes for minigpt-4 and minigpt-v2 (https://minigpt-4.github.io, https://minigpt-v2.github.io/) It is written in Python. The repository has 25,756 stars, indicating high community interest."
  },
  {
    "input": "Describe the facebookresearch/faiss repository that implements Billion-scale similarity search with GPUs.",
    "output": "A library for efficient similarity search and clustering of dense vectors."
  },
  {
    "input": "Describe the ZhuiyiTechnology/roformer repository that implements RoFormer: Enhanced Transformer with Rotary Position Embedding.",
    "output": "Rotary Transformer"
  },
  {
    "input": "How many stars does the Codex: Evaluating Large Language Models Trained on Code repository have?",
    "output": "The openai/human-eval repository has 3,030 stars and 422 forks on GitHub."
  },
  {
    "input": "What programming language is FlashAttention: Fast and Memory-Efficient Exact Attention implemented in?",
    "output": "The FlashAttention: Fast and Memory-Efficient Exact Attention implementation is written in Python."
  },
  {
    "input": "Describe the ShishirPatil/gorilla repository that implements Gorilla: Large Language Model Connected with Massive APIs.",
    "output": "Gorilla: Training and Evaluating LLMs for Function Calls (Tool Calls)"
  },
  {
    "input": "Give me an overview of Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning.",
    "output": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning is a 2018 paper in Machine Learning. The implementation is available at rail-berkeley/softlearning which softlearning is a reinforcement learning framework for training maximum entropy policies in continuous domains. includes the official implementation of the soft actor-critic algorithm. It is written in Python. The repository has 1,370 stars, indicating high community interest."
  },
  {
    "input": "When was the Linformer: Self-Attention with Linear Complexity paper published?",
    "output": "The Linformer: Self-Attention with Linear Complexity paper was published in 2021."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2303.03378&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2303.03378&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at NVlabs/prismer which the implementation of \"prismer: a vision-language model with multi-task experts\". It is written in Python. The repository has 1,307 stars, indicating high community interest."
  },
  {
    "input": "Give me an overview of SmoothQuant: Accurate and Efficient Post-Training Quantization.",
    "output": "SmoothQuant: Accurate and Efficient Post-Training Quantization is a 2022 paper in Machine Learning. The implementation is available at mit-han-lab/smoothquant which [icml 2023] smoothquant: accurate and efficient post-training quantization for large language models It is written in Python. The repository has 1,561 stars, indicating high community interest."
  },
  {
    "input": "What year was PaLM: Scaling Language Modeling with Pathways released?",
    "output": "The PaLM: Scaling Language Modeling with Pathways paper was published in 2022."
  },
  {
    "input": "Describe the Shark-NLP/OpenICL repository that implements arXiv Query: search_query=&id_list=2303.02913&start=0&max_results=10.",
    "output": "OpenICL is an open-source framework to facilitate research, development, and prototyping of in-context learning."
  },
  {
    "input": "What does the uber/orbit repository do?",
    "output": "A Python package for Bayesian forecasting with object-oriented design and probabilistic models under the hood."
  },
  {
    "input": "Tell me the title of ArXiv paper 2104.05556.",
    "output": "Multi-Task Deep Neural Networks for Natural Language Understanding"
  },
  {
    "input": "What topics does the ReAct: Synergizing Reasoning and Acting in Language Models implementation cover?",
    "output": "The ysymyth/ReAct repository covers: decision-making, large-language-models, llm, prompting, reasoning."
  },
  {
    "input": "Where can I find the code for Faster R-CNN: Towards Real-Time Object Detection?",
    "output": "The code for Faster R-CNN: Towards Real-Time Object Detection is available at https://github.com/rbgirshick/py-faster-rcnn (rbgirshick/py-faster-rcnn)."
  },
  {
    "input": "What language is the openai/frontier-evals repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2504.01848&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What programming language is ReAct: Synergizing Reasoning and Acting in Language Models implemented in?",
    "output": "The ReAct: Synergizing Reasoning and Acting in Language Models implementation is written in Jupyter Notebook."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2405.04324&start=0&max_results=10 implementation?",
    "output": "The ibm-granite/granite-code-models repository has 1,240 stars and 88 forks on GitHub."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2503.22678&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2503.22678&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at MAXNORM8650/MedAgentSim which medagentsim: self-evolving multi-agent simulations for realistic clinical interactions, miccai 2025 (oral and early accepted) It is written in Python. The repository has 94 stars, indicating high community interest."
  },
  {
    "input": "When was the Semi-Supervised Classification with Graph Convolutional Networks paper published?",
    "output": "The Semi-Supervised Classification with Graph Convolutional Networks paper was published in 2016."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2007.16122&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2007.16122&start=0&max_results=10 is a 2020 paper in Machine Learning. The implementation is available at linkedin/gdmix which a deep ranking personalization framework It is written in Python. The repository has 133 stars, indicating high community interest."
  },
  {
    "input": "What is the ArXiv category of DreamBooth: Fine Tuning Text-to-Image Diffusion Models?",
    "output": "DreamBooth: Fine Tuning Text-to-Image Diffusion Models is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "Describe the siddk/voltron-robotics repository that implements arXiv Query: search_query=&id_list=2302.12766&start=0&max_results=10.",
    "output": "Voltron: Language-Driven Representation Learning for Robotics"
  },
  {
    "input": "How popular is the Generating Wikipedia by Summarizing Long Sequences implementation?",
    "output": "The tensorflow/tensor2tensor repository has 16,760 stars and 3,702 forks on GitHub."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2301.00303&start=0&max_results=10 implementation?",
    "output": "The facebookresearch/ConvNeXt-V2 repository has 1,890 stars and 155 forks on GitHub."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2301.12652?",
    "output": "arXiv Query: search_query=&id_list=2301.12652&start=0&max_results=10"
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2101.04909&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2101.04909&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is the GitHub repository for JAX: Composable transformations of Python+NumPy programs?",
    "output": "The code for JAX: Composable transformations of Python+NumPy programs is available at https://github.com/jax-ml/jax (jax-ml/jax)."
  },
  {
    "input": "What are the key topics in the NVlabs/prismer repository?",
    "output": "The NVlabs/prismer repository covers: image-captioning, language-model, multi-modal-learning, multi-task-learning, vision-language-model, vision-and-language, vqa."
  },
  {
    "input": "What topics does the Synthetic Data Generation with Large Language Models implementation cover?",
    "output": "The google-research/google-research repository covers: machine-learning, ai, research."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2406.07394&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2406.07394&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2006.16779&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2006.16779&start=0&max_results=10 paper was published in 2020."
  },
  {
    "input": "Tell me the title of ArXiv paper 2006.03511.",
    "output": "arXiv Query: search_query=&id_list=2006.03511&start=0&max_results=10"
  },
  {
    "input": "What topics does the Horovod: fast and easy distributed deep learning in TensorFlow implementation cover?",
    "output": "The horovod/horovod repository covers: tensorflow, uber, machine-learning, machinelearning, mpi, baidu, deep-learning, deeplearning, keras, pytorch, mxnet, spark, ray."
  },
  {
    "input": "What does the huawei-noah/Pretrained-Language-Model repository do?",
    "output": "Pretrained language model and its related optimization techniques developed by Huawei Noah's Ark Lab."
  },
  {
    "input": "What are the key topics in the ContextualAI/HALOs repository?",
    "output": "The ContextualAI/HALOs repository covers: alignment, dpo, halos, kto, ppo, rlhf."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2405.02803&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2405.02803&start=0&max_results=10 is available at https://github.com/ibm-granite/granite-code-models (ibm-granite/granite-code-models)."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2405.03548&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2405.03548&start=0&max_results=10 is a 2024 paper in Machine Learning. The implementation is available at ibm-granite/granite-code-models which granite code models: a family of open foundation models for code intelligence The repository has 1,240 stars, indicating high community interest."
  },
  {
    "input": "What year was S4: Efficiently Modeling Long Sequences released?",
    "output": "The S4: Efficiently Modeling Long Sequences paper was published in 2021."
  },
  {
    "input": "What is the ArXiv category of RetNet: Retentive Network: A Successor to Transformer?",
    "output": "RetNet: Retentive Network: A Successor to Transformer is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is BLIP: Bootstrapping Language-Image Pre-training about?",
    "output": "BLIP: Bootstrapping Language-Image Pre-training is a 2022 paper in Computer Vision and Pattern Recognition. The implementation is available at salesforce/BLIP which pytorch code for blip: bootstrapping language-image pre-training for unified vision-language understanding and generation   It is written in Jupyter Notebook. The repository has 5,583 stars, indicating high community interest."
  },
  {
    "input": "Summarize the Inductive Representation Learning on Large Graphs paper.",
    "output": "Inductive Representation Learning on Large Graphs is a 2017 paper in cs.SI. The implementation is available at williamleif/GraphSAGE which representation learning on large graphs using stochastic graph convolutions. It is written in Python. The repository has 3,630 stars, indicating high community interest."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2301.04856&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2301.04856&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at frankxu2004/knnlm-why which repo for icml23 \"why do nearest neighbor language models work?\" It is written in Python. The repository has 59 stars, indicating high community interest."
  },
  {
    "input": "What paper has ArXiv ID 1905.05583?",
    "output": "Billion-scale similarity search with GPUs"
  },
  {
    "input": "Give me an overview of Text Embeddings by Weakly-Supervised Contrastive Pre-training.",
    "output": "Text Embeddings by Weakly-Supervised Contrastive Pre-training is a 2022 paper in Computation and Language (Natural Language Processing). The implementation is available at microsoft/unilm which large-scale self-supervised pre-training across tasks, languages, and modalities It is written in Python. The repository has 21,848 stars, indicating high community interest."
  },
  {
    "input": "What programming language is SmoothQuant: Accurate and Efficient Post-Training Quantization implemented in?",
    "output": "The SmoothQuant: Accurate and Efficient Post-Training Quantization implementation is written in Python."
  },
  {
    "input": "When was the YOLOX: Exceeding YOLO Series in 2021 paper published?",
    "output": "The YOLOX: Exceeding YOLO Series in 2021 paper was published in 2018."
  },
  {
    "input": "How popular is the Contriever: Unsupervised Dense Information Retrieval implementation?",
    "output": "The facebookresearch/contriever repository has 763 stars and 67 forks on GitHub."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2503.23513&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2503.23513&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at MAXNORM8650/MedAgentSim which medagentsim: self-evolving multi-agent simulations for realistic clinical interactions, miccai 2025 (oral and early accepted) It is written in Python. The repository has 94 stars, indicating high community interest."
  },
  {
    "input": "What's the implementation repository for XLNet: Generalized Autoregressive Pretraining?",
    "output": "The code for XLNet: Generalized Autoregressive Pretraining is available at https://github.com/zihangdai/xlnet (zihangdai/xlnet)."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2301.04856&start=0&max_results=10 implementation?",
    "output": "The frankxu2004/knnlm-why repository has 59 stars and 3 forks on GitHub."
  },
  {
    "input": "What year was Masked Autoencoders Are Scalable Vision Learners released?",
    "output": "The Masked Autoencoders Are Scalable Vision Learners paper was published in 2022."
  },
  {
    "input": "What paper has ArXiv ID 2303.04226?",
    "output": "arXiv Query: search_query=&id_list=2303.04226&start=0&max_results=10"
  },
  {
    "input": "What does the FoundationAgents/MetaGPT repository do?",
    "output": "\ud83c\udf1f The Multi-Agent Framework: First AI Software Company, Towards Natural Language Programming"
  },
  {
    "input": "What's the implementation repository for Diffusion Models Beat GANs on Image Synthesis?",
    "output": "The code for Diffusion Models Beat GANs on Image Synthesis is available at https://github.com/openai/guided-diffusion (openai/guided-diffusion)."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2303.09431&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2303.09431&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Which field does InPars: Data Augmentation for Information Retrieval belong to?",
    "output": "InPars: Data Augmentation for Information Retrieval is in the Information Retrieval (cs.IR) category."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2302.04870&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2302.04870&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2104.00355&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2104.00355&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2301.09515&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2301.09515&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2406.07394&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2406.07394&start=0&max_results=10 is a 2024 paper in Machine Learning. The implementation is available at hpcaitech/Open-Sora which open-sora: democratizing efficient video production for all It is written in Python. The repository has 27,981 stars, indicating high community interest."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2502.12115&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2502.12115&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "What category is the Med-PaLM 2: Towards Expert-Level Medical Question Answering paper in?",
    "output": "Med-PaLM 2: Towards Expert-Level Medical Question Answering is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2302.12766&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2302.12766&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2102.01192&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2102.01192&start=0&max_results=10 is a 2021 paper in Machine Learning. The implementation is available at facebookresearch/fairseq which facebook ai research sequence-to-sequence toolkit written in python. It is written in Python. The repository has 31,988 stars, indicating high community interest."
  },
  {
    "input": "Give me an overview of BLIP-2: Bootstrapping Language-Image Pre-training.",
    "output": "BLIP-2: Bootstrapping Language-Image Pre-training is a 2023 paper in Computer Vision and Pattern Recognition. The implementation is available at salesforce/LAVIS which lavis - a one-stop library for language-vision intelligence It is written in Jupyter Notebook. The repository has 11,040 stars, indicating high community interest."
  },
  {
    "input": "What are the key topics in the lucidrains/muse-maskgit-pytorch repository?",
    "output": "The lucidrains/muse-maskgit-pytorch repository covers: artificial-intelligence, deep-learning, text-to-image, attention-mechanisms, transformers."
  },
  {
    "input": "Explain the facebookresearch/CovidPrognosis implementation of arXiv Query: search_query=&id_list=2101.04909&start=0&max_results=10.",
    "output": "COVID deterioration prediction based on chest X-ray radiographs via MoCo-trained image representations"
  },
  {
    "input": "Describe the haoliuhl/language-quantized-autoencoders repository that implements arXiv Query: search_query=&id_list=2302.00902&start=0&max_results=10.",
    "output": "Language Quantized AutoEncoders"
  },
  {
    "input": "What topics does the EfficientDet: Scalable and Efficient Object Detection implementation cover?",
    "output": "The google/automl repository covers: automl, efficientdet, object-detection, efficientnet, efficientnetv2."
  },
  {
    "input": "Tell me the title of ArXiv paper 2302.07842.",
    "output": "The Flan Collection: Designing Data and Methods for Effective Instruction Tuning"
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2301.00808&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2301.00808&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "Tell me the title of ArXiv paper 2406.10209.",
    "output": "arXiv Query: search_query=&id_list=2406.10209&start=0&max_results=10"
  },
  {
    "input": "What does the openai/human-eval repository do?",
    "output": "Code for the paper \"Evaluating Large Language Models Trained on Code\""
  },
  {
    "input": "What's the implementation repository for Billion-scale similarity search with GPUs?",
    "output": "The code for Billion-scale similarity search with GPUs is available at https://github.com/facebookresearch/faiss (facebookresearch/faiss)."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2006.03511&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2006.03511&start=0&max_results=10 paper was published in 2020."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2406.12824&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2406.12824&start=0&max_results=10 paper was published in 2024."
  },
  {
    "input": "In what year did AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning come out?",
    "output": "The AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning paper was published in 2022."
  },
  {
    "input": "What are the key topics in the bitsandbytes-foundation/bitsandbytes repository?",
    "output": "The bitsandbytes-foundation/bitsandbytes repository covers: llm, machine-learning, pytorch, qlora, quantization."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2406.12824&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2406.12824&start=0&max_results=10 is a 2024 paper in Machine Learning. The implementation is available at hpcaitech/Open-Sora which open-sora: democratizing efficient video production for all It is written in Python. The repository has 27,981 stars, indicating high community interest."
  },
  {
    "input": "What topics does the arXiv Query: search_query=&id_list=2302.08242&start=0&max_results=10 implementation cover?",
    "output": "The haoliuhl/language-quantized-autoencoders repository covers: autoencoders, bert, large-language-models, multimodal, roberta, vq, vqvae."
  },
  {
    "input": "What programming language is InstructBLIP: Towards General-purpose Vision-Language Models implemented in?",
    "output": "The InstructBLIP: Towards General-purpose Vision-Language Models implementation is written in Jupyter Notebook."
  },
  {
    "input": "What does the JohnNay/llm-lobbyist repository do?",
    "output": "Code for the paper: \"Large Language Models as Corporate Lobbyists\" (2023)."
  },
  {
    "input": "What's the implementation repository for SQuAD: 100,000+ Questions for Machine Comprehension of Text?",
    "output": "The code for SQuAD: 100,000+ Questions for Machine Comprehension of Text is available at https://github.com/rajpurkar/SQuAD-explorer (rajpurkar/SQuAD-explorer)."
  },
  {
    "input": "Tell me the title of ArXiv paper 2302.14838.",
    "output": "arXiv Query: search_query=&id_list=2302.14838&start=0&max_results=10"
  },
  {
    "input": "What year was Contriever: Unsupervised Dense Information Retrieval released?",
    "output": "The Contriever: Unsupervised Dense Information Retrieval paper was published in 2022."
  },
  {
    "input": "Where can I find the code for MetaGPT: Meta Programming for Multi-Agent Systems?",
    "output": "The code for MetaGPT: Meta Programming for Multi-Agent Systems is available at https://github.com/FoundationAgents/MetaGPT (FoundationAgents/MetaGPT)."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2502.19328&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2502.19328&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at LTH14/fractalgen which pytorch implementation of fractalgen https://arxiv.org/abs/2502.17437 It is written in Python. The repository has 1,196 stars, indicating high community interest."
  },
  {
    "input": "Tell me the title of ArXiv paper 2303.10512.",
    "output": "LLaMA-Adapter: Efficient Fine-tuning of Language Models"
  },
  {
    "input": "How many stars does the AudioPaLM: A Large Language Model for Speech Understanding repository have?",
    "output": "The lucidrains/audiolm-pytorch repository has 2,607 stars and 280 forks on GitHub."
  },
  {
    "input": "Summarize the Competition-Level Code Generation with AlphaCode paper.",
    "output": "Competition-Level Code Generation with AlphaCode is a 2022 paper in Machine Learning. The implementation is available at google-deepmind/code_contests It is written in C++. The repository has 2,177 stars, indicating high community interest."
  },
  {
    "input": "How many stars does the Synthetic Data Generation with Large Language Models repository have?",
    "output": "The google-research/google-research repository has 36,800 stars and 8,255 forks on GitHub."
  },
  {
    "input": "What language is the Dao-AILab/flash-attention repository written in?",
    "output": "The FlashAttention-2: Faster Attention with Better Parallelism implementation is written in Python."
  },
  {
    "input": "In what year did Zephyr: Direct Distillation of LM Alignment come out?",
    "output": "The Zephyr: Direct Distillation of LM Alignment paper was published in 2023."
  },
  {
    "input": "What paper has ArXiv ID 2303.01469?",
    "output": "arXiv Query: search_query=&id_list=2303.01469&start=0&max_results=10"
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2502.14815&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2502.14815&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at Open-Reasoner-Zero/Open-Reasoner-Zero which official repo for open-reasoner-zero It is written in Python. The repository has 2,068 stars, indicating high community interest."
  },
  {
    "input": "Explain the deepspeedai/DeepSpeed implementation of ZeRO: Memory Optimizations Toward Training Trillion Parameter Models.",
    "output": "DeepSpeed is a deep learning optimization library that makes distributed training and inference easy, efficient, and effective."
  },
  {
    "input": "What category is the Direct Preference Optimization: Your Language Model is a Reward Model paper in?",
    "output": "Direct Preference Optimization: Your Language Model is a Reward Model is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "How popular is the RoFormer: Enhanced Transformer with Rotary Position Embedding implementation?",
    "output": "The ZhuiyiTechnology/roformer repository has 1,051 stars and 59 forks on GitHub."
  },
  {
    "input": "What language is the facebookresearch/TransCoder repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2004.13637&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2101.04909&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2101.04909&start=0&max_results=10 is a 2021 paper in Machine Learning. The implementation is available at facebookresearch/CovidPrognosis which covid deterioration prediction based on chest x-ray radiographs via moco-trained image representations It is written in Python. The repository has 158 stars, indicating high community interest."
  },
  {
    "input": "What is Code Llama: Open Foundation Models for Code about?",
    "output": "Code Llama: Open Foundation Models for Code is a 2023 paper in Machine Learning. The implementation is available at meta-llama/codellama which inference code for codellama models It is written in Python. The repository has 16,365 stars, indicating high community interest."
  },
  {
    "input": "What language is the clarkkev/attention-analysis repository written in?",
    "output": "The BERT Rediscovers the Classical NLP Pipeline implementation is written in Jupyter Notebook."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2405.03520&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2405.03520&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What programming language is Grounding DINO: Marrying DINO with Grounded Pre-Training implemented in?",
    "output": "The Grounding DINO: Marrying DINO with Grounded Pre-Training implementation is written in Python."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2404.03592&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2404.03592&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2406.07394&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2406.07394&start=0&max_results=10 paper was published in 2024."
  },
  {
    "input": "What does the fastai/fastai repository do?",
    "output": "The fastai deep learning library"
  },
  {
    "input": "What is DeBERTa: Decoding-enhanced BERT with Disentangled Attention about?",
    "output": "DeBERTa: Decoding-enhanced BERT with Disentangled Attention is a 2020 paper in Computation and Language (Natural Language Processing). The implementation is available at microsoft/DeBERTa which the implementation of deberta It is written in Python. The repository has 2,169 stars, indicating high community interest."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2303.01469&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2303.01469&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at ruiqi-zhong/D5 which the github repo for goal driven discovery of distributional differences via language descriptions It is written in Python. The repository has 71 stars, indicating high community interest."
  },
  {
    "input": "Describe the state-spaces/mamba repository that implements Mamba: Linear-Time Sequence Modeling with Selective State Spaces.",
    "output": "Mamba SSM architecture"
  },
  {
    "input": "What language is the openai/human-eval repository written in?",
    "output": "The Codex: Evaluating Large Language Models Trained on Code implementation is written in Python."
  },
  {
    "input": "What is MiniGPT-4: Enhancing Vision-Language Understanding about?",
    "output": "MiniGPT-4: Enhancing Vision-Language Understanding is a 2023 paper in Computer Vision and Pattern Recognition. The implementation is available at Vision-CAIR/MiniGPT-4 which open-sourced codes for minigpt-4 and minigpt-v2 (https://minigpt-4.github.io, https://minigpt-v2.github.io/) It is written in Python. The repository has 25,756 stars, indicating high community interest."
  },
  {
    "input": "Give me an overview of Finetuned Language Models Are Zero-Shot Learners.",
    "output": "Finetuned Language Models Are Zero-Shot Learners is a 2021 paper in Computation and Language (Natural Language Processing). The implementation is available at google-research/FLAN It is written in Python. The repository has 1,554 stars, indicating high community interest."
  },
  {
    "input": "When was the Wide & Deep Learning for Recommender Systems paper published?",
    "output": "The Wide & Deep Learning for Recommender Systems paper was published in 2016."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2205.05638?",
    "output": "AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning"
  },
  {
    "input": "What programming language is Faster R-CNN: Towards Real-Time Object Detection implemented in?",
    "output": "The Faster R-CNN: Towards Real-Time Object Detection implementation is written in Python."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2302.08242?",
    "output": "arXiv Query: search_query=&id_list=2302.08242&start=0&max_results=10"
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2410.18050&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2410.18050&start=0&max_results=10 paper was published in 2024."
  },
  {
    "input": "What paper has ArXiv ID 2504.01848?",
    "output": "arXiv Query: search_query=&id_list=2504.01848&start=0&max_results=10"
  },
  {
    "input": "What paper has ArXiv ID 2103.14899?",
    "output": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows"
  },
  {
    "input": "Tell me the title of ArXiv paper 2504.19413.",
    "output": "arXiv Query: search_query=&id_list=2504.19413&start=0&max_results=10"
  },
  {
    "input": "How popular is the FlashAttention-2: Faster Attention with Better Parallelism implementation?",
    "output": "The Dao-AILab/flash-attention repository has 20,719 stars and 2,160 forks on GitHub."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2302.14233&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2302.14233&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Explain the NVlabs/prismer implementation of arXiv Query: search_query=&id_list=2303.02506&start=0&max_results=10.",
    "output": "The implementation of \"Prismer: A Vision-Language Model with Multi-Task Experts\"."
  },
  {
    "input": "What is the ArXiv category of Time-LLM: Time Series Forecasting by Reprogramming Large Language Models?",
    "output": "Time-LLM: Time Series Forecasting by Reprogramming Large Language Models is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2303.01469&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2303.01469&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What is the GitHub repository for Sequence to Sequence Learning with Neural Networks?",
    "output": "The code for Sequence to Sequence Learning with Neural Networks is available at https://github.com/farizrahman4u/seq2seq (farizrahman4u/seq2seq)."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2301.05062&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2301.05062&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at google-deepmind/tracr It is written in Python. The repository has 548 stars, indicating high community interest."
  },
  {
    "input": "What does the google/trax repository do?",
    "output": "Trax \u2014 Deep Learning with Clear Code and Speed"
  },
  {
    "input": "How popular is the Proximal Policy Optimization Algorithms implementation?",
    "output": "The openai/baselines repository has 16,566 stars and 4,951 forks on GitHub."
  },
  {
    "input": "What programming language is Flamingo: A Visual Language Model for Few-Shot Learning implemented in?",
    "output": "The Flamingo: A Visual Language Model for Few-Shot Learning implementation is written in Python."
  },
  {
    "input": "What are the key topics in the rajpurkar/SQuAD-explorer repository?",
    "output": "The rajpurkar/SQuAD-explorer repository covers: dataset, visual-analysis, leaderboard."
  },
  {
    "input": "Describe the haotian-liu/LLaVA repository that implements LLaVA-1.5: Improved Baselines with Visual Instruction Tuning.",
    "output": "[NeurIPS'23 Oral] Visual Instruction Tuning (LLaVA) built towards GPT-4V level capabilities and beyond."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2302.00923&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2302.00923&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2006.03511&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2006.03511&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is RoFormer: Enhanced Transformer with Rotary Position Embedding about?",
    "output": "RoFormer: Enhanced Transformer with Rotary Position Embedding is a 2021 paper in Computation and Language (Natural Language Processing). The implementation is available at ZhuiyiTechnology/roformer which rotary transformer It is written in Python. The repository has 1,051 stars, indicating high community interest."
  },
  {
    "input": "What is the GitHub repository for TinyBERT: Distilling BERT for Natural Language Understanding?",
    "output": "The code for TinyBERT: Distilling BERT for Natural Language Understanding is available at https://github.com/huawei-noah/Pretrained-Language-Model (huawei-noah/Pretrained-Language-Model)."
  },
  {
    "input": "Summarize the ViT: An Image is Worth 16x16 Words paper.",
    "output": "ViT: An Image is Worth 16x16 Words is a 2021 paper in Computer Vision and Pattern Recognition. The implementation is available at google-research/vision_transformer It is written in Jupyter Notebook. The repository has 12,040 stars, indicating high community interest."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2504.01848&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2504.01848&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at openai/frontier-evals which openai frontier evals It is written in Python. The repository has 949 stars, indicating high community interest."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2302.12246&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2302.12246&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "Where can I find the code for Text Embeddings by Weakly-Supervised Contrastive Pre-training?",
    "output": "The code for Text Embeddings by Weakly-Supervised Contrastive Pre-training is available at https://github.com/microsoft/unilm (microsoft/unilm)."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2103.14899?",
    "output": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows"
  },
  {
    "input": "What does the jacopotagliabue/you-dont-need-a-bigger-boat repository do?",
    "output": "An end-to-end implementation of intent prediction with Metaflow and other cool tools"
  },
  {
    "input": "Where can I find the code for T5: Exploring the Limits of Transfer Learning?",
    "output": "The code for T5: Exploring the Limits of Transfer Learning is available at https://github.com/google-research/text-to-text-transfer-transformer (google-research/text-to-text-transfer-transformer)."
  },
  {
    "input": "What year was An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale released?",
    "output": "The An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale paper was published in 2020."
  },
  {
    "input": "Where can I find the code for PaLM: Scaling Language Modeling with Pathways?",
    "output": "The code for PaLM: Scaling Language Modeling with Pathways is available at https://github.com/lucidrains/PaLM-pytorch (lucidrains/PaLM-pytorch)."
  },
  {
    "input": "Describe the Open-Reasoner-Zero/Open-Reasoner-Zero repository that implements arXiv Query: search_query=&id_list=2502.12115&start=0&max_results=10.",
    "output": "Official Repo for Open-Reasoner-Zero"
  },
  {
    "input": "How many stars does the BLIP-2: Bootstrapping Language-Image Pre-training repository have?",
    "output": "The salesforce/LAVIS repository has 11,040 stars and 1,086 forks on GitHub."
  },
  {
    "input": "What does the salesforce/LAVIS repository do?",
    "output": "LAVIS - A One-stop Library for Language-Vision Intelligence"
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2301.00808?",
    "output": "arXiv Query: search_query=&id_list=2301.00808&start=0&max_results=10"
  },
  {
    "input": "What topics does the arXiv Query: search_query=&id_list=2303.02913&start=0&max_results=10 implementation cover?",
    "output": "The Shark-NLP/OpenICL repository covers: in-context-learning, nlp, language-model."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2502.14815&start=0&max_results=10 implementation?",
    "output": "The Open-Reasoner-Zero/Open-Reasoner-Zero repository has 2,068 stars and 119 forks on GitHub."
  },
  {
    "input": "What paper has ArXiv ID 2305.06161?",
    "output": "StarCoder: May the source be with you!"
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2301.05062&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2301.05062&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at google-deepmind/tracr It is written in Python. The repository has 548 stars, indicating high community interest."
  },
  {
    "input": "Which field does T5: Exploring the Limits of Transfer Learning belong to?",
    "output": "T5: Exploring the Limits of Transfer Learning is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What language is the lm-sys/FastChat repository written in?",
    "output": "The Vicuna: An Open-Source Chatbot implementation is written in Python."
  },
  {
    "input": "What programming language is Dream to Control: Learning Behaviors by Latent Imagination implemented in?",
    "output": "The Dream to Control: Learning Behaviors by Latent Imagination implementation is written in Python."
  },
  {
    "input": "How many stars does the SmoothQuant: Accurate and Efficient Post-Training Quantization repository have?",
    "output": "The mit-han-lab/smoothquant repository has 1,561 stars and 190 forks on GitHub."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2007.16122&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2007.16122&start=0&max_results=10 is a 2020 paper in Machine Learning. The implementation is available at linkedin/gdmix which a deep ranking personalization framework It is written in Python. The repository has 133 stars, indicating high community interest."
  },
  {
    "input": "Tell me the title of ArXiv paper 2209.00796.",
    "output": "DreamFusion: Text-to-3D using 2D Diffusion"
  },
  {
    "input": "When was the RAG: Retrieval-Augmented Generation paper published?",
    "output": "The RAG: Retrieval-Augmented Generation paper was published in 2020."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2503.20201?",
    "output": "arXiv Query: search_query=&id_list=2503.20201&start=0&max_results=10"
  },
  {
    "input": "In what year did SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding come out?",
    "output": "The SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding paper was published in 2019."
  },
  {
    "input": "What are the key topics in the jax-ml/jax repository?",
    "output": "The jax-ml/jax repository covers: jax."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2302.12246&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2302.12246&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What paper has ArXiv ID 2303.17580?",
    "output": "HuggingGPT: Solving AI Tasks with ChatGPT"
  },
  {
    "input": "What is The Flan Collection: Designing Data and Methods for Effective Instruction Tuning about?",
    "output": "The Flan Collection: Designing Data and Methods for Effective Instruction Tuning is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at google-research/FLAN It is written in Python. The repository has 1,554 stars, indicating high community interest."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2303.06865&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2303.06865&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2302.05206&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2302.05206&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "How popular is the AWQ: Activation-aware Weight Quantization for LLM Compression implementation?",
    "output": "The mit-han-lab/llm-awq repository has 3,357 stars and 283 forks on GitHub."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2406.07394?",
    "output": "arXiv Query: search_query=&id_list=2406.07394&start=0&max_results=10"
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2302.12246&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2302.12246&start=0&max_results=10 is available at https://github.com/shizhediao/active-prompt (shizhediao/active-prompt)."
  },
  {
    "input": "What is the ArXiv category of Grounding DINO: Marrying DINO with Grounded Pre-Training?",
    "output": "Grounding DINO: Marrying DINO with Grounded Pre-Training is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "Explain the google-research/albert implementation of ALBERT: A Lite BERT.",
    "output": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"
  },
  {
    "input": "What category is the Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning paper in?",
    "output": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Where can I find the code for Attention Is All You Need?",
    "output": "The code for Attention Is All You Need is available at https://github.com/tensorflow/tensor2tensor (tensorflow/tensor2tensor)."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2107.07346&start=0&max_results=10 implementation?",
    "output": "The jacopotagliabue/you-dont-need-a-bigger-boat repository has 873 stars and 66 forks on GitHub."
  },
  {
    "input": "What paper has ArXiv ID 2406.09308?",
    "output": "arXiv Query: search_query=&id_list=2406.09308&start=0&max_results=10"
  },
  {
    "input": "Which field does Prefix-Tuning: Optimizing Continuous Prompts for Generation belong to?",
    "output": "Prefix-Tuning: Optimizing Continuous Prompts for Generation is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2303.02506&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2303.02506&start=0&max_results=10 is available at https://github.com/NVlabs/prismer (NVlabs/prismer)."
  },
  {
    "input": "What does the MoonshotAI/Kimi-Audio repository do?",
    "output": "Kimi-Audio, an open-source audio foundation model excelling in audio understanding, generation, and conversation"
  },
  {
    "input": "Give me an overview of SQuAD: 100,000+ Questions for Machine Comprehension of Text.",
    "output": "SQuAD: 100,000+ Questions for Machine Comprehension of Text is a 2016 paper in Computation and Language (Natural Language Processing). The implementation is available at rajpurkar/SQuAD-explorer which visually explore the stanford question answering dataset It is written in JavaScript. The repository has 570 stars, indicating high community interest."
  },
  {
    "input": "In what year did Self-RAG: Learning to Retrieve, Generate, and Critique come out?",
    "output": "The Self-RAG: Learning to Retrieve, Generate, and Critique paper was published in 2022."
  },
  {
    "input": "What language is the microsoft/LoRA repository written in?",
    "output": "The LoRA: Low-Rank Adaptation of Large Language Models implementation is written in Python."
  },
  {
    "input": "What language is the eric-mitchell/direct-preference-optimization repository written in?",
    "output": "The Direct Preference Optimization: Your Language Model is a Reward Model implementation is written in Python."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2303.04671&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2303.04671&start=0&max_results=10 is available at https://github.com/chenfei-wu/TaskMatrix (chenfei-wu/TaskMatrix)."
  },
  {
    "input": "Which field does The Power of Scale for Parameter-Efficient Prompt Tuning belong to?",
    "output": "The Power of Scale for Parameter-Efficient Prompt Tuning is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What language is the rajpurkar/SQuAD-explorer repository written in?",
    "output": "The SQuAD: 100,000+ Questions for Machine Comprehension of Text implementation is written in JavaScript."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2301.10343&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2301.10343&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What is the GitHub repository for Analyzing the Structure of Attention in a Transformer Language Model?",
    "output": "The code for Analyzing the Structure of Attention in a Transformer Language Model is available at https://github.com/clarkkev/attention-analysis (clarkkev/attention-analysis)."
  },
  {
    "input": "What language is the meta-llama/llama repository written in?",
    "output": "The Llama 2: Open Foundation and Fine-Tuned Chat Models implementation is written in Python."
  },
  {
    "input": "Explain the MetaGPT: Meta Programming for Multi-Agent Systems paper.",
    "output": "MetaGPT: Meta Programming for Multi-Agent Systems is a 2023 paper in Artificial Intelligence. The implementation is available at FoundationAgents/MetaGPT which \ud83c\udf1f the multi-agent framework: first ai software company, towards natural language programming It is written in Python. The repository has 59,669 stars, indicating high community interest."
  },
  {
    "input": "What programming language is MetaGPT: Meta Programming for Multi-Agent Systems implemented in?",
    "output": "The MetaGPT: Meta Programming for Multi-Agent Systems implementation is written in Python."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2404.03592&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2404.03592&start=0&max_results=10 is a 2024 paper in Machine Learning. The implementation is available at OpenBMB/Eurus It is written in Python. The repository has 320 stars, indicating high community interest."
  },
  {
    "input": "Where can I find the code for HuggingGPT: Solving AI Tasks with ChatGPT?",
    "output": "The code for HuggingGPT: Solving AI Tasks with ChatGPT is available at https://github.com/microsoft/JARVIS (microsoft/JARVIS)."
  },
  {
    "input": "What category is the MetaGPT: Meta Programming for Multi-Agent Systems paper in?",
    "output": "MetaGPT: Meta Programming for Multi-Agent Systems is in the Artificial Intelligence (cs.AI) category."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2301.04104&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2301.04104&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "In what year did HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction come out?",
    "output": "The HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction paper was published in 2021."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2301.00704&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2301.00704&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What does the salesforce/GeDi repository do?",
    "output": "GeDi: Generative Discriminator Guided Sequence Generation"
  },
  {
    "input": "What language is the danijar/dreamer repository written in?",
    "output": "The Dream to Control: Learning Behaviors by Latent Imagination implementation is written in Python."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2501.13824&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2501.13824&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What paper has ArXiv ID 2406.06326?",
    "output": "arXiv Query: search_query=&id_list=2406.06326&start=0&max_results=10"
  },
  {
    "input": "Explain the Self-RAG: Learning to Retrieve, Generate, and Critique paper.",
    "output": "Self-RAG: Learning to Retrieve, Generate, and Critique is a 2022 paper in Computation and Language (Natural Language Processing). The implementation is available at AkariAsai/self-rag which this includes the original implementation of self-rag: learning to retrieve, generate and critique through self-reflection by akari asai, zeqiu wu, yizhong wang, avirup sil, and hannaneh hajishirzi. It is written in Python. The repository has 2,252 stars, indicating high community interest."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2004.08492&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2004.08492&start=0&max_results=10 is available at https://github.com/uber/orbit (uber/orbit)."
  },
  {
    "input": "In what year did ControlNet: Adding Conditional Control to Text-to-Image Diffusion come out?",
    "output": "The ControlNet: Adding Conditional Control to Text-to-Image Diffusion paper was published in 2023."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2505.23006&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2505.23006&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "What year was Graph Attention Networks released?",
    "output": "The Graph Attention Networks paper was published in 2017."
  },
  {
    "input": "Explain the huggingface/sentence-transformers implementation of Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks.",
    "output": "State-of-the-Art Text Embeddings"
  },
  {
    "input": "What is the ArXiv category of DeBERTa: Decoding-enhanced BERT with Disentangled Attention?",
    "output": "DeBERTa: Decoding-enhanced BERT with Disentangled Attention is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "When was the Know What You Don't Know: Unanswerable Questions for SQuAD paper published?",
    "output": "The Know What You Don't Know: Unanswerable Questions for SQuAD paper was published in 2018."
  },
  {
    "input": "What are the key topics in the microsoft/DeBERTa repository?",
    "output": "The microsoft/DeBERTa repository covers: bert, deeplearning, representation-learning, roberta, language-model, natural-language-understanding, self-attention, transformer-encoder."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2502.19328&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2502.19328&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2503.01141&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2503.01141&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2006.03511&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2006.03511&start=0&max_results=10 is a 2020 paper in Machine Learning. The implementation is available at facebookresearch/TransCoder which public release of the transcoder research project https://arxiv.org/pdf/2006.03511.pdf It is written in Python. The repository has 1,722 stars, indicating high community interest."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2504.21801&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2504.21801&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at MoonshotAI/Kimi-Audio which kimi-audio, an open-source audio foundation model excelling in audio understanding, generation, and conversation It is written in Python. The repository has 4,367 stars, indicating high community interest."
  },
  {
    "input": "Give me an overview of GPT-3: Language Models are Few-Shot Learners.",
    "output": "GPT-3: Language Models are Few-Shot Learners is a 2020 paper in Computation and Language (Natural Language Processing). The implementation is available at openai/gpt-3 which gpt-3: language models are few-shot learners The repository has 15,784 stars, indicating high community interest."
  },
  {
    "input": "What is the GitHub repository for Very Deep Convolutional Networks for Large-Scale Image Recognition?",
    "output": "The code for Very Deep Convolutional Networks for Large-Scale Image Recognition is available at https://github.com/machrisaa/tensorflow-vgg (machrisaa/tensorflow-vgg)."
  },
  {
    "input": "What are the key topics in the lucidrains/flamingo-pytorch repository?",
    "output": "The lucidrains/flamingo-pytorch repository covers: artificial-intelligence, attention-mechanism, deep-learning, transformers, visual-question-answering."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2406.09308?",
    "output": "arXiv Query: search_query=&id_list=2406.09308&start=0&max_results=10"
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2501.13824&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2501.13824&start=0&max_results=10 is available at https://github.com/plurai-ai/intellagent (plurai-ai/intellagent)."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2301.04104&start=0&max_results=10 implementation?",
    "output": "The google-deepmind/tracr repository has 548 stars and 49 forks on GitHub."
  },
  {
    "input": "What programming language is Whisper: Robust Speech Recognition via Large-Scale Weak Supervision implemented in?",
    "output": "The Whisper: Robust Speech Recognition via Large-Scale Weak Supervision implementation is written in Python."
  },
  {
    "input": "Where can I find the code for ALBERT: A Lite BERT?",
    "output": "The code for ALBERT: A Lite BERT is available at https://github.com/google-research/albert (google-research/albert)."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2012.06009&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2012.06009&start=0&max_results=10 is available at https://github.com/facebookresearch/CovidPrognosis (facebookresearch/CovidPrognosis)."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2203.05556?",
    "output": "FlashAttention: Fast and Memory-Efficient Exact Attention"
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2302.03917&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2302.03917&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "Give me an overview of SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding.",
    "output": "SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding is a 2019 paper in Computation and Language (Natural Language Processing). The implementation is available at nyu-mll/jiant which jiant is an nlp toolkit It is written in Python. The repository has 1,672 stars, indicating high community interest."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 1905.05583?",
    "output": "Billion-scale similarity search with GPUs"
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2302.14233&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2302.14233&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "How popular is the Billion-scale similarity search with GPUs implementation?",
    "output": "The facebookresearch/faiss repository has 38,143 stars and 4,132 forks on GitHub."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2504.21801&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2504.21801&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "In what year did Competition-Level Code Generation with AlphaCode come out?",
    "output": "The Competition-Level Code Generation with AlphaCode paper was published in 2022."
  },
  {
    "input": "What year was Multi-Task Deep Neural Networks for Natural Language Understanding released?",
    "output": "The Multi-Task Deep Neural Networks for Natural Language Understanding paper was published in 2021."
  },
  {
    "input": "What year was The Power of Scale for Parameter-Efficient Prompt Tuning released?",
    "output": "The The Power of Scale for Parameter-Efficient Prompt Tuning paper was published in 2021."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2503.23513&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2503.23513&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at MAXNORM8650/MedAgentSim which medagentsim: self-evolving multi-agent simulations for realistic clinical interactions, miccai 2025 (oral and early accepted) It is written in Python. The repository has 94 stars, indicating high community interest."
  },
  {
    "input": "What is the GitHub repository for Reformer: The Efficient Transformer?",
    "output": "The code for Reformer: The Efficient Transformer is available at https://github.com/google/trax (google/trax)."
  },
  {
    "input": "Where can I find the code for GPT-4 Technical Report?",
    "output": "The code for GPT-4 Technical Report is available at https://github.com/openai/evals (openai/evals)."
  },
  {
    "input": "What does the google-research/google-research repository do?",
    "output": "Google Research"
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2009.06367&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2009.06367&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "In what year did ChemCrow: Augmenting large-language models with chemistry tools come out?",
    "output": "The ChemCrow: Augmenting large-language models with chemistry tools paper was published in 2023."
  },
  {
    "input": "What is Densely Connected Convolutional Networks about?",
    "output": "Densely Connected Convolutional Networks is a 2016 paper in Computer Vision and Pattern Recognition. The implementation is available at liuzhuang13/DenseNet which densely connected convolutional networks, in cvpr 2017 (best paper award). It is written in Lua. The repository has 4,843 stars, indicating high community interest."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2502.12115&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2502.12115&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at Open-Reasoner-Zero/Open-Reasoner-Zero which official repo for open-reasoner-zero It is written in Python. The repository has 2,068 stars, indicating high community interest."
  },
  {
    "input": "What topics does the BLIP-2: Bootstrapping Language-Image Pre-training implementation cover?",
    "output": "The salesforce/LAVIS repository covers: deep-learning, deep-learning-library, image-captioning, salesforce, vision-and-language, vision-framework, vision-language-pretraining, vision-language-transformer, visual-question-anwsering, multimodal-datasets, multimodal-deep-learning."
  },
  {
    "input": "What topics does the JAX: Composable transformations of Python+NumPy programs implementation cover?",
    "output": "The jax-ml/jax repository covers: jax."
  },
  {
    "input": "Explain the LoRA: Low-Rank Adaptation of Large Language Models paper.",
    "output": "LoRA: Low-Rank Adaptation of Large Language Models is a 2021 paper in Computation and Language (Natural Language Processing). The implementation is available at microsoft/LoRA which code for loralib, an implementation of \"lora: low-rank adaptation of large language models\" It is written in Python. The repository has 12,979 stars, indicating high community interest."
  },
  {
    "input": "How many stars does the MMLU: Measuring Massive Multitask Language Understanding repository have?",
    "output": "The hendrycks/test repository has 1,523 stars and 113 forks on GitHub."
  },
  {
    "input": "Tell me the title of ArXiv paper 2301.13688.",
    "output": "arXiv Query: search_query=&id_list=2301.13688&start=0&max_results=10"
  },
  {
    "input": "Where can I find the code for Feature Pyramid Networks for Object Detection?",
    "output": "The code for Feature Pyramid Networks for Object Detection is available at https://github.com/facebookresearch/Detectron (facebookresearch/Detectron)."
  },
  {
    "input": "Explain the facebookresearch/DPR implementation of Dense Passage Retrieval for Open-Domain Question Answering.",
    "output": "Dense Passage Retriever - is a set of tools and models for open domain Q&A task."
  },
  {
    "input": "What year was LLaMA-Adapter: Efficient Fine-tuning of Language Models released?",
    "output": "The LLaMA-Adapter: Efficient Fine-tuning of Language Models paper was published in 2023."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2004.13637&start=0&max_results=10 repository have?",
    "output": "The facebookresearch/TransCoder repository has 1,722 stars and 261 forks on GitHub."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2301.04856&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2301.04856&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2406.07394&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2406.07394&start=0&max_results=10 paper was published in 2024."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2312.02783?",
    "output": "arXiv Query: search_query=&id_list=2312.02783&start=0&max_results=10"
  },
  {
    "input": "In what year did Generating Wikipedia by Summarizing Long Sequences come out?",
    "output": "The Generating Wikipedia by Summarizing Long Sequences paper was published in 2018."
  },
  {
    "input": "What is the GitHub repository for DeBERTa: Decoding-enhanced BERT with Disentangled Attention?",
    "output": "The code for DeBERTa: Decoding-enhanced BERT with Disentangled Attention is available at https://github.com/microsoft/DeBERTa (microsoft/DeBERTa)."
  },
  {
    "input": "What does the HazyResearch/safari repository do?",
    "output": "Convolutions for Sequence Modeling"
  },
  {
    "input": "How many stars does the Billion-scale similarity search with GPUs repository have?",
    "output": "The facebookresearch/faiss repository has 38,143 stars and 4,132 forks on GitHub."
  },
  {
    "input": "Explain the Llama 2: Open Foundation and Fine-Tuned Chat Models paper.",
    "output": "Llama 2: Open Foundation and Fine-Tuned Chat Models is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at meta-llama/llama which inference code for llama models It is written in Python. The repository has 58,946 stars, indicating high community interest."
  },
  {
    "input": "Explain the hpcaitech/Open-Sora implementation of arXiv Query: search_query=&id_list=2406.10209&start=0&max_results=10.",
    "output": "Open-Sora: Democratizing Efficient Video Production for All"
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2302.03011&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2302.03011&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at mit-han-lab/offsite-tuning which offsite-tuning: transfer learning without full model It is written in Python. The repository has 382 stars, indicating high community interest."
  },
  {
    "input": "What language is the huggingface/alignment-handbook repository written in?",
    "output": "The Zephyr: Direct Distillation of LM Alignment implementation is written in Python."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 1612.03144?",
    "output": "Feature Pyramid Networks for Object Detection"
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2505.22954&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2505.22954&start=0&max_results=10 is available at https://github.com/ruixin31/Spurious_Rewards (ruixin31/Spurious_Rewards)."
  },
  {
    "input": "What year was LLaMA: Open and Efficient Foundation Language Models released?",
    "output": "The LLaMA: Open and Efficient Foundation Language Models paper was published in 2023."
  },
  {
    "input": "What is the GitHub repository for Neural Collaborative Filtering?",
    "output": "The code for Neural Collaborative Filtering is available at https://github.com/hexiangnan/neural_collaborative_filtering (hexiangnan/neural_collaborative_filtering)."
  },
  {
    "input": "Give me an overview of An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.",
    "output": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale is a 2020 paper in Computer Vision and Pattern Recognition. The implementation is available at google-research/vision_transformer It is written in Jupyter Notebook. The repository has 12,040 stars, indicating high community interest."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 1801.01290?",
    "output": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning"
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2504.16084&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2504.16084&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2103.14030?",
    "output": "BEiT: BERT Pre-Training of Image Transformers"
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2006.03511&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2006.03511&start=0&max_results=10 is available at https://github.com/facebookresearch/TransCoder (facebookresearch/TransCoder)."
  },
  {
    "input": "How many stars does the Flamingo: A Visual Language Model for Few-Shot Learning repository have?",
    "output": "The lucidrains/flamingo-pytorch repository has 1,270 stars and 66 forks on GitHub."
  },
  {
    "input": "What is the ArXiv category of SimCSE: Simple Contrastive Learning of Sentence Embeddings?",
    "output": "SimCSE: Simple Contrastive Learning of Sentence Embeddings is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What are the key topics in the pytorch/pytorch repository?",
    "output": "The pytorch/pytorch repository covers: neural-network, autograd, gpu, numpy, deep-learning, tensor, python, machine-learning."
  },
  {
    "input": "What is the GitHub repository for SQuAD: 100,000+ Questions for Machine Comprehension of Text?",
    "output": "The code for SQuAD: 100,000+ Questions for Machine Comprehension of Text is available at https://github.com/rajpurkar/SQuAD-explorer (rajpurkar/SQuAD-explorer)."
  },
  {
    "input": "Explain the Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning paper.",
    "output": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning is a 2018 paper in Machine Learning. The implementation is available at rail-berkeley/softlearning which softlearning is a reinforcement learning framework for training maximum entropy policies in continuous domains. includes the official implementation of the soft actor-critic algorithm. It is written in Python. The repository has 1,370 stars, indicating high community interest."
  },
  {
    "input": "How many stars does the Semi-Supervised Classification with Graph Convolutional Networks repository have?",
    "output": "The tkipf/gcn repository has 7,334 stars and 2,014 forks on GitHub."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2301.10343&start=0&max_results=10 repository have?",
    "output": "The autonomousvision/stylegan-t repository has 1,196 stars and 61 forks on GitHub."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2403.17887&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2403.17887&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What is the ArXiv category of Competition-Level Code Generation with AlphaCode?",
    "output": "Competition-Level Code Generation with AlphaCode is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "When was the MetaGPT: Meta Programming for Multi-Agent Systems paper published?",
    "output": "The MetaGPT: Meta Programming for Multi-Agent Systems paper was published in 2023."
  },
  {
    "input": "Which field does Masked Autoencoders Are Scalable Vision Learners belong to?",
    "output": "Masked Autoencoders Are Scalable Vision Learners is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "What does the google-deepmind/dqn repository do?",
    "output": "Lua/Torch implementation of DQN (Nature, 2015)"
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2302.10866&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2302.10866&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "Explain the OpenGVLab/LLaMA-Adapter implementation of LLaMA-Adapter: Efficient Fine-tuning of Language Models.",
    "output": "[ICLR 2024] Fine-tuning LLaMA to follow Instructions within 1 Hour and 1.2M Parameters"
  },
  {
    "input": "What year was Inductive Representation Learning on Large Graphs released?",
    "output": "The Inductive Representation Learning on Large Graphs paper was published in 2017."
  },
  {
    "input": "Explain the The Power of Scale for Parameter-Efficient Prompt Tuning paper.",
    "output": "The Power of Scale for Parameter-Efficient Prompt Tuning is a 2021 paper in Computation and Language (Natural Language Processing). The implementation is available at google-research/prompt-tuning which original implementation of prompt tuning from lester, et al, 2021 It is written in Python. The repository has 698 stars, indicating high community interest."
  },
  {
    "input": "Tell me the title of ArXiv paper 2304.08485.",
    "output": "MiniGPT-4: Enhancing Vision-Language Understanding"
  },
  {
    "input": "Summarize the MiniGPT-4: Enhancing Vision-Language Understanding paper.",
    "output": "MiniGPT-4: Enhancing Vision-Language Understanding is a 2023 paper in Computer Vision and Pattern Recognition. The implementation is available at Vision-CAIR/MiniGPT-4 which open-sourced codes for minigpt-4 and minigpt-v2 (https://minigpt-4.github.io, https://minigpt-v2.github.io/) It is written in Python. The repository has 25,756 stars, indicating high community interest."
  },
  {
    "input": "How popular is the Segment Anything implementation?",
    "output": "The facebookresearch/segment-anything repository has 52,657 stars and 6,157 forks on GitHub."
  },
  {
    "input": "When was the Gorilla: Large Language Model Connected with Massive APIs paper published?",
    "output": "The Gorilla: Large Language Model Connected with Massive APIs paper was published in 2023."
  },
  {
    "input": "What language is the ruiqi-zhong/D5 repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2302.14233&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Explain the facebookresearch/Detectron implementation of Feature Pyramid Networks for Object Detection.",
    "output": "FAIR's research platform for object detection research, implementing popular algorithms like Mask R-CNN and RetinaNet."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2301.01751&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2301.01751&start=0&max_results=10 is available at https://github.com/oughtinc/ice (oughtinc/ice)."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2404.03592&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2404.03592&start=0&max_results=10 paper was published in 2024."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2301.00774&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2301.00774&start=0&max_results=10 is available at https://github.com/facebookresearch/ConvNeXt-V2 (facebookresearch/ConvNeXt-V2)."
  },
  {
    "input": "Give me an overview of Neural Message Passing for Quantum Chemistry.",
    "output": "Neural Message Passing for Quantum Chemistry is a 2017 paper in Machine Learning. The implementation is available at priba/nmp_qc which our own implementation of neural message passing for computer vision paper It is written in Python. The repository has 343 stars, indicating high community interest."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2302.00923&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2302.00923&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at amazon-science/mm-cot which official implementation for \"multimodal chain-of-thought reasoning in language models\" (stay tuned and more will be updated) It is written in Python. The repository has 3,982 stars, indicating high community interest."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2503.22678&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2503.22678&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is the ArXiv category of EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification?",
    "output": "EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2301.00704&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2301.00704&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2504.00698&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2504.00698&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at allenai/codescientist which codescientist: an automated scientific discovery system for code-based experiments It is written in Python. The repository has 301 stars, indicating high community interest."
  },
  {
    "input": "What programming language is S4: Efficiently Modeling Long Sequences implemented in?",
    "output": "The S4: Efficiently Modeling Long Sequences implementation is written in Jupyter Notebook."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 1609.02907?",
    "output": "Semi-Supervised Classification with Graph Convolutional Networks"
  },
  {
    "input": "What programming language is LoRA: Low-Rank Adaptation of Large Language Models implemented in?",
    "output": "The LoRA: Low-Rank Adaptation of Large Language Models implementation is written in Python."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2502.19328&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2502.19328&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2301.13188&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2301.13188&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "How many stars does the LLaVA-1.5: Improved Baselines with Visual Instruction Tuning repository have?",
    "output": "The haotian-liu/LLaVA repository has 24,034 stars and 2,665 forks on GitHub."
  },
  {
    "input": "Summarize the Synthetic Data Generation with Large Language Models paper.",
    "output": "Synthetic Data Generation with Large Language Models is a 2023 paper in Machine Learning. The implementation is available at google-research/google-research which google research It is written in Jupyter Notebook. The repository has 36,800 stars, indicating high community interest."
  },
  {
    "input": "What language is the ysymyth/ReAct repository written in?",
    "output": "The ReAct: Synergizing Reasoning and Acting in Language Models implementation is written in Jupyter Notebook."
  },
  {
    "input": "What language is the amazon-science/mm-cot repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2301.13188&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Describe the facebookresearch/ConvNeXt-V2 repository that implements arXiv Query: search_query=&id_list=2301.00303&start=0&max_results=10.",
    "output": "Code release for ConvNeXt V2 model"
  },
  {
    "input": "What topics does the BERT: Pre-training of Deep Bidirectional Transformers implementation cover?",
    "output": "The google-research/bert repository covers: nlp, google, natural-language-processing, natural-language-understanding, tensorflow."
  },
  {
    "input": "Give me an overview of Deep Residual Learning for Image Recognition.",
    "output": "Deep Residual Learning for Image Recognition is a 2015 paper in Computer Vision and Pattern Recognition. The implementation is available at KaimingHe/deep-residual-networks which deep residual learning for image recognition  The repository has 6,664 stars, indicating high community interest."
  },
  {
    "input": "What is the GitHub repository for SmoothQuant: Accurate and Efficient Post-Training Quantization?",
    "output": "The code for SmoothQuant: Accurate and Efficient Post-Training Quantization is available at https://github.com/mit-han-lab/smoothquant (mit-han-lab/smoothquant)."
  },
  {
    "input": "What does the shizhediao/active-prompt repository do?",
    "output": "Source code for the paper \"Active Prompting with Chain-of-Thought for Large Language Models\""
  },
  {
    "input": "What paper has ArXiv ID 2406.09403?",
    "output": "arXiv Query: search_query=&id_list=2406.09403&start=0&max_results=10"
  },
  {
    "input": "What does the allenai/longformer repository do?",
    "output": "Longformer: The Long-Document Transformer"
  },
  {
    "input": "Summarize the Diffusion Models Beat GANs on Image Synthesis paper.",
    "output": "Diffusion Models Beat GANs on Image Synthesis is a 2021 paper in Computer Vision and Pattern Recognition. The implementation is available at openai/guided-diffusion It is written in Python. The repository has 7,161 stars, indicating high community interest."
  },
  {
    "input": "What programming language is Proximal Policy Optimization Algorithms implemented in?",
    "output": "The Proximal Policy Optimization Algorithms implementation is written in Python."
  },
  {
    "input": "What programming language is GPT-4 Technical Report implemented in?",
    "output": "The GPT-4 Technical Report implementation is written in Python."
  },
  {
    "input": "What topics does the LLaVA-1.5: Improved Baselines with Visual Instruction Tuning implementation cover?",
    "output": "The haotian-liu/LLaVA repository covers: gpt-4, chatbot, chatgpt, llama, multimodal, llava, foundation-models, instruction-tuning, multi-modality, visual-language-learning, llama-2, llama2, vision-language-model."
  },
  {
    "input": "Describe the facebookresearch/fairseq repository that implements arXiv Query: search_query=&id_list=2109.03264&start=0&max_results=10.",
    "output": "Facebook AI Research Sequence-to-Sequence Toolkit written in Python."
  },
  {
    "input": "What is the ArXiv category of High-Resolution Image Synthesis with Latent Diffusion Models?",
    "output": "High-Resolution Image Synthesis with Latent Diffusion Models is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "Tell me the title of ArXiv paper 1810.04805.",
    "output": "BERT: Pre-training of Deep Bidirectional Transformers"
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2504.00698&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2504.00698&start=0&max_results=10 is available at https://github.com/allenai/codescientist (allenai/codescientist)."
  },
  {
    "input": "What does the tatp22/linformer-pytorch repository do?",
    "output": "My take on a practical implementation of Linformer for Pytorch."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2505.23006&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2505.23006&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at ruixin31/Spurious_Rewards It is written in Python. The repository has 342 stars, indicating high community interest."
  },
  {
    "input": "Where can I find the code for P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning?",
    "output": "The code for P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning is available at https://github.com/THUDM/P-tuning-v2 (THUDM/P-tuning-v2)."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2301.00774?",
    "output": "arXiv Query: search_query=&id_list=2301.00774&start=0&max_results=10"
  },
  {
    "input": "Explain the FLAN: Finetuned Language Models Are Zero-Shot Learners paper.",
    "output": "FLAN: Finetuned Language Models Are Zero-Shot Learners is a 2021 paper in Computation and Language (Natural Language Processing). The implementation is available at google-research/FLAN It is written in Python. The repository has 1,554 stars, indicating high community interest."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2303.08518&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2303.08518&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at FMInference/FlexLLMGen which running large language models on a single gpu for throughput-oriented scenarios. It is written in Python. The repository has 9,379 stars, indicating high community interest."
  },
  {
    "input": "Describe the 0nutation/SpeechGPT repository that implements SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities.",
    "output": "SpeechGPT Series: Speech Large Language Models"
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2004.13637&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2004.13637&start=0&max_results=10 is a 2020 paper in Machine Learning. The implementation is available at facebookresearch/TransCoder which public release of the transcoder research project https://arxiv.org/pdf/2006.03511.pdf It is written in Python. The repository has 1,722 stars, indicating high community interest."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2504.20734&start=0&max_results=10 implementation?",
    "output": "The MoonshotAI/Kimi-Audio repository has 4,367 stars and 316 forks on GitHub."
  },
  {
    "input": "Give me an overview of Multi-Task Deep Neural Networks for Natural Language Understanding.",
    "output": "Multi-Task Deep Neural Networks for Natural Language Understanding is a 2021 paper in Computation and Language (Natural Language Processing). The implementation is available at namisan/mt-dnn which multi-task deep neural networks for natural language understanding It is written in Python. The repository has 2,258 stars, indicating high community interest."
  },
  {
    "input": "What does the oughtinc/ice repository do?",
    "output": "Interactive Composition Explorer: a debugger for compositional language model programs"
  },
  {
    "input": "Give me an overview of AudioPaLM: A Large Language Model for Speech Understanding.",
    "output": "AudioPaLM: A Large Language Model for Speech Understanding is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at lucidrains/audiolm-pytorch which implementation of audiolm, a sota language modeling approach to audio generation out of google research, in pytorch It is written in Python. The repository has 2,607 stars, indicating high community interest."
  },
  {
    "input": "How many stars does the Lookahead Optimizer: k steps forward, 1 step back repository have?",
    "output": "The alphadl/lookahead.pytorch repository has 337 stars and 64 forks on GitHub."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2504.16084&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2504.16084&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What does the beir-cellar/beir repository do?",
    "output": "A Heterogeneous Benchmark for Information Retrieval. Easy to use, evaluate your models across 15+ diverse IR datasets."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2302.00923&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2302.00923&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning about?",
    "output": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning is a 2018 paper in Machine Learning. The implementation is available at rail-berkeley/softlearning which softlearning is a reinforcement learning framework for training maximum entropy policies in continuous domains. includes the official implementation of the soft actor-critic algorithm. It is written in Python. The repository has 1,370 stars, indicating high community interest."
  },
  {
    "input": "How many stars does the Text Embeddings by Weakly-Supervised Contrastive Pre-training repository have?",
    "output": "The microsoft/unilm repository has 21,848 stars and 2,674 forks on GitHub."
  },
  {
    "input": "Where can I find the code for Question and Answer Test-Train Overlap in Open-Domain QA Datasets?",
    "output": "The code for Question and Answer Test-Train Overlap in Open-Domain QA Datasets is available at https://github.com/google-research-datasets/natural-questions (google-research-datasets/natural-questions)."
  },
  {
    "input": "What does the FMInference/FlexLLMGen repository do?",
    "output": "Running large language models on a single GPU for throughput-oriented scenarios."
  },
  {
    "input": "What paper has ArXiv ID 1603.04467?",
    "output": "TensorFlow: A System for Large-Scale Machine Learning"
  },
  {
    "input": "How many stars does the RetNet: Retentive Network: A Successor to Transformer repository have?",
    "output": "The microsoft/torchscale repository has 3,121 stars and 222 forks on GitHub."
  },
  {
    "input": "What are the key topics in the huggingface/transformers repository?",
    "output": "The huggingface/transformers repository covers: nlp, natural-language-processing, pytorch, pytorch-transformers, transformer, model-hub, pretrained-models, speech-recognition, hacktoberfest, python, machine-learning, deep-learning, audio, deepseek, gemma, glm, llm, qwen, vlm."
  },
  {
    "input": "What is the ArXiv category of RoBERTa: A Robustly Optimized BERT Pretraining Approach?",
    "output": "RoBERTa: A Robustly Optimized BERT Pretraining Approach is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2502.09992&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2502.09992&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What programming language is AWQ: Activation-aware Weight Quantization for LLM Compression implemented in?",
    "output": "The AWQ: Activation-aware Weight Quantization for LLM Compression implementation is written in Python."
  },
  {
    "input": "What's the implementation repository for Denoising Diffusion Probabilistic Models?",
    "output": "The code for Denoising Diffusion Probabilistic Models is available at https://github.com/hojonathanho/diffusion (hojonathanho/diffusion)."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2006.16779&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2006.16779&start=0&max_results=10 is a 2020 paper in Machine Learning. The implementation is available at google-research/pegasus It is written in Python. The repository has 1,644 stars, indicating high community interest."
  },
  {
    "input": "How popular is the Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena implementation?",
    "output": "The lm-sys/FastChat repository has 39,271 stars and 4,772 forks on GitHub."
  },
  {
    "input": "How many stars does the Distilling the Knowledge in a Neural Network repository have?",
    "output": "The haitongli/knowledge-distillation-pytorch repository has 1,973 stars and 351 forks on GitHub."
  },
  {
    "input": "What language is the FMInference/FlexLLMGen repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2303.06865&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "How popular is the Longformer: The Long-Document Transformer implementation?",
    "output": "The allenai/longformer repository has 2,175 stars and 288 forks on GitHub."
  },
  {
    "input": "What programming language is Vicuna: An Open-Source Chatbot implemented in?",
    "output": "The Vicuna: An Open-Source Chatbot implementation is written in Python."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2302.03011&start=0&max_results=10 repository have?",
    "output": "The mit-han-lab/offsite-tuning repository has 382 stars and 39 forks on GitHub."
  },
  {
    "input": "What does the zetaalphavector/InPars repository do?",
    "output": "Inquisitive Parrots for Search"
  },
  {
    "input": "What is the GitHub repository for Denoising Diffusion Implicit Models?",
    "output": "The code for Denoising Diffusion Implicit Models is available at https://github.com/ermongroup/ddim (ermongroup/ddim)."
  },
  {
    "input": "Explain the Semi-Supervised Classification with Graph Convolutional Networks paper.",
    "output": "Semi-Supervised Classification with Graph Convolutional Networks is a 2016 paper in Machine Learning. The implementation is available at tkipf/gcn which implementation of graph convolutional networks in tensorflow It is written in Python. The repository has 7,334 stars, indicating high community interest."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2501.11067&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2501.11067&start=0&max_results=10 is available at https://github.com/plurai-ai/intellagent (plurai-ai/intellagent)."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2312.02120&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2312.02120&start=0&max_results=10 is available at https://github.com/ContextualAI/HALOs (ContextualAI/HALOs)."
  },
  {
    "input": "Describe the oughtinc/ice repository that implements arXiv Query: search_query=&id_list=2301.01947&start=0&max_results=10.",
    "output": "Interactive Composition Explorer: a debugger for compositional language model programs"
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2406.09308&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2406.09308&start=0&max_results=10 paper was published in 2024."
  },
  {
    "input": "What does the autonomousvision/stylegan-t repository do?",
    "output": "[ICML'23] StyleGAN-T: Unlocking the Power of GANs for Fast Large-Scale Text-to-Image Synthesis"
  },
  {
    "input": "Summarize the ZeRO: Memory Optimizations Toward Training Trillion Parameter Models paper.",
    "output": "ZeRO: Memory Optimizations Toward Training Trillion Parameter Models is a 2019 paper in Machine Learning. The implementation is available at deepspeedai/DeepSpeed which deepspeed is a deep learning optimization library that makes distributed training and inference easy, efficient, and effective. It is written in Python. The repository has 40,796 stars, indicating high community interest."
  },
  {
    "input": "When was the Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena paper published?",
    "output": "The Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena paper was published in 2023."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2302.12246?",
    "output": "arXiv Query: search_query=&id_list=2302.12246&start=0&max_results=10"
  },
  {
    "input": "Describe the microsoft/torchscale repository that implements RetNet: Retentive Network: A Successor to Transformer.",
    "output": "Foundation Architecture for (M)LLMs"
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2302.08242&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2302.08242&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2501.19393&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2501.19393&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at simplescaling/s1 which s1: simple test-time scaling It is written in Python. The repository has 6,605 stars, indicating high community interest."
  },
  {
    "input": "What's the implementation repository for Question and Answer Test-Train Overlap in Open-Domain QA Datasets?",
    "output": "The code for Question and Answer Test-Train Overlap in Open-Domain QA Datasets is available at https://github.com/google-research-datasets/natural-questions (google-research-datasets/natural-questions)."
  },
  {
    "input": "Where can I find the code for ChemCrow: Augmenting large-language models with chemistry tools?",
    "output": "The code for ChemCrow: Augmenting large-language models with chemistry tools is available at https://github.com/ur-whitelab/chemcrow-public (ur-whitelab/chemcrow-public)."
  },
  {
    "input": "Give me an overview of BERT: Pre-training of Deep Bidirectional Transformers.",
    "output": "BERT: Pre-training of Deep Bidirectional Transformers is a 2018 paper in Computation and Language (Natural Language Processing). The implementation is available at google-research/bert which tensorflow code and pre-trained models for bert It is written in Python. The repository has 39,687 stars, indicating high community interest."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2301.01947?",
    "output": "arXiv Query: search_query=&id_list=2301.01947&start=0&max_results=10"
  },
  {
    "input": "Describe the mit-han-lab/offsite-tuning repository that implements arXiv Query: search_query=&id_list=2302.04023&start=0&max_results=10.",
    "output": "Offsite-Tuning: Transfer Learning without Full Model"
  },
  {
    "input": "How many stars does the Playing Atari with Deep Reinforcement Learning repository have?",
    "output": "The google-deepmind/dqn repository has 618 stars and 164 forks on GitHub."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2312.02120&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2312.02120&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at ContextualAI/HALOs which a library with extensible implementations of dpo, kto, ppo, orpo, and other human-aware loss functions (halos). It is written in Python. The repository has 893 stars, indicating high community interest."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2301.01181&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2301.01181&start=0&max_results=10 is available at https://github.com/JohnNay/llm-lobbyist (JohnNay/llm-lobbyist)."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2301.13688&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2301.13688&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at amazon-science/mm-cot which official implementation for \"multimodal chain-of-thought reasoning in language models\" (stay tuned and more will be updated) It is written in Python. The repository has 3,982 stars, indicating high community interest."
  },
  {
    "input": "What paper has ArXiv ID 2302.08575?",
    "output": "SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities"
  },
  {
    "input": "Tell me the title of ArXiv paper 1906.08237.",
    "output": "XLNet: Generalized Autoregressive Pretraining"
  },
  {
    "input": "What programming language is Beir: A Heterogeneous Benchmark for IR implemented in?",
    "output": "The Beir: A Heterogeneous Benchmark for IR implementation is written in Python."
  },
  {
    "input": "What's the implementation repository for Competition-Level Code Generation with AlphaCode?",
    "output": "The code for Competition-Level Code Generation with AlphaCode is available at https://github.com/google-deepmind/code_contests (google-deepmind/code_contests)."
  },
  {
    "input": "Describe the beir-cellar/beir repository that implements Beir: A Heterogeneous Benchmark for IR.",
    "output": "A Heterogeneous Benchmark for Information Retrieval. Easy to use, evaluate your models across 15+ diverse IR datasets."
  },
  {
    "input": "What category is the XLNet: Generalized Autoregressive Pretraining paper in?",
    "output": "XLNet: Generalized Autoregressive Pretraining is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Where can I find the code for SmoothQuant: Accurate and Efficient Post-Training Quantization?",
    "output": "The code for SmoothQuant: Accurate and Efficient Post-Training Quantization is available at https://github.com/mit-han-lab/smoothquant (mit-han-lab/smoothquant)."
  },
  {
    "input": "In what year did TinyBERT: Distilling BERT for Natural Language Understanding come out?",
    "output": "The TinyBERT: Distilling BERT for Natural Language Understanding paper was published in 2019."
  },
  {
    "input": "When was the SQuAD: 100,000+ Questions for Machine Comprehension of Text paper published?",
    "output": "The SQuAD: 100,000+ Questions for Machine Comprehension of Text paper was published in 2016."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2303.03378&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2303.03378&start=0&max_results=10 is available at https://github.com/NVlabs/prismer (NVlabs/prismer)."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2406.06326&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2406.06326&start=0&max_results=10 paper was published in 2024."
  },
  {
    "input": "What year was Efficient Estimation of Word Representations in Vector Space released?",
    "output": "The Efficient Estimation of Word Representations in Vector Space paper was published in 2013."
  },
  {
    "input": "Give me an overview of Mistral 7B.",
    "output": "Mistral 7B is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at mistralai/mistral-inference which official inference library for mistral models It is written in Jupyter Notebook. The repository has 10,550 stars, indicating high community interest."
  },
  {
    "input": "What is the ArXiv category of Neural Message Passing for Quantum Chemistry?",
    "output": "Neural Message Passing for Quantum Chemistry is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "When was the DeBERTa: Decoding-enhanced BERT with Disentangled Attention paper published?",
    "output": "The DeBERTa: Decoding-enhanced BERT with Disentangled Attention paper was published in 2020."
  },
  {
    "input": "When was the PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel paper published?",
    "output": "The PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel paper was published in 2022."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2502.17437&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2502.17437&start=0&max_results=10 is available at https://github.com/LTH14/fractalgen (LTH14/fractalgen)."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2406.09403&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2406.09403&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2301.00774&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2301.00774&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "In what year did Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning come out?",
    "output": "The Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning paper was published in 2018."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2301.04856&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2301.04856&start=0&max_results=10 is available at https://github.com/frankxu2004/knnlm-why (frankxu2004/knnlm-why)."
  },
  {
    "input": "What are the key topics in the pytorch/pytorch repository?",
    "output": "The pytorch/pytorch repository covers: neural-network, autograd, gpu, numpy, deep-learning, tensor, python, machine-learning."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2007.16122&start=0&max_results=10 implementation?",
    "output": "The linkedin/gdmix repository has 133 stars and 18 forks on GitHub."
  },
  {
    "input": "What programming language is Graph Attention Networks implemented in?",
    "output": "The Graph Attention Networks implementation is written in Python."
  },
  {
    "input": "What is the ArXiv category of Efficient Estimation of Word Representations in Vector Space?",
    "output": "Efficient Estimation of Word Representations in Vector Space is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Explain the lucidrains/audiolm-pytorch implementation of AudioPaLM: A Large Language Model for Speech Understanding.",
    "output": "Implementation of AudioLM, a SOTA Language Modeling Approach to Audio Generation out of Google Research, in Pytorch"
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2303.04129&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2303.04129&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "Describe the tensorflow/tensor2tensor repository that implements Generating Wikipedia by Summarizing Long Sequences.",
    "output": "Library of deep learning models and datasets designed to make deep learning more accessible and accelerate ML research."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2302.14838&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2302.14838&start=0&max_results=10 is available at https://github.com/ruiqi-zhong/D5 (ruiqi-zhong/D5)."
  },
  {
    "input": "Explain the Open-Reasoner-Zero/Open-Reasoner-Zero implementation of arXiv Query: search_query=&id_list=2502.09992&start=0&max_results=10.",
    "output": "Official Repo for Open-Reasoner-Zero"
  },
  {
    "input": "Describe the priba/nmp_qc repository that implements Neural Message Passing for Quantum Chemistry.",
    "output": "Our own implementation of Neural Message Passing for Computer Vision paper"
  },
  {
    "input": "What is the ArXiv category of Analyzing the Structure of Attention in a Transformer Language Model?",
    "output": "Analyzing the Structure of Attention in a Transformer Language Model is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2301.13688&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2301.13688&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "Explain the FMInference/FlexLLMGen implementation of arXiv Query: search_query=&id_list=2303.06865&start=0&max_results=10.",
    "output": "Running large language models on a single GPU for throughput-oriented scenarios."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2303.06865&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2303.06865&start=0&max_results=10 is available at https://github.com/FMInference/FlexLLMGen (FMInference/FlexLLMGen)."
  },
  {
    "input": "Explain the amazon-science/mm-cot implementation of arXiv Query: search_query=&id_list=2301.13688&start=0&max_results=10.",
    "output": "Official implementation for \"Multimodal Chain-of-Thought Reasoning in Language Models\" (stay tuned and more will be updated)"
  },
  {
    "input": "What programming language is Codex: Evaluating Large Language Models Trained on Code implemented in?",
    "output": "The Codex: Evaluating Large Language Models Trained on Code implementation is written in Python."
  },
  {
    "input": "Explain the oughtinc/ice implementation of arXiv Query: search_query=&id_list=2301.01947&start=0&max_results=10.",
    "output": "Interactive Composition Explorer: a debugger for compositional language model programs"
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2404.03592&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2404.03592&start=0&max_results=10 paper was published in 2024."
  },
  {
    "input": "What is LLaVA-1.5: Improved Baselines with Visual Instruction Tuning about?",
    "output": "LLaVA-1.5: Improved Baselines with Visual Instruction Tuning is a 2023 paper in Computer Vision and Pattern Recognition. The implementation is available at haotian-liu/LLaVA which [neurips'23 oral] visual instruction tuning (llava) built towards gpt-4v level capabilities and beyond. It is written in Python. The repository has 24,034 stars, indicating high community interest."
  },
  {
    "input": "What topics does the Lookahead Optimizer: k steps forward, 1 step back implementation cover?",
    "output": "The alphadl/lookahead.pytorch repository covers: pytorch, optimizer, lookahead."
  },
  {
    "input": "What category is the LLaVA: Large Language and Vision Assistant paper in?",
    "output": "LLaVA: Large Language and Vision Assistant is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2301.01181&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2301.01181&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "In what year did Text Embeddings by Weakly-Supervised Contrastive Pre-training come out?",
    "output": "The Text Embeddings by Weakly-Supervised Contrastive Pre-training paper was published in 2022."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2504.01848&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2504.01848&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2102.02779?",
    "output": "ViT: An Image is Worth 16x16 Words"
  },
  {
    "input": "Give me an overview of InstructBLIP: Towards General-purpose Vision-Language Models.",
    "output": "InstructBLIP: Towards General-purpose Vision-Language Models is a 2023 paper in Computer Vision and Pattern Recognition. The implementation is available at salesforce/LAVIS which lavis - a one-stop library for language-vision intelligence It is written in Jupyter Notebook. The repository has 11,040 stars, indicating high community interest."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2501.13545&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2501.13545&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Where can I find the code for TinyBERT: Distilling BERT for Natural Language Understanding?",
    "output": "The code for TinyBERT: Distilling BERT for Natural Language Understanding is available at https://github.com/huawei-noah/Pretrained-Language-Model (huawei-noah/Pretrained-Language-Model)."
  },
  {
    "input": "Tell me the title of ArXiv paper 2309.16588.",
    "output": "Segment Anything in Medical Images"
  },
  {
    "input": "In what year did Vicuna: An Open-Source Chatbot come out?",
    "output": "The Vicuna: An Open-Source Chatbot paper was published in 2023."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2109.03264&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2109.03264&start=0&max_results=10 paper was published in 2021."
  },
  {
    "input": "Which field does Denoising Diffusion Implicit Models belong to?",
    "output": "Denoising Diffusion Implicit Models is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "How popular is the Synthetic Data Generation with Large Language Models implementation?",
    "output": "The google-research/google-research repository has 36,800 stars and 8,255 forks on GitHub."
  },
  {
    "input": "Where can I find the code for Billion-scale similarity search with GPUs?",
    "output": "The code for Billion-scale similarity search with GPUs is available at https://github.com/facebookresearch/faiss (facebookresearch/faiss)."
  },
  {
    "input": "What category is the Denoising Diffusion Implicit Models paper in?",
    "output": "Denoising Diffusion Implicit Models is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Explain the rajpurkar/SQuAD-explorer implementation of SQuAD: 100,000+ Questions for Machine Comprehension of Text.",
    "output": "Visually Explore the Stanford Question Answering Dataset"
  },
  {
    "input": "Which field does EfficientDet: Scalable and Efficient Object Detection belong to?",
    "output": "EfficientDet: Scalable and Efficient Object Detection is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2012.06009&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2012.06009&start=0&max_results=10 paper was published in 2020."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2301.00774&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2301.00774&start=0&max_results=10 is available at https://github.com/facebookresearch/ConvNeXt-V2 (facebookresearch/ConvNeXt-V2)."
  },
  {
    "input": "Which field does Inductive Representation Learning on Large Graphs belong to?",
    "output": "Inductive Representation Learning on Large Graphs is in the cs.SI (cs.SI) category."
  },
  {
    "input": "Describe the google-deepmind/dqn repository that implements Playing Atari with Deep Reinforcement Learning.",
    "output": "Lua/Torch implementation of DQN (Nature, 2015)"
  },
  {
    "input": "Give me an overview of Denoising Diffusion Probabilistic Models.",
    "output": "Denoising Diffusion Probabilistic Models is a 2020 paper in Machine Learning. The implementation is available at hojonathanho/diffusion which denoising diffusion probabilistic models It is written in Python. The repository has 4,860 stars, indicating high community interest."
  },
  {
    "input": "What programming language is Efficient Estimation of Word Representations in Vector Space implemented in?",
    "output": "The Efficient Estimation of Word Representations in Vector Space implementation is written in C."
  },
  {
    "input": "What paper has ArXiv ID 2303.08774?",
    "output": "GPT-4 Technical Report"
  },
  {
    "input": "Where can I find the code for Direct Preference Optimization: Your Language Model is a Reward Model?",
    "output": "The code for Direct Preference Optimization: Your Language Model is a Reward Model is available at https://github.com/eric-mitchell/direct-preference-optimization (eric-mitchell/direct-preference-optimization)."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2302.05206&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2302.05206&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at tianjunz/HIR It is written in Python. The repository has 159 stars, indicating high community interest."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2501.13545&start=0&max_results=10 repository have?",
    "output": "The plurai-ai/intellagent repository has 1,148 stars and 139 forks on GitHub."
  },
  {
    "input": "What are the key topics in the google-research/google-research repository?",
    "output": "The google-research/google-research repository covers: machine-learning, ai, research."
  },
  {
    "input": "What paper has ArXiv ID 2109.03264?",
    "output": "arXiv Query: search_query=&id_list=2109.03264&start=0&max_results=10"
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2502.19328&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2502.19328&start=0&max_results=10 is available at https://github.com/LTH14/fractalgen (LTH14/fractalgen)."
  },
  {
    "input": "What is the GitHub repository for AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning?",
    "output": "The code for AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning is available at https://github.com/QingruZhang/AdaLoRA (QingruZhang/AdaLoRA)."
  },
  {
    "input": "Which field does JAX: Composable transformations of Python+NumPy programs belong to?",
    "output": "JAX: Composable transformations of Python+NumPy programs is in the cs.MS (cs.MS) category."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2406.12824&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2406.12824&start=0&max_results=10 is available at https://github.com/hpcaitech/Open-Sora (hpcaitech/Open-Sora)."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2406.09308&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2406.09308&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What's the implementation repository for Faster R-CNN: Towards Real-Time Object Detection?",
    "output": "The code for Faster R-CNN: Towards Real-Time Object Detection is available at https://github.com/rbgirshick/py-faster-rcnn (rbgirshick/py-faster-rcnn)."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2406.09403&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2406.09403&start=0&max_results=10 is a 2024 paper in Machine Learning. The implementation is available at lamini-ai/Lamini-Memory-Tuning which banishing llm hallucinations requires rethinking generalization The repository has 275 stars, indicating high community interest."
  },
  {
    "input": "Tell me the title of ArXiv paper 2301.01751.",
    "output": "arXiv Query: search_query=&id_list=2301.01751&start=0&max_results=10"
  },
  {
    "input": "Explain the Know What You Don't Know: Unanswerable Questions for SQuAD paper.",
    "output": "Know What You Don't Know: Unanswerable Questions for SQuAD is a 2018 paper in Computation and Language (Natural Language Processing). The implementation is available at rajpurkar/SQuAD-explorer which visually explore the stanford question answering dataset It is written in JavaScript. The repository has 570 stars, indicating high community interest."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2406.12824&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2406.12824&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Where can I find the code for Deep Residual Learning for Image Recognition?",
    "output": "The code for Deep Residual Learning for Image Recognition is available at https://github.com/KaimingHe/deep-residual-networks (KaimingHe/deep-residual-networks)."
  },
  {
    "input": "What topics does the arXiv Query: search_query=&id_list=2301.01947&start=0&max_results=10 implementation cover?",
    "output": "The oughtinc/ice repository covers: debugging, gpt-3, python, language-model."
  },
  {
    "input": "Which field does ControlNet: Adding Conditional Control to Text-to-Image Diffusion belong to?",
    "output": "ControlNet: Adding Conditional Control to Text-to-Image Diffusion is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2303.04226&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2303.04226&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "Which field does SmoothQuant: Accurate and Efficient Post-Training Quantization belong to?",
    "output": "SmoothQuant: Accurate and Efficient Post-Training Quantization is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What language is the tensorflow/tensorflow repository written in?",
    "output": "The TensorFlow: A System for Large-Scale Machine Learning implementation is written in C++."
  },
  {
    "input": "What is the ArXiv category of Billion-scale similarity search with GPUs?",
    "output": "Billion-scale similarity search with GPUs is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "What paper has ArXiv ID 2205.00445?",
    "output": "MRKL Systems: A modular approach to AGI"
  },
  {
    "input": "Explain the Inductive Representation Learning on Large Graphs paper.",
    "output": "Inductive Representation Learning on Large Graphs is a 2017 paper in cs.SI. The implementation is available at williamleif/GraphSAGE which representation learning on large graphs using stochastic graph convolutions. It is written in Python. The repository has 3,630 stars, indicating high community interest."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2502.03387&start=0&max_results=10 implementation?",
    "output": "The GAIR-NLP/LIMO repository has 1,052 stars and 52 forks on GitHub."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 1903.07293?",
    "output": "PyTorch Geometric: Library for Geometric Deep Learning"
  },
  {
    "input": "Summarize the ChemCrow: Augmenting large-language models with chemistry tools paper.",
    "output": "ChemCrow: Augmenting large-language models with chemistry tools is a 2023 paper in Artificial Intelligence. The implementation is available at ur-whitelab/chemcrow-public which chemcrow It is written in Python. The repository has 838 stars, indicating high community interest."
  },
  {
    "input": "Tell me the title of ArXiv paper 2405.04324.",
    "output": "arXiv Query: search_query=&id_list=2405.04324&start=0&max_results=10"
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2505.22954&start=0&max_results=10 repository have?",
    "output": "The ruixin31/Spurious_Rewards repository has 342 stars and 20 forks on GitHub."
  },
  {
    "input": "Explain the LLM.int8(): 8-bit Matrix Multiplication for Transformers paper.",
    "output": "LLM.int8(): 8-bit Matrix Multiplication for Transformers is a 2022 paper in Machine Learning. The implementation is available at bitsandbytes-foundation/bitsandbytes which accessible large language models via k-bit quantization for pytorch. It is written in Python. The repository has 7,771 stars, indicating high community interest."
  },
  {
    "input": "What is the GitHub repository for ControlNet: Adding Conditional Control to Text-to-Image Diffusion?",
    "output": "The code for ControlNet: Adding Conditional Control to Text-to-Image Diffusion is available at https://github.com/lllyasviel/ControlNet (lllyasviel/ControlNet)."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2504.02732&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2504.02732&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at MAXNORM8650/MedAgentSim which medagentsim: self-evolving multi-agent simulations for realistic clinical interactions, miccai 2025 (oral and early accepted) It is written in Python. The repository has 94 stars, indicating high community interest."
  },
  {
    "input": "In what year did Mamba: Linear-Time Sequence Modeling with Selective State Spaces come out?",
    "output": "The Mamba: Linear-Time Sequence Modeling with Selective State Spaces paper was published in 2023."
  },
  {
    "input": "When was the LLaVA: Large Language and Vision Assistant paper published?",
    "output": "The LLaVA: Large Language and Vision Assistant paper was published in 2023."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2302.08242&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2302.08242&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What are the key topics in the fastai/fastai repository?",
    "output": "The fastai/fastai repository covers: deep-learning, machine-learning, pytorch, python, gpu, fastai, notebooks, colab."
  },
  {
    "input": "What's the implementation repository for DreamFusion: Text-to-3D using 2D Diffusion?",
    "output": "The code for DreamFusion: Text-to-3D using 2D Diffusion is available at https://github.com/ashawkey/stable-dreamfusion (ashawkey/stable-dreamfusion)."
  },
  {
    "input": "What programming language is ViT: An Image is Worth 16x16 Words implemented in?",
    "output": "The ViT: An Image is Worth 16x16 Words implementation is written in Jupyter Notebook."
  },
  {
    "input": "What's the implementation repository for InPars: Data Augmentation for Information Retrieval?",
    "output": "The code for InPars: Data Augmentation for Information Retrieval is available at https://github.com/zetaalphavector/InPars (zetaalphavector/InPars)."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2503.23513&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2503.23513&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "Tell me the title of ArXiv paper 2304.02643.",
    "output": "Segment Anything"
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2303.04671&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2303.04671&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at chenfei-wu/TaskMatrix It is written in Python. The repository has 34,352 stars, indicating high community interest."
  },
  {
    "input": "Explain the Synthetic Data Generation with Large Language Models paper.",
    "output": "Synthetic Data Generation with Large Language Models is a 2023 paper in Machine Learning. The implementation is available at google-research/google-research which google research It is written in Jupyter Notebook. The repository has 36,800 stars, indicating high community interest."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2503.00735&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2503.00735&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What does the deepspeedai/DeepSpeed repository do?",
    "output": "DeepSpeed is a deep learning optimization library that makes distributed training and inference easy, efficient, and effective."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2405.02803&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2405.02803&start=0&max_results=10 is available at https://github.com/ibm-granite/granite-code-models (ibm-granite/granite-code-models)."
  },
  {
    "input": "In what year did SmoothQuant: Accurate and Efficient Post-Training Quantization come out?",
    "output": "The SmoothQuant: Accurate and Efficient Post-Training Quantization paper was published in 2022."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2303.02913&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2303.02913&start=0&max_results=10 is available at https://github.com/Shark-NLP/OpenICL (Shark-NLP/OpenICL)."
  },
  {
    "input": "In what year did BERT Rediscovers the Classical NLP Pipeline come out?",
    "output": "The BERT Rediscovers the Classical NLP Pipeline paper was published in 2019."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2406.09403&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2406.09403&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2302.00923&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2302.00923&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "How popular is the Learning to summarize from human feedback implementation?",
    "output": "The openai/summarize-from-feedback repository has 1,055 stars and 152 forks on GitHub."
  },
  {
    "input": "What's the implementation repository for The Flan Collection: Designing Data and Methods for Effective Instruction Tuning?",
    "output": "The code for The Flan Collection: Designing Data and Methods for Effective Instruction Tuning is available at https://github.com/google-research/FLAN (google-research/FLAN)."
  },
  {
    "input": "What paper has ArXiv ID 1905.07830?",
    "output": "SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding"
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2302.03917&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2302.03917&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at mit-han-lab/offsite-tuning which offsite-tuning: transfer learning without full model It is written in Python. The repository has 382 stars, indicating high community interest."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2307.08691?",
    "output": "FlashAttention-2: Faster Attention with Better Parallelism"
  },
  {
    "input": "Describe the FoundationAgents/MetaGPT repository that implements MetaGPT: Meta Programming for Multi-Agent Systems.",
    "output": "\ud83c\udf1f The Multi-Agent Framework: First AI Software Company, Towards Natural Language Programming"
  },
  {
    "input": "Tell me the title of ArXiv paper 2104.00355.",
    "output": "arXiv Query: search_query=&id_list=2104.00355&start=0&max_results=10"
  },
  {
    "input": "What paper has ArXiv ID 2302.12766?",
    "output": "arXiv Query: search_query=&id_list=2302.12766&start=0&max_results=10"
  },
  {
    "input": "What language is the frankxu2004/knnlm-why repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2301.04856&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2301.00704&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2301.00704&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Summarize the Billion-scale similarity search with GPUs paper.",
    "output": "Billion-scale similarity search with GPUs is a 2019 paper in Computer Vision and Pattern Recognition. The implementation is available at facebookresearch/faiss which a library for efficient similarity search and clustering of dense vectors. It is written in C++. The repository has 38,143 stars, indicating high community interest."
  },
  {
    "input": "What language is the plurai-ai/intellagent repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2501.13545&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What paper has ArXiv ID 2109.01652?",
    "output": "Finetuned Language Models Are Zero-Shot Learners"
  },
  {
    "input": "What year was P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning released?",
    "output": "The P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning paper was published in 2021."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2501.13824&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2501.13824&start=0&max_results=10 is available at https://github.com/plurai-ai/intellagent (plurai-ai/intellagent)."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2303.08518&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2303.08518&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2406.09308&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2406.09308&start=0&max_results=10 is a 2024 paper in Machine Learning. The implementation is available at lamini-ai/Lamini-Memory-Tuning which banishing llm hallucinations requires rethinking generalization The repository has 275 stars, indicating high community interest."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2302.00902&start=0&max_results=10 implementation?",
    "output": "The haoliuhl/language-quantized-autoencoders repository has 111 stars and 5 forks on GitHub."
  },
  {
    "input": "Describe the facebookresearch/fairseq repository that implements XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale.",
    "output": "Facebook AI Research Sequence-to-Sequence Toolkit written in Python."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2410.12896&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2410.12896&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Which field does Segment Anything belong to?",
    "output": "Segment Anything is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2502.17437&start=0&max_results=10 implementation?",
    "output": "The LTH14/fractalgen repository has 1,196 stars and 65 forks on GitHub."
  },
  {
    "input": "In what year did Sequence to Sequence Learning with Neural Networks come out?",
    "output": "The Sequence to Sequence Learning with Neural Networks paper was published in 2014."
  },
  {
    "input": "What is the ArXiv category of Codex: Evaluating Large Language Models Trained on Code?",
    "output": "Codex: Evaluating Large Language Models Trained on Code is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2301.02828&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2301.02828&start=0&max_results=10 is available at https://github.com/frankxu2004/knnlm-why (frankxu2004/knnlm-why)."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2302.13007?",
    "output": "Synthetic Data Generation with Large Language Models"
  },
  {
    "input": "Explain the openai/human-eval implementation of Codex: Evaluating Large Language Models Trained on Code.",
    "output": "Code for the paper \"Evaluating Large Language Models Trained on Code\""
  },
  {
    "input": "Explain the LLaMA-Adapter: Efficient Fine-tuning of Language Models paper.",
    "output": "LLaMA-Adapter: Efficient Fine-tuning of Language Models is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at OpenGVLab/LLaMA-Adapter which [iclr 2024] fine-tuning llama to follow instructions within 1 hour and 1.2m parameters It is written in Python. The repository has 5,923 stars, indicating high community interest."
  },
  {
    "input": "How many stars does the Competition-Level Code Generation with AlphaCode repository have?",
    "output": "The google-deepmind/code_contests repository has 2,177 stars and 224 forks on GitHub."
  },
  {
    "input": "How many stars does the FlashAttention: Fast and Memory-Efficient Exact Attention repository have?",
    "output": "The Dao-AILab/flash-attention repository has 20,719 stars and 2,160 forks on GitHub."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2405.03548&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2405.03548&start=0&max_results=10 is available at https://github.com/ibm-granite/granite-code-models (ibm-granite/granite-code-models)."
  },
  {
    "input": "Tell me the title of ArXiv paper 1910.13461.",
    "output": "ALBERT: A Lite BERT"
  },
  {
    "input": "Summarize the InPars: Data Augmentation for Information Retrieval paper.",
    "output": "InPars: Data Augmentation for Information Retrieval is a 2022 paper in Information Retrieval. The implementation is available at zetaalphavector/InPars which inquisitive parrots for search It is written in Python. The repository has 198 stars, indicating high community interest."
  },
  {
    "input": "Which field does Vicuna: An Open-Source Chatbot belong to?",
    "output": "Vicuna: An Open-Source Chatbot is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Describe the meta-llama/llama repository that implements LIMA: Less Is More for Alignment.",
    "output": "Inference code for Llama models"
  },
  {
    "input": "Explain the Deep Residual Learning for Image Recognition paper.",
    "output": "Deep Residual Learning for Image Recognition is a 2015 paper in Computer Vision and Pattern Recognition. The implementation is available at KaimingHe/deep-residual-networks which deep residual learning for image recognition  The repository has 6,664 stars, indicating high community interest."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2301.04104&start=0&max_results=10 repository have?",
    "output": "The google-deepmind/tracr repository has 548 stars and 49 forks on GitHub."
  },
  {
    "input": "How many stars does the InstructBLIP: Towards General-purpose Vision-Language Models repository have?",
    "output": "The salesforce/LAVIS repository has 11,040 stars and 1,086 forks on GitHub."
  },
  {
    "input": "What language is the mit-han-lab/offsite-tuning repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2302.04023&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2301.09515&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2301.09515&start=0&max_results=10 is available at https://github.com/autonomousvision/stylegan-t (autonomousvision/stylegan-t)."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2303.03846&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2303.03846&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at HazyResearch/safari which convolutions for sequence modeling It is written in Assembly. The repository has 904 stars, indicating high community interest."
  },
  {
    "input": "Tell me the title of ArXiv paper 2301.01947.",
    "output": "arXiv Query: search_query=&id_list=2301.01947&start=0&max_results=10"
  },
  {
    "input": "Give me an overview of HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction.",
    "output": "HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction is a 2021 paper in Computation and Language (Natural Language Processing). The implementation is available at facebookresearch/fairseq which facebook ai research sequence-to-sequence toolkit written in python. It is written in Python. The repository has 31,985 stars, indicating high community interest."
  },
  {
    "input": "Which field does Time-LLM: Time Series Forecasting by Reprogramming Large Language Models belong to?",
    "output": "Time-LLM: Time Series Forecasting by Reprogramming Large Language Models is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What programming language is Segment Anything in Medical Images implemented in?",
    "output": "The Segment Anything in Medical Images implementation is written in Jupyter Notebook."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2503.20201&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2503.20201&start=0&max_results=10 is available at https://github.com/sentient-agi/OpenDeepSearch (sentient-agi/OpenDeepSearch)."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2302.12766&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2302.12766&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2301.13688&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2301.13688&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at amazon-science/mm-cot which official implementation for \"multimodal chain-of-thought reasoning in language models\" (stay tuned and more will be updated) It is written in Python. The repository has 3,982 stars, indicating high community interest."
  },
  {
    "input": "What language is the stanford-futuredata/ColBERT repository written in?",
    "output": "The ColBERT: Efficient Passage Search via Contextualized Late Interaction implementation is written in Python."
  },
  {
    "input": "What is the GitHub repository for Inductive Representation Learning on Large Graphs?",
    "output": "The code for Inductive Representation Learning on Large Graphs is available at https://github.com/williamleif/GraphSAGE (williamleif/GraphSAGE)."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2302.08242&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2302.08242&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2301.12652&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2301.12652&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2501.13824&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2501.13824&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at plurai-ai/intellagent which a framework for comprehensive diagnosis and optimization of agents using simulated, realistic synthetic interactions It is written in Python. The repository has 1,148 stars, indicating high community interest."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2405.04324&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2405.04324&start=0&max_results=10 paper was published in 2024."
  },
  {
    "input": "What is Very Deep Convolutional Networks for Large-Scale Image Recognition about?",
    "output": "Very Deep Convolutional Networks for Large-Scale Image Recognition is a 2014 paper in Computer Vision and Pattern Recognition. The implementation is available at machrisaa/tensorflow-vgg which vgg19 and vgg16 on tensorflow It is written in Python. The repository has 2,239 stars, indicating high community interest."
  },
  {
    "input": "Where can I find the code for Very Deep Convolutional Networks for Large-Scale Image Recognition?",
    "output": "The code for Very Deep Convolutional Networks for Large-Scale Image Recognition is available at https://github.com/machrisaa/tensorflow-vgg (machrisaa/tensorflow-vgg)."
  },
  {
    "input": "What's the implementation repository for Time-LLM: Time Series Forecasting by Reprogramming Large Language Models?",
    "output": "The code for Time-LLM: Time Series Forecasting by Reprogramming Large Language Models is available at https://github.com/KimMeen/Time-LLM (KimMeen/Time-LLM)."
  },
  {
    "input": "What does the tensorflow/models repository do?",
    "output": "Models and examples built with TensorFlow"
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2107.07346&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2107.07346&start=0&max_results=10 is a 2021 paper in Machine Learning. The implementation is available at jacopotagliabue/you-dont-need-a-bigger-boat which an end-to-end implementation of intent prediction with metaflow and other cool tools It is written in Python. The repository has 873 stars, indicating high community interest."
  },
  {
    "input": "When was the Diffusion Models Beat GANs on Image Synthesis paper published?",
    "output": "The Diffusion Models Beat GANs on Image Synthesis paper was published in 2021."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2303.03846?",
    "output": "arXiv Query: search_query=&id_list=2303.03846&start=0&max_results=10"
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2009.06367&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2009.06367&start=0&max_results=10 paper was published in 2020."
  },
  {
    "input": "Explain the ELECTRA: Pre-training Text Encoders as Discriminators paper.",
    "output": "ELECTRA: Pre-training Text Encoders as Discriminators is a 2019 paper in Computation and Language (Natural Language Processing). The implementation is available at google-research/electra which electra: pre-training text encoders as discriminators rather than generators It is written in Python. The repository has 2,365 stars, indicating high community interest."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2104.00355&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2104.00355&start=0&max_results=10 is available at https://github.com/facebookresearch/fairseq (facebookresearch/fairseq)."
  },
  {
    "input": "What's the implementation repository for EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification?",
    "output": "The code for EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification is available at https://github.com/jasonwei20/eda_nlp (jasonwei20/eda_nlp)."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2301.05062&start=0&max_results=10 implementation?",
    "output": "The google-deepmind/tracr repository has 548 stars and 49 forks on GitHub."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2406.07394&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2406.07394&start=0&max_results=10 is available at https://github.com/hpcaitech/Open-Sora (hpcaitech/Open-Sora)."
  },
  {
    "input": "What paper has ArXiv ID 1912.01703?",
    "output": "PyTorch: An Imperative Style, High-Performance Deep Learning Library"
  },
  {
    "input": "How popular is the S4: Efficiently Modeling Long Sequences implementation?",
    "output": "The state-spaces/s4 repository has 2,783 stars and 349 forks on GitHub."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2303.01469&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2303.01469&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at ruiqi-zhong/D5 which the github repo for goal driven discovery of distributional differences via language descriptions It is written in Python. The repository has 71 stars, indicating high community interest."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2504.02732&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2504.02732&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2303.02506&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2303.02506&start=0&max_results=10 is available at https://github.com/NVlabs/prismer (NVlabs/prismer)."
  },
  {
    "input": "What topics does the Text Embeddings by Weakly-Supervised Contrastive Pre-training implementation cover?",
    "output": "The microsoft/unilm repository covers: nlp, pre-trained-model, unilm, minilm, layoutlm, layoutxlm, beit, document-ai, trocr, beit-3, foundation-models, xlm-e, deepnet, llm, multimodal, mllm, kosmos, kosmos-1, textdiffuser, bitnet."
  },
  {
    "input": "What does the hpcaitech/Open-Sora repository do?",
    "output": "Open-Sora: Democratizing Efficient Video Production for All"
  },
  {
    "input": "What category is the Gorilla: Large Language Model Connected with Massive APIs paper in?",
    "output": "Gorilla: Large Language Model Connected with Massive APIs is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Explain the facebookresearch/fairseq implementation of RoBERTa: A Robustly Optimized BERT Pretraining Approach.",
    "output": "Facebook AI Research Sequence-to-Sequence Toolkit written in Python."
  },
  {
    "input": "What year was Faster R-CNN: Towards Real-Time Object Detection released?",
    "output": "The Faster R-CNN: Towards Real-Time Object Detection paper was published in 2015."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2406.10209&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2406.10209&start=0&max_results=10 is a 2024 paper in Machine Learning. The implementation is available at hpcaitech/Open-Sora which open-sora: democratizing efficient video production for all It is written in Python. The repository has 27,981 stars, indicating high community interest."
  },
  {
    "input": "What is the GitHub repository for Mamba: Linear-Time Sequence Modeling with Selective State Spaces?",
    "output": "The code for Mamba: Linear-Time Sequence Modeling with Selective State Spaces is available at https://github.com/state-spaces/mamba (state-spaces/mamba)."
  },
  {
    "input": "What does the facebookresearch/faiss repository do?",
    "output": "A library for efficient similarity search and clustering of dense vectors."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2102.01192&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2102.01192&start=0&max_results=10 is a 2021 paper in Machine Learning. The implementation is available at facebookresearch/fairseq which facebook ai research sequence-to-sequence toolkit written in python. It is written in Python. The repository has 31,988 stars, indicating high community interest."
  },
  {
    "input": "Explain the jacopotagliabue/you-dont-need-a-bigger-boat implementation of arXiv Query: search_query=&id_list=2107.07346&start=0&max_results=10.",
    "output": "An end-to-end implementation of intent prediction with Metaflow and other cool tools"
  },
  {
    "input": "Give me an overview of The Flan Collection: Designing Data and Methods for Effective Instruction Tuning.",
    "output": "The Flan Collection: Designing Data and Methods for Effective Instruction Tuning is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at google-research/FLAN It is written in Python. The repository has 1,554 stars, indicating high community interest."
  },
  {
    "input": "Where can I find the code for Depthwise Separable Convolutions for Neural Machine Translation?",
    "output": "The code for Depthwise Separable Convolutions for Neural Machine Translation is available at https://github.com/google-research/google-research (google-research/google-research)."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2301.05062&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2301.05062&start=0&max_results=10 is available at https://github.com/google-deepmind/tracr (google-deepmind/tracr)."
  },
  {
    "input": "What paper has ArXiv ID 2501.13824?",
    "output": "arXiv Query: search_query=&id_list=2501.13824&start=0&max_results=10"
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2405.02803&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2405.02803&start=0&max_results=10 paper was published in 2024."
  },
  {
    "input": "When was the An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale paper published?",
    "output": "The An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale paper was published in 2020."
  },
  {
    "input": "Explain the Mamba: Linear-Time Sequence Modeling with Selective State Spaces paper.",
    "output": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces is a 2023 paper in Machine Learning. The implementation is available at state-spaces/mamba which mamba ssm architecture It is written in Python. The repository has 16,525 stars, indicating high community interest."
  },
  {
    "input": "What language is the MAXNORM8650/MedAgentSim repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2504.02732&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Which field does TinyBERT: Distilling BERT for Natural Language Understanding belong to?",
    "output": "TinyBERT: Distilling BERT for Natural Language Understanding is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What language is the lucidrains/muse-maskgit-pytorch repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2301.00704&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What is the ArXiv category of Whisper: Robust Speech Recognition via Large-Scale Weak Supervision?",
    "output": "Whisper: Robust Speech Recognition via Large-Scale Weak Supervision is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2302.07459&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2302.07459&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2301.02828&start=0&max_results=10 implementation?",
    "output": "The frankxu2004/knnlm-why repository has 59 stars and 3 forks on GitHub."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2302.03011&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2302.03011&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at mit-han-lab/offsite-tuning which offsite-tuning: transfer learning without full model It is written in Python. The repository has 382 stars, indicating high community interest."
  },
  {
    "input": "Describe the simplescaling/s1 repository that implements arXiv Query: search_query=&id_list=2501.19393&start=0&max_results=10.",
    "output": "s1: Simple test-time scaling"
  },
  {
    "input": "Explain the P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning paper.",
    "output": "P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning is a 2021 paper in Computation and Language (Natural Language Processing). The implementation is available at THUDM/P-tuning-v2 which an optimized deep prompt tuning strategy comparable to fine-tuning across scales and tasks It is written in Python. The repository has 2,067 stars, indicating high community interest."
  },
  {
    "input": "What is the GitHub repository for Multi-Task Deep Neural Networks for Natural Language Understanding?",
    "output": "The code for Multi-Task Deep Neural Networks for Natural Language Understanding is available at https://github.com/namisan/mt-dnn (namisan/mt-dnn)."
  },
  {
    "input": "What is Masked Autoencoders Are Scalable Vision Learners about?",
    "output": "Masked Autoencoders Are Scalable Vision Learners is a 2022 paper in Computer Vision and Pattern Recognition. The implementation is available at facebookresearch/mae which pytorch implementation of mae https//arxiv.org/abs/2111.06377 It is written in Python. The repository has 8,108 stars, indicating high community interest."
  },
  {
    "input": "What paper has ArXiv ID 2303.10512?",
    "output": "LLaMA-Adapter: Efficient Fine-tuning of Language Models"
  },
  {
    "input": "Describe the ContextualAI/HALOs repository that implements arXiv Query: search_query=&id_list=2312.02783&start=0&max_results=10.",
    "output": "A library with extensible implementations of DPO, KTO, PPO, ORPO, and other human-aware loss functions (HALOs)."
  },
  {
    "input": "What year was Finetuned Language Models Are Zero-Shot Learners released?",
    "output": "The Finetuned Language Models Are Zero-Shot Learners paper was published in 2021."
  },
  {
    "input": "Explain the Graph Attention Networks paper.",
    "output": "Graph Attention Networks is a 2017 paper in Machine Learning. The implementation is available at PetarV-/GAT which graph attention networks (https://arxiv.org/abs/1710.10903) It is written in Python. The repository has 3,460 stars, indicating high community interest."
  },
  {
    "input": "What category is the JAX: Composable transformations of Python+NumPy programs paper in?",
    "output": "JAX: Composable transformations of Python+NumPy programs is in the cs.MS (cs.MS) category."
  },
  {
    "input": "Which field does RetNet: Retentive Network: A Successor to Transformer belong to?",
    "output": "RetNet: Retentive Network: A Successor to Transformer is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "How popular is the GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers implementation?",
    "output": "The IST-DASLab/gptq repository has 2,221 stars and 186 forks on GitHub."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2006.03511&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2006.03511&start=0&max_results=10 paper was published in 2020."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2006.16779&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2006.16779&start=0&max_results=10 is a 2020 paper in Machine Learning. The implementation is available at google-research/pegasus It is written in Python. The repository has 1,644 stars, indicating high community interest."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2504.11844&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2504.11844&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "Explain the zihangdai/xlnet implementation of XLNet: Generalized Autoregressive Pretraining.",
    "output": "XLNet: Generalized Autoregressive Pretraining for Language Understanding"
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2410.18050&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2410.18050&start=0&max_results=10 is a 2024 paper in Machine Learning. The implementation is available at ibm-granite/granite-3.0-language-models The repository has 267 stars, indicating high community interest."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2303.04671&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2303.04671&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at chenfei-wu/TaskMatrix It is written in Python. The repository has 34,352 stars, indicating high community interest."
  },
  {
    "input": "Give me an overview of Depthwise Separable Convolutions for Neural Machine Translation.",
    "output": "Depthwise Separable Convolutions for Neural Machine Translation is a 2017 paper in Computation and Language (Natural Language Processing). The implementation is available at google-research/google-research which google research It is written in Jupyter Notebook. The repository has 36,800 stars, indicating high community interest."
  },
  {
    "input": "What paper has ArXiv ID 2502.01061?",
    "output": "arXiv Query: search_query=&id_list=2502.01061&start=0&max_results=10"
  },
  {
    "input": "What language is the openai/CLIP repository written in?",
    "output": "The CLIP: Learning Transferable Visual Models From Natural Language implementation is written in Jupyter Notebook."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2302.00902&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2302.00902&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at haoliuhl/language-quantized-autoencoders which language quantized autoencoders It is written in Python. The repository has 111 stars, indicating high community interest."
  },
  {
    "input": "What paper has ArXiv ID 1801.01290?",
    "output": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning"
  },
  {
    "input": "What language is the noahshinn/reflexion repository written in?",
    "output": "The Reflexion: Language Agents with Verbal Reinforcement Learning implementation is written in Python."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2302.03917&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2302.03917&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at mit-han-lab/offsite-tuning which offsite-tuning: transfer learning without full model It is written in Python. The repository has 382 stars, indicating high community interest."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2303.04226&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2303.04226&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at HazyResearch/safari which convolutions for sequence modeling It is written in Assembly. The repository has 904 stars, indicating high community interest."
  },
  {
    "input": "What language is the NVlabs/prismer repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2303.02506&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What paper has ArXiv ID 2308.08155?",
    "output": "MetaGPT: Meta Programming for Multi-Agent Systems"
  },
  {
    "input": "Explain the Whisper: Robust Speech Recognition via Large-Scale Weak Supervision paper.",
    "output": "Whisper: Robust Speech Recognition via Large-Scale Weak Supervision is a 2022 paper in Computation and Language (Natural Language Processing). The implementation is available at openai/whisper which robust speech recognition via large-scale weak supervision It is written in Python. The repository has 91,236 stars, indicating high community interest."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2501.13824&start=0&max_results=10 repository have?",
    "output": "The plurai-ai/intellagent repository has 1,148 stars and 139 forks on GitHub."
  },
  {
    "input": "Where can I find the code for Prefix-Tuning: Optimizing Continuous Prompts for Generation?",
    "output": "The code for Prefix-Tuning: Optimizing Continuous Prompts for Generation is available at https://github.com/XiangLi1999/PrefixTuning (XiangLi1999/PrefixTuning)."
  },
  {
    "input": "What topics does the LoRA: Low-Rank Adaptation of Large Language Models implementation cover?",
    "output": "The microsoft/LoRA repository covers: gpt-2, adaptation, language-model, gpt-3, low-rank, pytorch, deep-learning, roberta, deberta, lora."
  },
  {
    "input": "What category is the Segment Anything paper in?",
    "output": "Segment Anything is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "Describe the salesforce/GeDi repository that implements arXiv Query: search_query=&id_list=2009.06367&start=0&max_results=10.",
    "output": "GeDi: Generative Discriminator Guided Sequence Generation"
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2302.05206&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2302.05206&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "Describe the MAXNORM8650/MedAgentSim repository that implements arXiv Query: search_query=&id_list=2504.02732&start=0&max_results=10.",
    "output": "MedAgentSim: Self-Evolving Multi-Agent Simulations for Realistic Clinical Interactions, MICCAI 2025 (oral and early accepted)"
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2303.03378&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2303.03378&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2502.19328&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2502.19328&start=0&max_results=10 is available at https://github.com/LTH14/fractalgen (LTH14/fractalgen)."
  },
  {
    "input": "Where can I find the code for Dream to Control: Learning Behaviors by Latent Imagination?",
    "output": "The code for Dream to Control: Learning Behaviors by Latent Imagination is available at https://github.com/danijar/dreamer (danijar/dreamer)."
  },
  {
    "input": "Explain the AlpacaEval: An Automatic Evaluator for Instruction-following paper.",
    "output": "AlpacaEval: An Automatic Evaluator for Instruction-following is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at tatsu-lab/alpaca_eval which an automatic evaluator for instruction-following language models. human-validated, high-quality, cheap, and fast. It is written in Jupyter Notebook. The repository has 1,907 stars, indicating high community interest."
  },
  {
    "input": "What's the implementation repository for Sequence to Sequence Learning with Neural Networks?",
    "output": "The code for Sequence to Sequence Learning with Neural Networks is available at https://github.com/farizrahman4u/seq2seq (farizrahman4u/seq2seq)."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2301.02828&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2301.02828&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Where can I find the code for GPT-3: Language Models are Few-Shot Learners?",
    "output": "The code for GPT-3: Language Models are Few-Shot Learners is available at https://github.com/openai/gpt-3 (openai/gpt-3)."
  },
  {
    "input": "Give me an overview of ControlNet: Adding Conditional Control to Text-to-Image Diffusion.",
    "output": "ControlNet: Adding Conditional Control to Text-to-Image Diffusion is a 2023 paper in Computer Vision and Pattern Recognition. The implementation is available at lllyasviel/ControlNet which let us control diffusion models! It is written in Python. The repository has 33,344 stars, indicating high community interest."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2301.01751&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2301.01751&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What is the ArXiv category of Reflexion: Language Agents with Verbal Reinforcement Learning?",
    "output": "Reflexion: Language Agents with Verbal Reinforcement Learning is in the Artificial Intelligence (cs.AI) category."
  },
  {
    "input": "Tell me the title of ArXiv paper 2103.00020.",
    "output": "CLIP: Learning Transferable Visual Models From Natural Language"
  },
  {
    "input": "What is the GitHub repository for Billion-scale similarity search with GPUs?",
    "output": "The code for Billion-scale similarity search with GPUs is available at https://github.com/facebookresearch/faiss (facebookresearch/faiss)."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2410.18050&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2410.18050&start=0&max_results=10 paper was published in 2024."
  },
  {
    "input": "What's the implementation repository for Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context?",
    "output": "The code for Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context is available at https://github.com/kimiyoung/transformer-xl (kimiyoung/transformer-xl)."
  },
  {
    "input": "What's the implementation repository for SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities?",
    "output": "The code for SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities is available at https://github.com/0nutation/SpeechGPT (0nutation/SpeechGPT)."
  },
  {
    "input": "Where can I find the code for RoFormer: Enhanced Transformer with Rotary Position Embedding?",
    "output": "The code for RoFormer: Enhanced Transformer with Rotary Position Embedding is available at https://github.com/ZhuiyiTechnology/roformer (ZhuiyiTechnology/roformer)."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2504.11844&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2504.11844&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at Crista23/goal_directedness_llms It is written in Python. The repository has 12 stars, indicating high community interest."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2501.13545&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2501.13545&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2410.18050&start=0&max_results=10 repository have?",
    "output": "The ibm-granite/granite-3.0-language-models repository has 267 stars and 28 forks on GitHub."
  },
  {
    "input": "What topics does the Reflexion: Language Agents with Verbal Reinforcement Learning implementation cover?",
    "output": "The noahshinn/reflexion repository covers: ai, artificial-intelligence, llm."
  },
  {
    "input": "Summarize the Distilling the Knowledge in a Neural Network paper.",
    "output": "Distilling the Knowledge in a Neural Network is a 2015 paper in Machine Learning. The implementation is available at haitongli/knowledge-distillation-pytorch which a pytorch implementation for exploring deep and shallow knowledge distillation (kd) experiments with flexibility It is written in Python. The repository has 1,973 stars, indicating high community interest."
  },
  {
    "input": "What does the linkedin/gdmix repository do?",
    "output": "A deep ranking personalization framework"
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2504.01848&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2504.01848&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "Give me an overview of The Power of Scale for Parameter-Efficient Prompt Tuning.",
    "output": "The Power of Scale for Parameter-Efficient Prompt Tuning is a 2021 paper in Computation and Language (Natural Language Processing). The implementation is available at google-research/prompt-tuning which original implementation of prompt tuning from lester, et al, 2021 It is written in Python. The repository has 698 stars, indicating high community interest."
  },
  {
    "input": "What language is the mit-han-lab/offsite-tuning repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2302.03917&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "How popular is the Training language models to follow instructions with human feedback implementation?",
    "output": "The openai/following-instructions-human-feedback repository has 1,249 stars and 147 forks on GitHub."
  },
  {
    "input": "Give me an overview of XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale.",
    "output": "XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale is a 2021 paper in Computation and Language (Natural Language Processing). The implementation is available at facebookresearch/fairseq which facebook ai research sequence-to-sequence toolkit written in python. It is written in Python. The repository has 31,985 stars, indicating high community interest."
  },
  {
    "input": "Which field does BERT: Pre-training of Deep Bidirectional Transformers belong to?",
    "output": "BERT: Pre-training of Deep Bidirectional Transformers is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2301.09515&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2301.09515&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at autonomousvision/stylegan-t which [icml'23] stylegan-t: unlocking the power of gans for fast large-scale text-to-image synthesis It is written in Python. The repository has 1,196 stars, indicating high community interest."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2104.00355&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2104.00355&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "How popular is the Semi-Supervised Classification with Graph Convolutional Networks implementation?",
    "output": "The tkipf/gcn repository has 7,334 stars and 2,014 forks on GitHub."
  },
  {
    "input": "Describe the microsoft/unilm repository that implements Text Embeddings by Weakly-Supervised Contrastive Pre-training.",
    "output": "Large-scale Self-supervised Pre-training Across Tasks, Languages, and Modalities"
  },
  {
    "input": "Give me an overview of Efficient Estimation of Word Representations in Vector Space.",
    "output": "Efficient Estimation of Word Representations in Vector Space is a 2013 paper in Computation and Language (Natural Language Processing). The implementation is available at tmikolov/word2vec which automatically exported from code.google.com/p/word2vec It is written in C. The repository has 1,569 stars, indicating high community interest."
  },
  {
    "input": "What programming language is HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction implemented in?",
    "output": "The HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction implementation is written in Python."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2504.20734?",
    "output": "arXiv Query: search_query=&id_list=2504.20734&start=0&max_results=10"
  },
  {
    "input": "How many stars does the An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale repository have?",
    "output": "The google-research/vision_transformer repository has 12,040 stars and 1,432 forks on GitHub."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 1508.07909?",
    "output": "Neural Machine Translation by Jointly Learning to Align and Translate"
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2208.01618?",
    "output": "DreamBooth: Fine Tuning Text-to-Image Diffusion Models"
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2303.02913&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2303.02913&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "In what year did Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context come out?",
    "output": "The Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context paper was published in 2019."
  },
  {
    "input": "Explain the langchain-ai/langchain implementation of MRKL Systems: A modular approach to AGI.",
    "output": "\ud83e\udd9c\ud83d\udd17 The platform for reliable agents."
  },
  {
    "input": "Explain the Densely Connected Convolutional Networks paper.",
    "output": "Densely Connected Convolutional Networks is a 2016 paper in Computer Vision and Pattern Recognition. The implementation is available at liuzhuang13/DenseNet which densely connected convolutional networks, in cvpr 2017 (best paper award). It is written in Lua. The repository has 4,843 stars, indicating high community interest."
  },
  {
    "input": "Explain the Flamingo: A Visual Language Model for Few-Shot Learning paper.",
    "output": "Flamingo: A Visual Language Model for Few-Shot Learning is a 2022 paper in Computer Vision and Pattern Recognition. The implementation is available at lucidrains/flamingo-pytorch which implementation of \ud83e\udda9 flamingo, state-of-the-art few-shot visual question answering attention net out of deepmind, in pytorch It is written in Python. The repository has 1,270 stars, indicating high community interest."
  },
  {
    "input": "Explain the salesforce/LAVIS implementation of InstructBLIP: Towards General-purpose Vision-Language Models.",
    "output": "LAVIS - A One-stop Library for Language-Vision Intelligence"
  },
  {
    "input": "Where can I find the code for Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena?",
    "output": "The code for Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena is available at https://github.com/lm-sys/FastChat (lm-sys/FastChat)."
  },
  {
    "input": "What language is the farizrahman4u/seq2seq repository written in?",
    "output": "The Sequence to Sequence Learning with Neural Networks implementation is written in Python."
  },
  {
    "input": "Which field does Whisper: Robust Speech Recognition via Large-Scale Weak Supervision belong to?",
    "output": "Whisper: Robust Speech Recognition via Large-Scale Weak Supervision is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Where can I find the code for Mistral 7B?",
    "output": "The code for Mistral 7B is available at https://github.com/mistralai/mistral-inference (mistralai/mistral-inference)."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2102.01192?",
    "output": "arXiv Query: search_query=&id_list=2102.01192&start=0&max_results=10"
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2502.01061&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2502.01061&start=0&max_results=10 is available at https://github.com/GAIR-NLP/LIMO (GAIR-NLP/LIMO)."
  },
  {
    "input": "What language is the hpcaitech/Open-Sora repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2406.10209&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2504.19413&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2504.19413&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at MoonshotAI/Kimi-Audio which kimi-audio, an open-source audio foundation model excelling in audio understanding, generation, and conversation It is written in Python. The repository has 4,367 stars, indicating high community interest."
  },
  {
    "input": "What's the implementation repository for Neural Machine Translation by Jointly Learning to Align and Translate?",
    "output": "The code for Neural Machine Translation by Jointly Learning to Align and Translate is available at https://github.com/lisa-groundhog/GroundHog (lisa-groundhog/GroundHog)."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2302.12246&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2302.12246&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at shizhediao/active-prompt which source code for the paper \"active prompting with chain-of-thought for large language models\" It is written in Python. The repository has 247 stars, indicating high community interest."
  },
  {
    "input": "What is Multi-Task Deep Neural Networks for Natural Language Understanding about?",
    "output": "Multi-Task Deep Neural Networks for Natural Language Understanding is a 2021 paper in Computation and Language (Natural Language Processing). The implementation is available at namisan/mt-dnn which multi-task deep neural networks for natural language understanding It is written in Python. The repository has 2,258 stars, indicating high community interest."
  },
  {
    "input": "What is the GitHub repository for Neural Machine Translation by Jointly Learning to Align and Translate?",
    "output": "The code for Neural Machine Translation by Jointly Learning to Align and Translate is available at https://github.com/lisa-groundhog/GroundHog (lisa-groundhog/GroundHog)."
  },
  {
    "input": "What language is the facebookresearch/DPR repository written in?",
    "output": "The RAG: Retrieval-Augmented Generation implementation is written in Python."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2502.09992&start=0&max_results=10 implementation?",
    "output": "The Open-Reasoner-Zero/Open-Reasoner-Zero repository has 2,068 stars and 119 forks on GitHub."
  },
  {
    "input": "In what year did InPars: Data Augmentation for Information Retrieval come out?",
    "output": "The InPars: Data Augmentation for Information Retrieval paper was published in 2022."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2405.03520&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2405.03520&start=0&max_results=10 is a 2024 paper in Machine Learning. The implementation is available at ibm-granite/granite-code-models which granite code models: a family of open foundation models for code intelligence The repository has 1,240 stars, indicating high community interest."
  },
  {
    "input": "What category is the SmoothQuant: Accurate and Efficient Post-Training Quantization paper in?",
    "output": "SmoothQuant: Accurate and Efficient Post-Training Quantization is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Explain the Very Deep Convolutional Networks for Large-Scale Image Recognition paper.",
    "output": "Very Deep Convolutional Networks for Large-Scale Image Recognition is a 2014 paper in Computer Vision and Pattern Recognition. The implementation is available at machrisaa/tensorflow-vgg which vgg19 and vgg16 on tensorflow It is written in Python. The repository has 2,239 stars, indicating high community interest."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2301.12652&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2301.12652&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at amazon-science/mm-cot which official implementation for \"multimodal chain-of-thought reasoning in language models\" (stay tuned and more will be updated) It is written in Python. The repository has 3,982 stars, indicating high community interest."
  },
  {
    "input": "What language is the horovod/horovod repository written in?",
    "output": "The Horovod: fast and easy distributed deep learning in TensorFlow implementation is written in Python."
  },
  {
    "input": "How many stars does the RAG: Retrieval-Augmented Generation repository have?",
    "output": "The facebookresearch/DPR repository has 1,847 stars and 315 forks on GitHub."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2410.12896&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2410.12896&start=0&max_results=10 is available at https://github.com/ibm-granite/granite-3.0-language-models (ibm-granite/granite-3.0-language-models)."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2505.22954&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2505.22954&start=0&max_results=10 is available at https://github.com/ruixin31/Spurious_Rewards (ruixin31/Spurious_Rewards)."
  },
  {
    "input": "When was the BEiT: BERT Pre-Training of Image Transformers paper published?",
    "output": "The BEiT: BERT Pre-Training of Image Transformers paper was published in 2021."
  },
  {
    "input": "What programming language is Competition-Level Code Generation with AlphaCode implemented in?",
    "output": "The Competition-Level Code Generation with AlphaCode implementation is written in C++."
  },
  {
    "input": "What paper has ArXiv ID 2309.16588?",
    "output": "Segment Anything in Medical Images"
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2302.05206?",
    "output": "arXiv Query: search_query=&id_list=2302.05206&start=0&max_results=10"
  },
  {
    "input": "What topics does the HuggingGPT: Solving AI Tasks with ChatGPT implementation cover?",
    "output": "The microsoft/JARVIS repository covers: deep-learning, platform, pytorch."
  },
  {
    "input": "Explain the lllyasviel/ControlNet implementation of ControlNet: Adding Conditional Control to Text-to-Image Diffusion.",
    "output": "Let us control diffusion models!"
  },
  {
    "input": "What is the GitHub repository for Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context?",
    "output": "The code for Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context is available at https://github.com/kimiyoung/transformer-xl (kimiyoung/transformer-xl)."
  },
  {
    "input": "What language is the google-research/FLAN repository written in?",
    "output": "The FLAN: Finetuned Language Models Are Zero-Shot Learners implementation is written in Python."
  },
  {
    "input": "Describe the THUDM/P-tuning-v2 repository that implements P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning.",
    "output": "An optimized deep prompt tuning strategy comparable to fine-tuning across scales and tasks"
  },
  {
    "input": "Tell me the title of ArXiv paper 2103.14030.",
    "output": "BEiT: BERT Pre-Training of Image Transformers"
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2302.12246&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2302.12246&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "Explain the Diffusion Models Beat GANs on Image Synthesis paper.",
    "output": "Diffusion Models Beat GANs on Image Synthesis is a 2021 paper in Computer Vision and Pattern Recognition. The implementation is available at openai/guided-diffusion It is written in Python. The repository has 7,161 stars, indicating high community interest."
  },
  {
    "input": "Explain the QLoRA: Efficient Finetuning of Quantized LLMs paper.",
    "output": "QLoRA: Efficient Finetuning of Quantized LLMs is a 2023 paper in Machine Learning. The implementation is available at artidoro/qlora which qlora: efficient finetuning of quantized llms It is written in Jupyter Notebook. The repository has 10,764 stars, indicating high community interest."
  },
  {
    "input": "What year was Semi-Supervised Classification with Graph Convolutional Networks released?",
    "output": "The Semi-Supervised Classification with Graph Convolutional Networks paper was published in 2016."
  },
  {
    "input": "What are the key topics in the salesforce/LAVIS repository?",
    "output": "The salesforce/LAVIS repository covers: deep-learning, deep-learning-library, image-captioning, salesforce, vision-and-language, vision-framework, vision-language-pretraining, vision-language-transformer, visual-question-anwsering, multimodal-datasets, multimodal-deep-learning."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2012.06009&start=0&max_results=10 implementation?",
    "output": "The facebookresearch/CovidPrognosis repository has 158 stars and 38 forks on GitHub."
  },
  {
    "input": "What is Training language models to follow instructions with human feedback about?",
    "output": "Training language models to follow instructions with human feedback is a 2022 paper in Computation and Language (Natural Language Processing). The implementation is available at openai/following-instructions-human-feedback The repository has 1,249 stars, indicating high community interest."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2302.03917&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2302.03917&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Explain the Multi-Task Deep Neural Networks for Natural Language Understanding paper.",
    "output": "Multi-Task Deep Neural Networks for Natural Language Understanding is a 2021 paper in Computation and Language (Natural Language Processing). The implementation is available at namisan/mt-dnn which multi-task deep neural networks for natural language understanding It is written in Python. The repository has 2,258 stars, indicating high community interest."
  },
  {
    "input": "What's the implementation repository for Gorilla: Large Language Model Connected with Massive APIs?",
    "output": "The code for Gorilla: Large Language Model Connected with Massive APIs is available at https://github.com/ShishirPatil/gorilla (ShishirPatil/gorilla)."
  },
  {
    "input": "How many stars does the Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning repository have?",
    "output": "The rail-berkeley/softlearning repository has 1,370 stars and 249 forks on GitHub."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2004.08492&start=0&max_results=10 implementation?",
    "output": "The uber/orbit repository has 2,019 stars and 142 forks on GitHub."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2301.13188&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2301.13188&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What is Direct Preference Optimization: Your Language Model is a Reward Model about?",
    "output": "Direct Preference Optimization: Your Language Model is a Reward Model is a 2023 paper in Machine Learning. The implementation is available at eric-mitchell/direct-preference-optimization which reference implementation for dpo (direct preference optimization) It is written in Python. The repository has 2,789 stars, indicating high community interest."
  },
  {
    "input": "What does the ur-whitelab/chemcrow-public repository do?",
    "output": "Chemcrow"
  },
  {
    "input": "Give me an overview of DistilBERT: a distilled version of BERT.",
    "output": "DistilBERT: a distilled version of BERT is a 2019 paper in Computation and Language (Natural Language Processing). The implementation is available at huggingface/transformers which \ud83e\udd17 transformers: the model-definition framework for state-of-the-art machine learning models in text, vision, audio, and multimodal models, for both inference and training.  It is written in Python. The repository has 152,994 stars, indicating high community interest."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2101.04909&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2101.04909&start=0&max_results=10 is available at https://github.com/facebookresearch/CovidPrognosis (facebookresearch/CovidPrognosis)."
  },
  {
    "input": "Explain the mit-han-lab/llm-awq implementation of AWQ: Activation-aware Weight Quantization for LLM Compression.",
    "output": "[MLSys 2024 Best Paper Award] AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration"
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2303.04129&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2303.04129&start=0&max_results=10 is available at https://github.com/HazyResearch/safari (HazyResearch/safari)."
  },
  {
    "input": "Explain the Dao-AILab/flash-attention implementation of FlashAttention: Fast and Memory-Efficient Exact Attention.",
    "output": "Fast and memory-efficient exact attention"
  },
  {
    "input": "Tell me the title of ArXiv paper 2205.05638.",
    "output": "AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning"
  },
  {
    "input": "Explain the Masked Autoencoders Are Scalable Vision Learners paper.",
    "output": "Masked Autoencoders Are Scalable Vision Learners is a 2022 paper in Computer Vision and Pattern Recognition. The implementation is available at facebookresearch/mae which pytorch implementation of mae https//arxiv.org/abs/2111.06377 It is written in Python. The repository has 8,108 stars, indicating high community interest."
  },
  {
    "input": "Give me an overview of Learning to summarize from human feedback.",
    "output": "Learning to summarize from human feedback is a 2020 paper in Computation and Language (Natural Language Processing). The implementation is available at openai/summarize-from-feedback which code for \"learning to summarize from human feedback\" It is written in Python. The repository has 1,055 stars, indicating high community interest."
  },
  {
    "input": "What's the implementation repository for BLIP-2: Bootstrapping Language-Image Pre-training?",
    "output": "The code for BLIP-2: Bootstrapping Language-Image Pre-training is available at https://github.com/salesforce/LAVIS (salesforce/LAVIS)."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2303.03846&start=0&max_results=10 repository have?",
    "output": "The HazyResearch/safari repository has 904 stars and 70 forks on GitHub."
  },
  {
    "input": "What category is the Deep Graph Library: A Graph-Centric, Highly-Performant Package paper in?",
    "output": "Deep Graph Library: A Graph-Centric, Highly-Performant Package is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2302.00923&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2302.00923&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What's the implementation repository for MMLU: Measuring Massive Multitask Language Understanding?",
    "output": "The code for MMLU: Measuring Massive Multitask Language Understanding is available at https://github.com/hendrycks/test (hendrycks/test)."
  },
  {
    "input": "What's the implementation repository for RAG: Retrieval-Augmented Generation?",
    "output": "The code for RAG: Retrieval-Augmented Generation is available at https://github.com/facebookresearch/DPR (facebookresearch/DPR)."
  },
  {
    "input": "What category is the DeBERTa: Decoding-enhanced BERT with Disentangled Attention paper in?",
    "output": "DeBERTa: Decoding-enhanced BERT with Disentangled Attention is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What does the jasonwei20/eda_nlp repository do?",
    "output": "Data augmentation for NLP, presented at EMNLP 2019"
  },
  {
    "input": "Explain the GPT-3: Language Models are Few-Shot Learners paper.",
    "output": "GPT-3: Language Models are Few-Shot Learners is a 2020 paper in Computation and Language (Natural Language Processing). The implementation is available at openai/gpt-3 which gpt-3: language models are few-shot learners The repository has 15,784 stars, indicating high community interest."
  },
  {
    "input": "How many stars does the AlpacaEval: An Automatic Evaluator for Instruction-following repository have?",
    "output": "The tatsu-lab/alpaca_eval repository has 1,907 stars and 290 forks on GitHub."
  },
  {
    "input": "What year was Mamba: Linear-Time Sequence Modeling with Selective State Spaces released?",
    "output": "The Mamba: Linear-Time Sequence Modeling with Selective State Spaces paper was published in 2023."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2301.00808&start=0&max_results=10 implementation?",
    "output": "The facebookresearch/ConvNeXt-V2 repository has 1,890 stars and 155 forks on GitHub."
  },
  {
    "input": "Tell me the title of ArXiv paper 1904.12848.",
    "output": "EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification"
  },
  {
    "input": "What programming language is JAX: Composable transformations of Python+NumPy programs implemented in?",
    "output": "The JAX: Composable transformations of Python+NumPy programs implementation is written in Python."
  },
  {
    "input": "What paper has ArXiv ID 2301.05062?",
    "output": "arXiv Query: search_query=&id_list=2301.05062&start=0&max_results=10"
  },
  {
    "input": "What is ViT: An Image is Worth 16x16 Words about?",
    "output": "ViT: An Image is Worth 16x16 Words is a 2021 paper in Computer Vision and Pattern Recognition. The implementation is available at google-research/vision_transformer It is written in Jupyter Notebook. The repository has 12,040 stars, indicating high community interest."
  },
  {
    "input": "What paper has ArXiv ID 2005.00687?",
    "output": "Deep Graph Library: A Graph-Centric, Highly-Performant Package"
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2111.00396?",
    "output": "S4: Efficiently Modeling Long Sequences"
  },
  {
    "input": "Give me an overview of ZeRO: Memory Optimizations Toward Training Trillion Parameter Models.",
    "output": "ZeRO: Memory Optimizations Toward Training Trillion Parameter Models is a 2019 paper in Machine Learning. The implementation is available at deepspeedai/DeepSpeed which deepspeed is a deep learning optimization library that makes distributed training and inference easy, efficient, and effective. It is written in Python. The repository has 40,796 stars, indicating high community interest."
  },
  {
    "input": "Describe the lllyasviel/ControlNet repository that implements ControlNet: Adding Conditional Control to Text-to-Image Diffusion.",
    "output": "Let us control diffusion models!"
  },
  {
    "input": "Tell me the title of ArXiv paper 2302.03917.",
    "output": "arXiv Query: search_query=&id_list=2302.03917&start=0&max_results=10"
  },
  {
    "input": "What paper has ArXiv ID 1711.05101?",
    "output": "Decoupled Weight Decay Regularization"
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2504.21801&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2504.21801&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "Give me an overview of Neural Machine Translation by Jointly Learning to Align and Translate.",
    "output": "Neural Machine Translation by Jointly Learning to Align and Translate is a 2015 paper in Computation and Language (Natural Language Processing). The implementation is available at lisa-groundhog/GroundHog which library for implementing rnns with theano It is written in Python. The repository has 613 stars, indicating high community interest."
  },
  {
    "input": "What paper has ArXiv ID 1910.10683?",
    "output": "T5: Exploring the Limits of Transfer Learning"
  },
  {
    "input": "What is the title of the paper with ArXiv ID 1910.02054?",
    "output": "ZeRO: Memory Optimizations Toward Training Trillion Parameter Models"
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2403.17887&start=0&max_results=10 implementation?",
    "output": "The OpenBMB/Eurus repository has 320 stars and 14 forks on GitHub."
  },
  {
    "input": "What is Competition-Level Code Generation with AlphaCode about?",
    "output": "Competition-Level Code Generation with AlphaCode is a 2022 paper in Machine Learning. The implementation is available at google-deepmind/code_contests It is written in C++. The repository has 2,177 stars, indicating high community interest."
  },
  {
    "input": "Summarize the Deep Residual Learning for Image Recognition paper.",
    "output": "Deep Residual Learning for Image Recognition is a 2015 paper in Computer Vision and Pattern Recognition. The implementation is available at KaimingHe/deep-residual-networks which deep residual learning for image recognition  The repository has 6,664 stars, indicating high community interest."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 1703.04691?",
    "output": "Neural Message Passing for Quantum Chemistry"
  },
  {
    "input": "Explain the salesforce/BLIP implementation of BLIP: Bootstrapping Language-Image Pre-training.",
    "output": "PyTorch code for BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation  "
  },
  {
    "input": "What is the title of the paper with ArXiv ID 1301.3781?",
    "output": "Efficient Estimation of Word Representations in Vector Space"
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2105.05233?",
    "output": "Diffusion Models Beat GANs on Image Synthesis"
  },
  {
    "input": "What is the ArXiv category of Very Deep Convolutional Networks for Large-Scale Image Recognition?",
    "output": "Very Deep Convolutional Networks for Large-Scale Image Recognition is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "What does the priba/nmp_qc repository do?",
    "output": "Our own implementation of Neural Message Passing for Computer Vision paper"
  },
  {
    "input": "What is ZeRO: Memory Optimizations Toward Training Trillion Parameter Models about?",
    "output": "ZeRO: Memory Optimizations Toward Training Trillion Parameter Models is a 2019 paper in Machine Learning. The implementation is available at deepspeedai/DeepSpeed which deepspeed is a deep learning optimization library that makes distributed training and inference easy, efficient, and effective. It is written in Python. The repository has 40,796 stars, indicating high community interest."
  },
  {
    "input": "What's the implementation repository for RoFormer: Enhanced Transformer with Rotary Position Embedding?",
    "output": "The code for RoFormer: Enhanced Transformer with Rotary Position Embedding is available at https://github.com/ZhuiyiTechnology/roformer (ZhuiyiTechnology/roformer)."
  },
  {
    "input": "What paper has ArXiv ID 2302.13971?",
    "output": "LLaMA: Open and Efficient Foundation Language Models"
  },
  {
    "input": "Explain the google-research/electra implementation of ELECTRA: Pre-training Text Encoders as Discriminators.",
    "output": "ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators"
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2505.22954&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2505.22954&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What language is the facebookresearch/Detectron repository written in?",
    "output": "The Feature Pyramid Networks for Object Detection implementation is written in Python."
  },
  {
    "input": "What are the key topics in the zihangdai/xlnet repository?",
    "output": "The zihangdai/xlnet repository covers: tensorflow, nlp, deep-learning."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2406.10209?",
    "output": "arXiv Query: search_query=&id_list=2406.10209&start=0&max_results=10"
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2302.14233&start=0&max_results=10 implementation?",
    "output": "The ruiqi-zhong/D5 repository has 71 stars and 13 forks on GitHub."
  },
  {
    "input": "Explain the facebookresearch/fairseq implementation of HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction.",
    "output": "Facebook AI Research Sequence-to-Sequence Toolkit written in Python."
  },
  {
    "input": "What paper has ArXiv ID 2009.01325?",
    "output": "Learning to summarize from human feedback"
  },
  {
    "input": "What does the GAIR-NLP/LIMO repository do?",
    "output": "[COLM 2025] LIMO: Less is More for Reasoning"
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2504.11844&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2504.11844&start=0&max_results=10 is available at https://github.com/Crista23/goal_directedness_llms (Crista23/goal_directedness_llms)."
  },
  {
    "input": "What year was SmoothQuant: Accurate and Efficient Post-Training Quantization released?",
    "output": "The SmoothQuant: Accurate and Efficient Post-Training Quantization paper was published in 2022."
  },
  {
    "input": "What's the implementation repository for Adam: A Method for Stochastic Optimization?",
    "output": "The code for Adam: A Method for Stochastic Optimization is available at https://github.com/pytorch/pytorch (pytorch/pytorch)."
  },
  {
    "input": "Which field does Enriching Word Vectors with Subword Information belong to?",
    "output": "Enriching Word Vectors with Subword Information is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2501.13824&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2501.13824&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at plurai-ai/intellagent which a framework for comprehensive diagnosis and optimization of agents using simulated, realistic synthetic interactions It is written in Python. The repository has 1,148 stars, indicating high community interest."
  },
  {
    "input": "How many stars does the LLaMA-Adapter: Efficient Fine-tuning of Language Models repository have?",
    "output": "The OpenGVLab/LLaMA-Adapter repository has 5,923 stars and 383 forks on GitHub."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 1608.06993?",
    "output": "Densely Connected Convolutional Networks"
  },
  {
    "input": "What topics does the YOLOX: Exceeding YOLO Series in 2021 implementation cover?",
    "output": "The Megvii-BaseDetection/YOLOX repository covers: yolox, yolov3, onnx, tensorrt, ncnn, openvino, pytorch, megengine, object-detection, yolo, deep-learning."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2406.09308&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2406.09308&start=0&max_results=10 is a 2024 paper in Machine Learning. The implementation is available at lamini-ai/Lamini-Memory-Tuning which banishing llm hallucinations requires rethinking generalization The repository has 275 stars, indicating high community interest."
  },
  {
    "input": "Which field does ZeRO: Memory Optimizations Toward Training Trillion Parameter Models belong to?",
    "output": "ZeRO: Memory Optimizations Toward Training Trillion Parameter Models is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Which field does Sequence to Sequence Learning with Neural Networks belong to?",
    "output": "Sequence to Sequence Learning with Neural Networks is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "When was the FLAN: Finetuned Language Models Are Zero-Shot Learners paper published?",
    "output": "The FLAN: Finetuned Language Models Are Zero-Shot Learners paper was published in 2021."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2301.02828&start=0&max_results=10 repository have?",
    "output": "The frankxu2004/knnlm-why repository has 59 stars and 3 forks on GitHub."
  },
  {
    "input": "What language is the facebookresearch/segment-anything repository written in?",
    "output": "The Segment Anything implementation is written in Jupyter Notebook."
  },
  {
    "input": "When was the S4: Efficiently Modeling Long Sequences paper published?",
    "output": "The S4: Efficiently Modeling Long Sequences paper was published in 2021."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2312.02120&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2312.02120&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "How many stars does the QLoRA: Efficient Finetuning of Quantized LLMs repository have?",
    "output": "The artidoro/qlora repository has 10,764 stars and 866 forks on GitHub."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2502.14815&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2502.14815&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Explain the tensorflow/tensor2tensor implementation of Generating Wikipedia by Summarizing Long Sequences.",
    "output": "Library of deep learning models and datasets designed to make deep learning more accessible and accelerate ML research."
  },
  {
    "input": "Summarize the Lookahead Optimizer: k steps forward, 1 step back paper.",
    "output": "Lookahead Optimizer: k steps forward, 1 step back is a 2020 paper in Machine Learning. The implementation is available at alphadl/lookahead.pytorch which lookahead optimizer (lookahead optimizer: k steps forward, 1 step back) for pytorch  It is written in Python. The repository has 337 stars, indicating high community interest."
  },
  {
    "input": "Summarize the CLIP: Learning Transferable Visual Models From Natural Language paper.",
    "output": "CLIP: Learning Transferable Visual Models From Natural Language is a 2021 paper in Computer Vision and Pattern Recognition. The implementation is available at openai/CLIP which clip (contrastive language-image pretraining),  predict the most relevant text snippet given an image It is written in Jupyter Notebook. The repository has 31,706 stars, indicating high community interest."
  },
  {
    "input": "What is the GitHub repository for BEiT: BERT Pre-Training of Image Transformers?",
    "output": "The code for BEiT: BERT Pre-Training of Image Transformers is available at https://github.com/microsoft/unilm (microsoft/unilm)."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2502.01061&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2502.01061&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "What paper has ArXiv ID 2312.02120?",
    "output": "arXiv Query: search_query=&id_list=2312.02120&start=0&max_results=10"
  },
  {
    "input": "Explain the DreamBooth: Fine Tuning Text-to-Image Diffusion Models paper.",
    "output": "DreamBooth: Fine Tuning Text-to-Image Diffusion Models is a 2022 paper in Computer Vision and Pattern Recognition. The implementation is available at google/dreambooth The repository has 998 stars, indicating high community interest."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2504.16084&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2504.16084&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "Tell me the title of ArXiv paper 2109.01652.",
    "output": "Finetuned Language Models Are Zero-Shot Learners"
  },
  {
    "input": "What is the GitHub repository for Direct Preference Optimization: Your Language Model is a Reward Model?",
    "output": "The code for Direct Preference Optimization: Your Language Model is a Reward Model is available at https://github.com/eric-mitchell/direct-preference-optimization (eric-mitchell/direct-preference-optimization)."
  },
  {
    "input": "In what year did LLaVA: Large Language and Vision Assistant come out?",
    "output": "The LLaVA: Large Language and Vision Assistant paper was published in 2023."
  },
  {
    "input": "What year was GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers released?",
    "output": "The GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers paper was published in 2022."
  },
  {
    "input": "What does the amazon-science/mm-cot repository do?",
    "output": "Official implementation for \"Multimodal Chain-of-Thought Reasoning in Language Models\" (stay tuned and more will be updated)"
  },
  {
    "input": "What is the GitHub repository for Linformer: Self-Attention with Linear Complexity?",
    "output": "The code for Linformer: Self-Attention with Linear Complexity is available at https://github.com/tatp22/linformer-pytorch (tatp22/linformer-pytorch)."
  },
  {
    "input": "Which field does Dense Passage Retrieval for Open-Domain Question Answering belong to?",
    "output": "Dense Passage Retrieval for Open-Domain Question Answering is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What programming language is Dense Passage Retrieval for Open-Domain Question Answering implemented in?",
    "output": "The Dense Passage Retrieval for Open-Domain Question Answering implementation is written in Python."
  },
  {
    "input": "Which field does S4: Efficiently Modeling Long Sequences belong to?",
    "output": "S4: Efficiently Modeling Long Sequences is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What does the OpenGVLab/LLaMA-Adapter repository do?",
    "output": "[ICLR 2024] Fine-tuning LLaMA to follow Instructions within 1 Hour and 1.2M Parameters"
  },
  {
    "input": "How many stars does the Multi-Task Deep Neural Networks for Natural Language Understanding repository have?",
    "output": "The namisan/mt-dnn repository has 2,258 stars and 414 forks on GitHub."
  },
  {
    "input": "Explain the artidoro/qlora implementation of QLoRA: Efficient Finetuning of Quantized LLMs.",
    "output": "QLoRA: Efficient Finetuning of Quantized LLMs"
  },
  {
    "input": "What are the key topics in the google/automl repository?",
    "output": "The google/automl repository covers: automl, efficientdet, object-detection, efficientnet, efficientnetv2."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2301.04856&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2301.04856&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2505.22101&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2505.22101&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "What paper has ArXiv ID 2302.14838?",
    "output": "arXiv Query: search_query=&id_list=2302.14838&start=0&max_results=10"
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2302.03011&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2302.03011&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2302.00902&start=0&max_results=10 repository have?",
    "output": "The haoliuhl/language-quantized-autoencoders repository has 111 stars and 5 forks on GitHub."
  },
  {
    "input": "What's the implementation repository for Prefix-Tuning: Optimizing Continuous Prompts for Generation?",
    "output": "The code for Prefix-Tuning: Optimizing Continuous Prompts for Generation is available at https://github.com/XiangLi1999/PrefixTuning (XiangLi1999/PrefixTuning)."
  },
  {
    "input": "When was the Neural Message Passing for Quantum Chemistry paper published?",
    "output": "The Neural Message Passing for Quantum Chemistry paper was published in 2017."
  },
  {
    "input": "What programming language is Self-RAG: Learning to Retrieve, Generate, and Critique implemented in?",
    "output": "The Self-RAG: Learning to Retrieve, Generate, and Critique implementation is written in Python."
  },
  {
    "input": "What year was YOLOX: Exceeding YOLO Series in 2021 released?",
    "output": "The YOLOX: Exceeding YOLO Series in 2021 paper was published in 2018."
  },
  {
    "input": "When was the ZeRO: Memory Optimizations Toward Training Trillion Parameter Models paper published?",
    "output": "The ZeRO: Memory Optimizations Toward Training Trillion Parameter Models paper was published in 2019."
  },
  {
    "input": "What language is the tensorflow/tensor2tensor repository written in?",
    "output": "The Generating Wikipedia by Summarizing Long Sequences implementation is written in Python."
  },
  {
    "input": "What category is the Wide & Deep Learning for Recommender Systems paper in?",
    "output": "Wide & Deep Learning for Recommender Systems is in the Information Retrieval (cs.IR) category."
  },
  {
    "input": "What language is the autonomousvision/stylegan-t repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2301.11305&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2504.19413&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2504.19413&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "Give me an overview of Prefix-Tuning: Optimizing Continuous Prompts for Generation.",
    "output": "Prefix-Tuning: Optimizing Continuous Prompts for Generation is a 2021 paper in Computation and Language (Natural Language Processing). The implementation is available at XiangLi1999/PrefixTuning which prefix-tuning: optimizing continuous prompts for generation It is written in Python. The repository has 954 stars, indicating high community interest."
  },
  {
    "input": "What is the ArXiv category of Direct Preference Optimization: Your Language Model is a Reward Model?",
    "output": "Direct Preference Optimization: Your Language Model is a Reward Model is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What topics does the Multi-Task Deep Neural Networks for Natural Language Understanding implementation cover?",
    "output": "The namisan/mt-dnn repository covers: multi-task-learning, natural-language-understanding, deep-learning, microsoft, ranking, named-entity-recognition, bert, machine-reading-comprehension, nlp, pytorch."
  },
  {
    "input": "Tell me the title of ArXiv paper 2004.03270.",
    "output": "Analyzing the Structure of Attention in a Transformer Language Model"
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2303.09431&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2303.09431&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What is AlpacaEval: An Automatic Evaluator for Instruction-following about?",
    "output": "AlpacaEval: An Automatic Evaluator for Instruction-following is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at tatsu-lab/alpaca_eval which an automatic evaluator for instruction-following language models. human-validated, high-quality, cheap, and fast. It is written in Jupyter Notebook. The repository has 1,907 stars, indicating high community interest."
  },
  {
    "input": "Explain the oughtinc/ice implementation of arXiv Query: search_query=&id_list=2301.01751&start=0&max_results=10.",
    "output": "Interactive Composition Explorer: a debugger for compositional language model programs"
  },
  {
    "input": "Where can I find the code for ZeRO: Memory Optimizations Toward Training Trillion Parameter Models?",
    "output": "The code for ZeRO: Memory Optimizations Toward Training Trillion Parameter Models is available at https://github.com/deepspeedai/DeepSpeed (deepspeedai/DeepSpeed)."
  },
  {
    "input": "What is the GitHub repository for FlashAttention-2: Faster Attention with Better Parallelism?",
    "output": "The code for FlashAttention-2: Faster Attention with Better Parallelism is available at https://github.com/Dao-AILab/flash-attention (Dao-AILab/flash-attention)."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2104.08691?",
    "output": "Prefix-Tuning: Optimizing Continuous Prompts for Generation"
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2501.13545&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2501.13545&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2302.10866&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2302.10866&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2302.12766&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2302.12766&start=0&max_results=10 is available at https://github.com/siddk/voltron-robotics (siddk/voltron-robotics)."
  },
  {
    "input": "What year was Codex: Evaluating Large Language Models Trained on Code released?",
    "output": "The Codex: Evaluating Large Language Models Trained on Code paper was published in 2021."
  },
  {
    "input": "Summarize the Neural Message Passing for Quantum Chemistry paper.",
    "output": "Neural Message Passing for Quantum Chemistry is a 2017 paper in Machine Learning. The implementation is available at priba/nmp_qc which our own implementation of neural message passing for computer vision paper It is written in Python. The repository has 343 stars, indicating high community interest."
  },
  {
    "input": "Summarize the Codex: Evaluating Large Language Models Trained on Code paper.",
    "output": "Codex: Evaluating Large Language Models Trained on Code is a 2021 paper in Machine Learning. The implementation is available at openai/human-eval which code for the paper \"evaluating large language models trained on code\" It is written in Python. The repository has 3,030 stars, indicating high community interest."
  },
  {
    "input": "What is Zephyr: Direct Distillation of LM Alignment about?",
    "output": "Zephyr: Direct Distillation of LM Alignment is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at huggingface/alignment-handbook which robust recipes to align language models with human and ai preferences It is written in Python. The repository has 5,429 stars, indicating high community interest."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2302.14233&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2302.14233&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What language is the mit-han-lab/llm-awq repository written in?",
    "output": "The AWQ: Activation-aware Weight Quantization for LLM Compression implementation is written in Python."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2302.09778&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2302.09778&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at ali-vilab/composer which official implementation of \"composer: creative and controllable image synthesis with composable conditions\" The repository has 1,560 stars, indicating high community interest."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2502.09992&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2502.09992&start=0&max_results=10 is available at https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero (Open-Reasoner-Zero/Open-Reasoner-Zero)."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2301.00704&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2301.00704&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at lucidrains/muse-maskgit-pytorch which implementation of muse: text-to-image generation via masked generative transformers, in pytorch It is written in Python. The repository has 916 stars, indicating high community interest."
  },
  {
    "input": "In what year did Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks come out?",
    "output": "The Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks paper was published in 2019."
  },
  {
    "input": "How many stars does the LLaMA: Open and Efficient Foundation Language Models repository have?",
    "output": "The meta-llama/llama repository has 58,946 stars and 9,817 forks on GitHub."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2405.04324&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2405.04324&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2303.01469&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2303.01469&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Explain the XiangLi1999/PrefixTuning implementation of Prefix-Tuning: Optimizing Continuous Prompts for Generation.",
    "output": "Prefix-Tuning: Optimizing Continuous Prompts for Generation"
  },
  {
    "input": "What topics does the Generating Wikipedia by Summarizing Long Sequences implementation cover?",
    "output": "The tensorflow/tensor2tensor repository covers: machine-learning, machine-translation, deep-learning, reinforcement-learning, tpu."
  },
  {
    "input": "What year was JAX: Composable transformations of Python+NumPy programs released?",
    "output": "The JAX: Composable transformations of Python+NumPy programs paper was published in 2019."
  },
  {
    "input": "Which field does Semi-Supervised Classification with Graph Convolutional Networks belong to?",
    "output": "Semi-Supervised Classification with Graph Convolutional Networks is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Summarize the Gorilla: Large Language Model Connected with Massive APIs paper.",
    "output": "Gorilla: Large Language Model Connected with Massive APIs is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at ShishirPatil/gorilla which gorilla: training and evaluating llms for function calls (tool calls) It is written in Python. The repository has 12,574 stars, indicating high community interest."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2303.04226&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2303.04226&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at HazyResearch/safari which convolutions for sequence modeling It is written in Assembly. The repository has 904 stars, indicating high community interest."
  },
  {
    "input": "What category is the Universal Language Model Fine-tuning for Text Classification paper in?",
    "output": "Universal Language Model Fine-tuning for Text Classification is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What paper has ArXiv ID 2303.11366?",
    "output": "Reflexion: Language Agents with Verbal Reinforcement Learning"
  },
  {
    "input": "Summarize the ELECTRA: Pre-training Text Encoders as Discriminators paper.",
    "output": "ELECTRA: Pre-training Text Encoders as Discriminators is a 2019 paper in Computation and Language (Natural Language Processing). The implementation is available at google-research/electra which electra: pre-training text encoders as discriminators rather than generators It is written in Python. The repository has 2,365 stars, indicating high community interest."
  },
  {
    "input": "What topics does the PyTorch Geometric: Library for Geometric Deep Learning implementation cover?",
    "output": "The pyg-team/pytorch_geometric repository covers: pytorch, geometric-deep-learning, graph-neural-networks, deep-learning, graph-convolutional-networks."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2405.03548&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2405.03548&start=0&max_results=10 is available at https://github.com/ibm-granite/granite-code-models (ibm-granite/granite-code-models)."
  },
  {
    "input": "What paper has ArXiv ID 2305.06500?",
    "output": "InstructBLIP: Towards General-purpose Vision-Language Models"
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2301.02828&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2301.02828&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What is the ArXiv category of TensorFlow: A System for Large-Scale Machine Learning?",
    "output": "TensorFlow: A System for Large-Scale Machine Learning is in the Distributed, Parallel, and Cluster Computing (cs.DC) category."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2012.06009&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2012.06009&start=0&max_results=10 is available at https://github.com/facebookresearch/CovidPrognosis (facebookresearch/CovidPrognosis)."
  },
  {
    "input": "What is the ArXiv category of QLoRA: Efficient Finetuning of Quantized LLMs?",
    "output": "QLoRA: Efficient Finetuning of Quantized LLMs is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2304.10592?",
    "output": "LLaVA: Large Language and Vision Assistant"
  },
  {
    "input": "What is Depthwise Separable Convolutions for Neural Machine Translation about?",
    "output": "Depthwise Separable Convolutions for Neural Machine Translation is a 2017 paper in Computation and Language (Natural Language Processing). The implementation is available at google-research/google-research which google research It is written in Jupyter Notebook. The repository has 36,800 stars, indicating high community interest."
  },
  {
    "input": "When was the Reflexion: Language Agents with Verbal Reinforcement Learning paper published?",
    "output": "The Reflexion: Language Agents with Verbal Reinforcement Learning paper was published in 2023."
  },
  {
    "input": "What is the GitHub repository for TensorFlow: A System for Large-Scale Machine Learning?",
    "output": "The code for TensorFlow: A System for Large-Scale Machine Learning is available at https://github.com/tensorflow/tensorflow (tensorflow/tensorflow)."
  },
  {
    "input": "Give me an overview of Beir: A Heterogeneous Benchmark for IR.",
    "output": "Beir: A Heterogeneous Benchmark for IR is a 2022 paper in Information Retrieval. The implementation is available at beir-cellar/beir which a heterogeneous benchmark for information retrieval. easy to use, evaluate your models across 15+ diverse ir datasets. It is written in Python. The repository has 2,007 stars, indicating high community interest."
  },
  {
    "input": "What language is the alphadl/lookahead.pytorch repository written in?",
    "output": "The Lookahead Optimizer: k steps forward, 1 step back implementation is written in Python."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2404.03592&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2404.03592&start=0&max_results=10 is a 2024 paper in Machine Learning. The implementation is available at OpenBMB/Eurus It is written in Python. The repository has 320 stars, indicating high community interest."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2504.02732&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2504.02732&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "What does the LTH14/fractalgen repository do?",
    "output": "PyTorch implementation of FractalGen https://arxiv.org/abs/2502.17437"
  },
  {
    "input": "Where can I find the code for GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers?",
    "output": "The code for GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers is available at https://github.com/IST-DASLab/gptq (IST-DASLab/gptq)."
  },
  {
    "input": "What paper has ArXiv ID 2006.16779?",
    "output": "arXiv Query: search_query=&id_list=2006.16779&start=0&max_results=10"
  },
  {
    "input": "What year was Med-PaLM 2: Towards Expert-Level Medical Question Answering released?",
    "output": "The Med-PaLM 2: Towards Expert-Level Medical Question Answering paper was published in 2023."
  },
  {
    "input": "Where can I find the code for Mamba: Linear-Time Sequence Modeling with Selective State Spaces?",
    "output": "The code for Mamba: Linear-Time Sequence Modeling with Selective State Spaces is available at https://github.com/state-spaces/mamba (state-spaces/mamba)."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2504.21801&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2504.21801&start=0&max_results=10 is available at https://github.com/MoonshotAI/Kimi-Audio (MoonshotAI/Kimi-Audio)."
  },
  {
    "input": "What programming language is ControlNet: Adding Conditional Control to Text-to-Image Diffusion implemented in?",
    "output": "The ControlNet: Adding Conditional Control to Text-to-Image Diffusion implementation is written in Python."
  },
  {
    "input": "What is the ArXiv category of Know What You Don't Know: Unanswerable Questions for SQuAD?",
    "output": "Know What You Don't Know: Unanswerable Questions for SQuAD is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2302.12766&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2302.12766&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at siddk/voltron-robotics which voltron: language-driven representation learning for robotics It is written in Python. The repository has 227 stars, indicating high community interest."
  },
  {
    "input": "What paper has ArXiv ID 1906.04341?",
    "output": "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks"
  },
  {
    "input": "What are the key topics in the mit-han-lab/offsite-tuning repository?",
    "output": "The mit-han-lab/offsite-tuning repository covers: deep-learning, transfer-learning."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2302.07459&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2302.07459&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What are the key topics in the princeton-nlp/SimCSE repository?",
    "output": "The princeton-nlp/SimCSE repository covers: nlp, sentence-embeddings."
  },
  {
    "input": "What paper has ArXiv ID 2108.12409?",
    "output": "Linformer: Self-Attention with Linear Complexity"
  },
  {
    "input": "In what year did XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale come out?",
    "output": "The XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale paper was published in 2021."
  },
  {
    "input": "What is Analyzing the Structure of Attention in a Transformer Language Model about?",
    "output": "Analyzing the Structure of Attention in a Transformer Language Model is a 2020 paper in Computation and Language (Natural Language Processing). The implementation is available at clarkkev/attention-analysis It is written in Jupyter Notebook. The repository has 469 stars, indicating high community interest."
  },
  {
    "input": "What is the ArXiv category of Horovod: fast and easy distributed deep learning in TensorFlow?",
    "output": "Horovod: fast and easy distributed deep learning in TensorFlow is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "In what year did Mistral 7B come out?",
    "output": "The Mistral 7B paper was published in 2023."
  },
  {
    "input": "What is the GitHub repository for GPT-4 Technical Report?",
    "output": "The code for GPT-4 Technical Report is available at https://github.com/openai/evals (openai/evals)."
  },
  {
    "input": "What language is the IST-DASLab/gptq repository written in?",
    "output": "The GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers implementation is written in Python."
  },
  {
    "input": "What paper has ArXiv ID 2410.18050?",
    "output": "arXiv Query: search_query=&id_list=2410.18050&start=0&max_results=10"
  },
  {
    "input": "What category is the Beir: A Heterogeneous Benchmark for IR paper in?",
    "output": "Beir: A Heterogeneous Benchmark for IR is in the Information Retrieval (cs.IR) category."
  },
  {
    "input": "Explain the tatsu-lab/alpaca_eval implementation of AlpacaEval: An Automatic Evaluator for Instruction-following.",
    "output": "An automatic evaluator for instruction-following language models. Human-validated, high-quality, cheap, and fast."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2303.02913&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2303.02913&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at Shark-NLP/OpenICL which openicl is an open-source framework to facilitate research, development, and prototyping of in-context learning. It is written in Python. The repository has 579 stars, indicating high community interest."
  },
  {
    "input": "What is Med-PaLM 2: Towards Expert-Level Medical Question Answering about?",
    "output": "Med-PaLM 2: Towards Expert-Level Medical Question Answering is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at google-research/google-research which google research It is written in Jupyter Notebook. The repository has 36,800 stars, indicating high community interest."
  },
  {
    "input": "What programming language is The Power of Scale for Parameter-Efficient Prompt Tuning implemented in?",
    "output": "The The Power of Scale for Parameter-Efficient Prompt Tuning implementation is written in Python."
  },
  {
    "input": "Give me an overview of QLoRA: Efficient Finetuning of Quantized LLMs.",
    "output": "QLoRA: Efficient Finetuning of Quantized LLMs is a 2023 paper in Machine Learning. The implementation is available at artidoro/qlora which qlora: efficient finetuning of quantized llms It is written in Jupyter Notebook. The repository has 10,764 stars, indicating high community interest."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2301.00808&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2301.00808&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2302.03917&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2302.03917&start=0&max_results=10 is available at https://github.com/mit-han-lab/offsite-tuning (mit-han-lab/offsite-tuning)."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2503.22708&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2503.22708&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Where can I find the code for Synthetic Data Generation with Large Language Models?",
    "output": "The code for Synthetic Data Generation with Large Language Models is available at https://github.com/google-research/google-research (google-research/google-research)."
  },
  {
    "input": "How many stars does the DreamBooth: Fine Tuning Text-to-Image Diffusion Models repository have?",
    "output": "The google/dreambooth repository has 998 stars and 91 forks on GitHub."
  },
  {
    "input": "What is Segment Anything about?",
    "output": "Segment Anything is a 2023 paper in Computer Vision and Pattern Recognition. The implementation is available at facebookresearch/segment-anything which the repository provides code for running inference with the segmentanything model (sam), links for downloading the trained model checkpoints, and example notebooks that show how to use the model. It is written in Jupyter Notebook. The repository has 52,657 stars, indicating high community interest."
  },
  {
    "input": "Which field does Flamingo: A Visual Language Model for Few-Shot Learning belong to?",
    "output": "Flamingo: A Visual Language Model for Few-Shot Learning is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "What does the openai/evals repository do?",
    "output": "Evals is a framework for evaluating LLMs and LLM systems, and an open-source registry of benchmarks."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2406.06326&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2406.06326&start=0&max_results=10 is available at https://github.com/lamini-ai/Lamini-Memory-Tuning (lamini-ai/Lamini-Memory-Tuning)."
  },
  {
    "input": "In what year did High-Resolution Image Synthesis with Latent Diffusion Models come out?",
    "output": "The High-Resolution Image Synthesis with Latent Diffusion Models paper was published in 2021."
  },
  {
    "input": "What is Time-LLM: Time Series Forecasting by Reprogramming Large Language Models about?",
    "output": "Time-LLM: Time Series Forecasting by Reprogramming Large Language Models is a 2023 paper in Machine Learning. The implementation is available at KimMeen/Time-LLM which [iclr 2024] official implementation of \" \ud83e\udd99 time-llm: time series forecasting by reprogramming large language models\" It is written in Python. The repository has 2,377 stars, indicating high community interest."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2301.01181&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2301.01181&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What does the facebookresearch/fairseq repository do?",
    "output": "Facebook AI Research Sequence-to-Sequence Toolkit written in Python."
  },
  {
    "input": "Give me an overview of Billion-scale similarity search with GPUs.",
    "output": "Billion-scale similarity search with GPUs is a 2019 paper in Computer Vision and Pattern Recognition. The implementation is available at facebookresearch/faiss which a library for efficient similarity search and clustering of dense vectors. It is written in C++. The repository has 38,143 stars, indicating high community interest."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2303.01469&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2303.01469&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "Which field does Wav2Vec 2.0: A Framework for Self-Supervised Learning of Speech belong to?",
    "output": "Wav2Vec 2.0: A Framework for Self-Supervised Learning of Speech is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What are the key topics in the namisan/mt-dnn repository?",
    "output": "The namisan/mt-dnn repository covers: multi-task-learning, natural-language-understanding, deep-learning, microsoft, ranking, named-entity-recognition, bert, machine-reading-comprehension, nlp, pytorch."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2301.00704&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2301.00704&start=0&max_results=10 is available at https://github.com/lucidrains/muse-maskgit-pytorch (lucidrains/muse-maskgit-pytorch)."
  },
  {
    "input": "Tell me the title of ArXiv paper 2302.04023.",
    "output": "arXiv Query: search_query=&id_list=2302.04023&start=0&max_results=10"
  },
  {
    "input": "What paper has ArXiv ID 2004.13637?",
    "output": "arXiv Query: search_query=&id_list=2004.13637&start=0&max_results=10"
  },
  {
    "input": "Which field does ELECTRA: Pre-training Text Encoders as Discriminators belong to?",
    "output": "ELECTRA: Pre-training Text Encoders as Discriminators is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "How many stars does the Vicuna: An Open-Source Chatbot repository have?",
    "output": "The lm-sys/FastChat repository has 39,271 stars and 4,772 forks on GitHub."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2502.19328&start=0&max_results=10 implementation?",
    "output": "The LTH14/fractalgen repository has 1,196 stars and 65 forks on GitHub."
  },
  {
    "input": "What topics does the arXiv Query: search_query=&id_list=2012.06009&start=0&max_results=10 implementation cover?",
    "output": "The facebookresearch/CovidPrognosis repository covers: medical-imaging, deep-learning, radiography, x-ray, pytorch, covid-19, medical-image-analysis."
  },
  {
    "input": "Where can I find the code for DreamFusion: Text-to-3D using 2D Diffusion?",
    "output": "The code for DreamFusion: Text-to-3D using 2D Diffusion is available at https://github.com/ashawkey/stable-dreamfusion (ashawkey/stable-dreamfusion)."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2405.02803&start=0&max_results=10 repository have?",
    "output": "The ibm-granite/granite-code-models repository has 1,240 stars and 88 forks on GitHub."
  },
  {
    "input": "Tell me the title of ArXiv paper 2307.08691.",
    "output": "FlashAttention-2: Faster Attention with Better Parallelism"
  },
  {
    "input": "In what year did SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities come out?",
    "output": "The SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities paper was published in 2023."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2004.08492&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2004.08492&start=0&max_results=10 is a 2020 paper in Machine Learning. The implementation is available at uber/orbit which a python package for bayesian forecasting with object-oriented design and probabilistic models under the hood. It is written in Python. The repository has 2,019 stars, indicating high community interest."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2302.07842?",
    "output": "The Flan Collection: Designing Data and Methods for Effective Instruction Tuning"
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2301.00808&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2301.00808&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at facebookresearch/ConvNeXt-V2 which code release for convnext v2 model It is written in Python. The repository has 1,890 stars, indicating high community interest."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2501.11067&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2501.11067&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What category is the Llama 2: Open Foundation and Fine-Tuned Chat Models paper in?",
    "output": "Llama 2: Open Foundation and Fine-Tuned Chat Models is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Summarize the Dream to Control: Learning Behaviors by Latent Imagination paper.",
    "output": "Dream to Control: Learning Behaviors by Latent Imagination is a 2019 paper in Machine Learning. The implementation is available at danijar/dreamer which dream to control: learning behaviors by latent imagination It is written in Python. The repository has 565 stars, indicating high community interest."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2501.19393&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2501.19393&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What topics does the Gorilla: Large Language Model Connected with Massive APIs implementation cover?",
    "output": "The ShishirPatil/gorilla repository covers: api, llm, api-documentation, chatgpt, gpt-4-api, claude-api, openai-api, openai-functions."
  },
  {
    "input": "What does the ShishirPatil/gorilla repository do?",
    "output": "Gorilla: Training and Evaluating LLMs for Function Calls (Tool Calls)"
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2503.01141&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2503.01141&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Summarize the AlpacaEval: An Automatic Evaluator for Instruction-following paper.",
    "output": "AlpacaEval: An Automatic Evaluator for Instruction-following is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at tatsu-lab/alpaca_eval which an automatic evaluator for instruction-following language models. human-validated, high-quality, cheap, and fast. It is written in Jupyter Notebook. The repository has 1,907 stars, indicating high community interest."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2312.02783&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2312.02783&start=0&max_results=10 is available at https://github.com/ContextualAI/HALOs (ContextualAI/HALOs)."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2406.06326&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2406.06326&start=0&max_results=10 paper was published in 2024."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2301.00808&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2301.00808&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "Summarize the T5: Exploring the Limits of Transfer Learning paper.",
    "output": "T5: Exploring the Limits of Transfer Learning is a 2019 paper in Computation and Language (Natural Language Processing). The implementation is available at google-research/text-to-text-transfer-transformer which code for the paper \"exploring the limits of transfer learning with a unified text-to-text transformer\" It is written in Python. The repository has 6,453 stars, indicating high community interest."
  },
  {
    "input": "Give me an overview of Question and Answer Test-Train Overlap in Open-Domain QA Datasets.",
    "output": "Question and Answer Test-Train Overlap in Open-Domain QA Datasets is a 2020 paper in Computation and Language (Natural Language Processing). The implementation is available at google-research-datasets/natural-questions which natural questions (nq) contains real user questions issued to google search, and answers found from wikipedia by annotators. nq is designed for the training and evaluation of automatic question answering systems. It is written in Python. The repository has 1,074 stars, indicating high community interest."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2503.01141&start=0&max_results=10 repository have?",
    "output": "The LTH14/fractalgen repository has 1,196 stars and 65 forks on GitHub."
  },
  {
    "input": "Describe the microsoft/JARVIS repository that implements HuggingGPT: Solving AI Tasks with ChatGPT.",
    "output": "JARVIS, a system to connect LLMs with ML community. Paper: https://arxiv.org/pdf/2303.17580.pdf"
  },
  {
    "input": "What category is the Reflexion: Language Agents with Verbal Reinforcement Learning paper in?",
    "output": "Reflexion: Language Agents with Verbal Reinforcement Learning is in the Artificial Intelligence (cs.AI) category."
  },
  {
    "input": "How popular is the Reflexion: Language Agents with Verbal Reinforcement Learning implementation?",
    "output": "The noahshinn/reflexion repository has 2,959 stars and 285 forks on GitHub."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2205.05198?",
    "output": "PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel"
  },
  {
    "input": "How many stars does the Time-LLM: Time Series Forecasting by Reprogramming Large Language Models repository have?",
    "output": "The KimMeen/Time-LLM repository has 2,377 stars and 416 forks on GitHub."
  },
  {
    "input": "How popular is the LLaVA-1.5: Improved Baselines with Visual Instruction Tuning implementation?",
    "output": "The haotian-liu/LLaVA repository has 24,034 stars and 2,665 forks on GitHub."
  },
  {
    "input": "Explain the Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks paper.",
    "output": "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks is a 2019 paper in Computation and Language (Natural Language Processing). The implementation is available at huggingface/sentence-transformers which state-of-the-art text embeddings It is written in Python. The repository has 17,904 stars, indicating high community interest."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2303.03378&start=0&max_results=10 repository have?",
    "output": "The NVlabs/prismer repository has 1,307 stars and 74 forks on GitHub."
  },
  {
    "input": "What programming language is T5: Exploring the Limits of Transfer Learning implemented in?",
    "output": "The T5: Exploring the Limits of Transfer Learning implementation is written in Python."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2303.04226&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2303.04226&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Explain the Wide & Deep Learning for Recommender Systems paper.",
    "output": "Wide & Deep Learning for Recommender Systems is a 2016 paper in Information Retrieval. The implementation is available at tensorflow/models which models and examples built with tensorflow It is written in Python. The repository has 77,677 stars, indicating high community interest."
  },
  {
    "input": "Where can I find the code for BERT Rediscovers the Classical NLP Pipeline?",
    "output": "The code for BERT Rediscovers the Classical NLP Pipeline is available at https://github.com/clarkkev/attention-analysis (clarkkev/attention-analysis)."
  },
  {
    "input": "What's the implementation repository for Masked Autoencoders Are Scalable Vision Learners?",
    "output": "The code for Masked Autoencoders Are Scalable Vision Learners is available at https://github.com/facebookresearch/mae (facebookresearch/mae)."
  },
  {
    "input": "Explain the facebookresearch/fairseq implementation of XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale.",
    "output": "Facebook AI Research Sequence-to-Sequence Toolkit written in Python."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2302.05206&start=0&max_results=10 repository have?",
    "output": "The tianjunz/HIR repository has 159 stars and 11 forks on GitHub."
  },
  {
    "input": "What year was T5: Exploring the Limits of Transfer Learning released?",
    "output": "The T5: Exploring the Limits of Transfer Learning paper was published in 2019."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2503.00735&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2503.00735&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at LTH14/fractalgen which pytorch implementation of fractalgen https://arxiv.org/abs/2502.17437 It is written in Python. The repository has 1,196 stars, indicating high community interest."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2004.04906?",
    "output": "Dense Passage Retrieval for Open-Domain Question Answering"
  },
  {
    "input": "How many stars does the BERT Rediscovers the Classical NLP Pipeline repository have?",
    "output": "The clarkkev/attention-analysis repository has 469 stars and 82 forks on GitHub."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2410.12896&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2410.12896&start=0&max_results=10 paper was published in 2024."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2302.12766&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2302.12766&start=0&max_results=10 is available at https://github.com/siddk/voltron-robotics (siddk/voltron-robotics)."
  },
  {
    "input": "When was the Multi-Task Deep Neural Networks for Natural Language Understanding paper published?",
    "output": "The Multi-Task Deep Neural Networks for Natural Language Understanding paper was published in 2021."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2301.13688&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2301.13688&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What's the implementation repository for AudioPaLM: A Large Language Model for Speech Understanding?",
    "output": "The code for AudioPaLM: A Large Language Model for Speech Understanding is available at https://github.com/lucidrains/audiolm-pytorch (lucidrains/audiolm-pytorch)."
  },
  {
    "input": "What programming language is Lookahead Optimizer: k steps forward, 1 step back implemented in?",
    "output": "The Lookahead Optimizer: k steps forward, 1 step back implementation is written in Python."
  },
  {
    "input": "Explain the Text Embeddings by Weakly-Supervised Contrastive Pre-training paper.",
    "output": "Text Embeddings by Weakly-Supervised Contrastive Pre-training is a 2022 paper in Computation and Language (Natural Language Processing). The implementation is available at microsoft/unilm which large-scale self-supervised pre-training across tasks, languages, and modalities It is written in Python. The repository has 21,848 stars, indicating high community interest."
  },
  {
    "input": "How many stars does the GPT-3: Language Models are Few-Shot Learners repository have?",
    "output": "The openai/gpt-3 repository has 15,784 stars and 2,294 forks on GitHub."
  },
  {
    "input": "What language is the google/trax repository written in?",
    "output": "The Reformer: The Efficient Transformer implementation is written in Python."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2503.01141&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2503.01141&start=0&max_results=10 is available at https://github.com/LTH14/fractalgen (LTH14/fractalgen)."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2104.00355&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2104.00355&start=0&max_results=10 is a 2021 paper in Machine Learning. The implementation is available at facebookresearch/fairseq which facebook ai research sequence-to-sequence toolkit written in python. It is written in Python. The repository has 31,988 stars, indicating high community interest."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2502.14815&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2502.14815&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at Open-Reasoner-Zero/Open-Reasoner-Zero which official repo for open-reasoner-zero It is written in Python. The repository has 2,068 stars, indicating high community interest."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2301.10343&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2301.10343&start=0&max_results=10 is available at https://github.com/autonomousvision/stylegan-t (autonomousvision/stylegan-t)."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2405.03548&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2405.03548&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What programming language is WizardCoder: Empowering Code LLMs with Evol-Instruct implemented in?",
    "output": "The WizardCoder: Empowering Code LLMs with Evol-Instruct implementation is written in Python."
  },
  {
    "input": "How many stars does the Segment Anything repository have?",
    "output": "The facebookresearch/segment-anything repository has 52,657 stars and 6,157 forks on GitHub."
  },
  {
    "input": "What does the langchain-ai/langchain repository do?",
    "output": "\ud83e\udd9c\ud83d\udd17 The platform for reliable agents."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2504.00698&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2504.00698&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at allenai/codescientist which codescientist: an automated scientific discovery system for code-based experiments It is written in Python. The repository has 301 stars, indicating high community interest."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2505.22101&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2505.22101&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at ruixin31/Spurious_Rewards It is written in Python. The repository has 342 stars, indicating high community interest."
  },
  {
    "input": "What is the ArXiv category of Distilling the Knowledge in a Neural Network?",
    "output": "Distilling the Knowledge in a Neural Network is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Tell me the title of ArXiv paper 2104.09864.",
    "output": "RoFormer: Enhanced Transformer with Rotary Position Embedding"
  },
  {
    "input": "How popular is the GPT-4 Technical Report implementation?",
    "output": "The openai/evals repository has 17,349 stars and 2,848 forks on GitHub."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2405.03548?",
    "output": "arXiv Query: search_query=&id_list=2405.03548&start=0&max_results=10"
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2301.01181&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2301.01181&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What are the key topics in the jasonwei20/eda_nlp repository?",
    "output": "The jasonwei20/eda_nlp repository covers: nlp, data-augmentation, text-classification, synonyms, embeddings, sentence, classification, rnn, cnn, swap, position."
  },
  {
    "input": "What year was ColBERT: Efficient Passage Search via Contextualized Late Interaction released?",
    "output": "The ColBERT: Efficient Passage Search via Contextualized Late Interaction paper was published in 2021."
  },
  {
    "input": "What category is the Code Llama: Open Foundation Models for Code paper in?",
    "output": "Code Llama: Open Foundation Models for Code is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What's the implementation repository for AWQ: Activation-aware Weight Quantization for LLM Compression?",
    "output": "The code for AWQ: Activation-aware Weight Quantization for LLM Compression is available at https://github.com/mit-han-lab/llm-awq (mit-han-lab/llm-awq)."
  },
  {
    "input": "What language is the microsoft/Swin-Transformer repository written in?",
    "output": "The Swin Transformer: Hierarchical Vision Transformer using Shifted Windows implementation is written in Python."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2301.02828&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2301.02828&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at frankxu2004/knnlm-why which repo for icml23 \"why do nearest neighbor language models work?\" It is written in Python. The repository has 59 stars, indicating high community interest."
  },
  {
    "input": "Explain the state-spaces/s4 implementation of S4: Efficiently Modeling Long Sequences.",
    "output": "Structured state space sequence models"
  },
  {
    "input": "What does the HumanAIGC/AnimateAnyone repository do?",
    "output": "Animate Anyone: Consistent and Controllable Image-to-Video Synthesis for Character Animation"
  },
  {
    "input": "What does the facebookresearch/fastText repository do?",
    "output": "Library for fast text representation and classification."
  },
  {
    "input": "What is Longformer: The Long-Document Transformer about?",
    "output": "Longformer: The Long-Document Transformer is a 2020 paper in Computation and Language (Natural Language Processing). The implementation is available at allenai/longformer which longformer: the long-document transformer It is written in Python. The repository has 2,175 stars, indicating high community interest."
  },
  {
    "input": "What programming language is Llama 2: Open Foundation and Fine-Tuned Chat Models implemented in?",
    "output": "The Llama 2: Open Foundation and Fine-Tuned Chat Models implementation is written in Python."
  },
  {
    "input": "Describe the haoliuhl/language-quantized-autoencoders repository that implements arXiv Query: search_query=&id_list=2302.08242&start=0&max_results=10.",
    "output": "Language Quantized AutoEncoders"
  },
  {
    "input": "What is the ArXiv category of Linformer: Self-Attention with Linear Complexity?",
    "output": "Linformer: Self-Attention with Linear Complexity is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is Animate Anyone: Consistent and Controllable Image-to-Video Synthesis about?",
    "output": "Animate Anyone: Consistent and Controllable Image-to-Video Synthesis is a 2023 paper in Computer Vision and Pattern Recognition. The implementation is available at HumanAIGC/AnimateAnyone which animate anyone: consistent and controllable image-to-video synthesis for character animation The repository has 14,797 stars, indicating high community interest."
  },
  {
    "input": "Tell me the title of ArXiv paper 2410.18050.",
    "output": "arXiv Query: search_query=&id_list=2410.18050&start=0&max_results=10"
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2406.06326&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2406.06326&start=0&max_results=10 paper was published in 2024."
  },
  {
    "input": "Which field does Feature Pyramid Networks for Object Detection belong to?",
    "output": "Feature Pyramid Networks for Object Detection is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2501.11067&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2501.11067&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2101.04909&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2101.04909&start=0&max_results=10 is a 2021 paper in Machine Learning. The implementation is available at facebookresearch/CovidPrognosis which covid deterioration prediction based on chest x-ray radiographs via moco-trained image representations It is written in Python. The repository has 158 stars, indicating high community interest."
  },
  {
    "input": "What's the implementation repository for Llama 2: Open Foundation and Fine-Tuned Chat Models?",
    "output": "The code for Llama 2: Open Foundation and Fine-Tuned Chat Models is available at https://github.com/meta-llama/llama (meta-llama/llama)."
  },
  {
    "input": "How many stars does the StarCoder: May the source be with you! repository have?",
    "output": "The bigcode-project/starcoder repository has 7,473 stars and 530 forks on GitHub."
  },
  {
    "input": "Which field does Distilling the Knowledge in a Neural Network belong to?",
    "output": "Distilling the Knowledge in a Neural Network is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is the ArXiv category of LLaMA: Open and Efficient Foundation Language Models?",
    "output": "LLaMA: Open and Efficient Foundation Language Models is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What programming language is PyTorch: An Imperative Style, High-Performance Deep Learning Library implemented in?",
    "output": "The PyTorch: An Imperative Style, High-Performance Deep Learning Library implementation is written in Python."
  },
  {
    "input": "Explain the plurai-ai/intellagent implementation of arXiv Query: search_query=&id_list=2501.13545&start=0&max_results=10.",
    "output": "A framework for comprehensive diagnosis and optimization of agents using simulated, realistic synthetic interactions"
  },
  {
    "input": "When was the Finetuned Language Models Are Zero-Shot Learners paper published?",
    "output": "The Finetuned Language Models Are Zero-Shot Learners paper was published in 2021."
  },
  {
    "input": "When was the PyTorch: An Imperative Style, High-Performance Deep Learning Library paper published?",
    "output": "The PyTorch: An Imperative Style, High-Performance Deep Learning Library paper was published in 2019."
  },
  {
    "input": "Describe the MAXNORM8650/MedAgentSim repository that implements arXiv Query: search_query=&id_list=2503.23513&start=0&max_results=10.",
    "output": "MedAgentSim: Self-Evolving Multi-Agent Simulations for Realistic Clinical Interactions, MICCAI 2025 (oral and early accepted)"
  },
  {
    "input": "Tell me the title of ArXiv paper 2305.15334.",
    "output": "Gorilla: Large Language Model Connected with Massive APIs"
  },
  {
    "input": "What is Neural Machine Translation by Jointly Learning to Align and Translate about?",
    "output": "Neural Machine Translation by Jointly Learning to Align and Translate is a 2015 paper in Computation and Language (Natural Language Processing). The implementation is available at lisa-groundhog/GroundHog which library for implementing rnns with theano It is written in Python. The repository has 613 stars, indicating high community interest."
  },
  {
    "input": "What category is the Synthetic Data Generation with Large Language Models paper in?",
    "output": "Synthetic Data Generation with Large Language Models is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2301.00704?",
    "output": "arXiv Query: search_query=&id_list=2301.00704&start=0&max_results=10"
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2302.09778&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2302.09778&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2502.12115&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2502.12115&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is the ArXiv category of Learning to summarize from human feedback?",
    "output": "Learning to summarize from human feedback is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What category is the Wav2Vec 2.0: A Framework for Self-Supervised Learning of Speech paper in?",
    "output": "Wav2Vec 2.0: A Framework for Self-Supervised Learning of Speech is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2502.19328&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2502.19328&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2504.16084&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2504.16084&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "Describe the hendrycks/test repository that implements MMLU: Measuring Massive Multitask Language Understanding.",
    "output": "Measuring Massive Multitask Language Understanding | ICLR 2021"
  },
  {
    "input": "How many stars does the TinyBERT: Distilling BERT for Natural Language Understanding repository have?",
    "output": "The huawei-noah/Pretrained-Language-Model repository has 3,149 stars and 644 forks on GitHub."
  },
  {
    "input": "What is the GitHub repository for SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities?",
    "output": "The code for SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities is available at https://github.com/0nutation/SpeechGPT (0nutation/SpeechGPT)."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2504.21801&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2504.21801&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What year was MMLU: Measuring Massive Multitask Language Understanding released?",
    "output": "The MMLU: Measuring Massive Multitask Language Understanding paper was published in 2021."
  },
  {
    "input": "In what year did Gorilla: Large Language Model Connected with Massive APIs come out?",
    "output": "The Gorilla: Large Language Model Connected with Massive APIs paper was published in 2023."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2004.08492&start=0&max_results=10 repository have?",
    "output": "The uber/orbit repository has 2,019 stars and 142 forks on GitHub."
  },
  {
    "input": "What programming language is QLoRA: Efficient Finetuning of Quantized LLMs implemented in?",
    "output": "The QLoRA: Efficient Finetuning of Quantized LLMs implementation is written in Jupyter Notebook."
  },
  {
    "input": "What's the implementation repository for Denoising Diffusion Implicit Models?",
    "output": "The code for Denoising Diffusion Implicit Models is available at https://github.com/ermongroup/ddim (ermongroup/ddim)."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2006.16779&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2006.16779&start=0&max_results=10 is a 2020 paper in Machine Learning. The implementation is available at google-research/pegasus It is written in Python. The repository has 1,644 stars, indicating high community interest."
  },
  {
    "input": "Explain the tmikolov/word2vec implementation of Efficient Estimation of Word Representations in Vector Space.",
    "output": "Automatically exported from code.google.com/p/word2vec"
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2505.22954&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2505.22954&start=0&max_results=10 is available at https://github.com/ruixin31/Spurious_Rewards (ruixin31/Spurious_Rewards)."
  },
  {
    "input": "What paper has ArXiv ID 1901.02860?",
    "output": "Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context"
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2501.13824&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2501.13824&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2007.16122&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2007.16122&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Describe the frankxu2004/knnlm-why repository that implements arXiv Query: search_query=&id_list=2301.04856&start=0&max_results=10.",
    "output": "Repo for ICML23 \"Why do Nearest Neighbor Language Models Work?\""
  },
  {
    "input": "What paper has ArXiv ID 2302.05543?",
    "output": "ControlNet: Adding Conditional Control to Text-to-Image Diffusion"
  },
  {
    "input": "What is InPars: Data Augmentation for Information Retrieval about?",
    "output": "InPars: Data Augmentation for Information Retrieval is a 2022 paper in Information Retrieval. The implementation is available at zetaalphavector/InPars which inquisitive parrots for search It is written in Python. The repository has 198 stars, indicating high community interest."
  },
  {
    "input": "What is the ArXiv category of PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel?",
    "output": "PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel is in the Distributed, Parallel, and Cluster Computing (cs.DC) category."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2406.12824?",
    "output": "arXiv Query: search_query=&id_list=2406.12824&start=0&max_results=10"
  },
  {
    "input": "What does the ruiqi-zhong/D5 repository do?",
    "output": "The GitHub repo for Goal Driven Discovery of Distributional Differences via Language Descriptions"
  },
  {
    "input": "How many stars does the BEiT: BERT Pre-Training of Image Transformers repository have?",
    "output": "The microsoft/unilm repository has 21,848 stars and 2,674 forks on GitHub."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2503.01141&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2503.01141&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at LTH14/fractalgen which pytorch implementation of fractalgen https://arxiv.org/abs/2502.17437 It is written in Python. The repository has 1,196 stars, indicating high community interest."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 1912.01703?",
    "output": "PyTorch: An Imperative Style, High-Performance Deep Learning Library"
  },
  {
    "input": "How popular is the Segment Anything in Medical Images implementation?",
    "output": "The bowang-lab/MedSAM repository has 3,930 stars and 532 forks on GitHub."
  },
  {
    "input": "Explain the pytorch/pytorch implementation of Decoupled Weight Decay Regularization.",
    "output": "Tensors and Dynamic neural networks in Python with strong GPU acceleration"
  },
  {
    "input": "Give me an overview of LLaMA: Open and Efficient Foundation Language Models.",
    "output": "LLaMA: Open and Efficient Foundation Language Models is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at meta-llama/llama which inference code for llama models It is written in Python. The repository has 58,946 stars, indicating high community interest."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2101.04909&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2101.04909&start=0&max_results=10 is available at https://github.com/facebookresearch/CovidPrognosis (facebookresearch/CovidPrognosis)."
  },
  {
    "input": "What year was Mistral 7B released?",
    "output": "The Mistral 7B paper was published in 2023."
  },
  {
    "input": "When was the AudioPaLM: A Large Language Model for Speech Understanding paper published?",
    "output": "The AudioPaLM: A Large Language Model for Speech Understanding paper was published in 2023."
  },
  {
    "input": "Describe the HazyResearch/safari repository that implements arXiv Query: search_query=&id_list=2303.04129&start=0&max_results=10.",
    "output": "Convolutions for Sequence Modeling"
  },
  {
    "input": "What's the implementation repository for GPT-3: Language Models are Few-Shot Learners?",
    "output": "The code for GPT-3: Language Models are Few-Shot Learners is available at https://github.com/openai/gpt-3 (openai/gpt-3)."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2503.22708&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2503.22708&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "When was the Prefix-Tuning: Optimizing Continuous Prompts for Generation paper published?",
    "output": "The Prefix-Tuning: Optimizing Continuous Prompts for Generation paper was published in 2021."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2010.02502?",
    "output": "Denoising Diffusion Implicit Models"
  },
  {
    "input": "Describe the noahshinn/reflexion repository that implements Reflexion: Language Agents with Verbal Reinforcement Learning.",
    "output": "[NeurIPS 2023] Reflexion: Language Agents with Verbal Reinforcement Learning"
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2502.19328&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2502.19328&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2302.03917&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2302.03917&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2303.04129&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2303.04129&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "Explain the HazyResearch/safari implementation of arXiv Query: search_query=&id_list=2302.10866&start=0&max_results=10.",
    "output": "Convolutions for Sequence Modeling"
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2302.03917&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2302.03917&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2301.05062&start=0&max_results=10 repository have?",
    "output": "The google-deepmind/tracr repository has 548 stars and 49 forks on GitHub."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2301.11316&start=0&max_results=10 repository have?",
    "output": "The autonomousvision/stylegan-t repository has 1,196 stars and 61 forks on GitHub."
  },
  {
    "input": "Describe the JohnNay/llm-lobbyist repository that implements arXiv Query: search_query=&id_list=2301.01181&start=0&max_results=10.",
    "output": "Code for the paper: \"Large Language Models as Corporate Lobbyists\" (2023)."
  },
  {
    "input": "What year was BLIP: Bootstrapping Language-Image Pre-training released?",
    "output": "The BLIP: Bootstrapping Language-Image Pre-training paper was published in 2022."
  },
  {
    "input": "How popular is the Inductive Representation Learning on Large Graphs implementation?",
    "output": "The williamleif/GraphSAGE repository has 3,630 stars and 849 forks on GitHub."
  },
  {
    "input": "What topics does the EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification implementation cover?",
    "output": "The jasonwei20/eda_nlp repository covers: nlp, data-augmentation, text-classification, synonyms, embeddings, sentence, classification, rnn, cnn, swap, position."
  },
  {
    "input": "What does the google-research/prompt-tuning repository do?",
    "output": "Original Implementation of Prompt Tuning from Lester, et al, 2021"
  },
  {
    "input": "What paper has ArXiv ID 2306.00978?",
    "output": "AWQ: Activation-aware Weight Quantization for LLM Compression"
  },
  {
    "input": "What is T5: Exploring the Limits of Transfer Learning about?",
    "output": "T5: Exploring the Limits of Transfer Learning is a 2019 paper in Computation and Language (Natural Language Processing). The implementation is available at google-research/text-to-text-transfer-transformer which code for the paper \"exploring the limits of transfer learning with a unified text-to-text transformer\" It is written in Python. The repository has 6,453 stars, indicating high community interest."
  },
  {
    "input": "What year was Depthwise Separable Convolutions for Neural Machine Translation released?",
    "output": "The Depthwise Separable Convolutions for Neural Machine Translation paper was published in 2017."
  },
  {
    "input": "What is the GitHub repository for Playing Atari with Deep Reinforcement Learning?",
    "output": "The code for Playing Atari with Deep Reinforcement Learning is available at https://github.com/google-deepmind/dqn (google-deepmind/dqn)."
  },
  {
    "input": "In what year did Time-LLM: Time Series Forecasting by Reprogramming Large Language Models come out?",
    "output": "The Time-LLM: Time Series Forecasting by Reprogramming Large Language Models paper was published in 2023."
  },
  {
    "input": "Tell me the title of ArXiv paper 2310.06825.",
    "output": "Mistral 7B"
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2503.20201&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2503.20201&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2301.01751&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2301.01751&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at oughtinc/ice which interactive composition explorer: a debugger for compositional language model programs It is written in Python. The repository has 563 stars, indicating high community interest."
  },
  {
    "input": "Tell me the title of ArXiv paper 2301.13188.",
    "output": "arXiv Query: search_query=&id_list=2301.13188&start=0&max_results=10"
  },
  {
    "input": "Explain the google-research/prompt-tuning implementation of The Power of Scale for Parameter-Efficient Prompt Tuning.",
    "output": "Original Implementation of Prompt Tuning from Lester, et al, 2021"
  },
  {
    "input": "What is the ArXiv category of SmoothQuant: Accurate and Efficient Post-Training Quantization?",
    "output": "SmoothQuant: Accurate and Efficient Post-Training Quantization is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2302.04023&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2302.04023&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at mit-han-lab/offsite-tuning which offsite-tuning: transfer learning without full model It is written in Python. The repository has 382 stars, indicating high community interest."
  },
  {
    "input": "What is the ArXiv category of MetaGPT: Meta Programming for Multi-Agent Systems?",
    "output": "MetaGPT: Meta Programming for Multi-Agent Systems is in the Artificial Intelligence (cs.AI) category."
  },
  {
    "input": "What year was Grounding DINO: Marrying DINO with Grounded Pre-Training released?",
    "output": "The Grounding DINO: Marrying DINO with Grounded Pre-Training paper was published in 2023."
  },
  {
    "input": "Describe the williamleif/GraphSAGE repository that implements Inductive Representation Learning on Large Graphs.",
    "output": "Representation learning on large graphs using stochastic graph convolutions."
  },
  {
    "input": "Where can I find the code for LLM.int8(): 8-bit Matrix Multiplication for Transformers?",
    "output": "The code for LLM.int8(): 8-bit Matrix Multiplication for Transformers is available at https://github.com/bitsandbytes-foundation/bitsandbytes (bitsandbytes-foundation/bitsandbytes)."
  },
  {
    "input": "Give me an overview of Attention Is All You Need.",
    "output": "Attention Is All You Need is a 2017 paper in Computation and Language (Natural Language Processing). The implementation is available at tensorflow/tensor2tensor which library of deep learning models and datasets designed to make deep learning more accessible and accelerate ml research. It is written in Python. The repository has 16,760 stars, indicating high community interest."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2405.03520&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2405.03520&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Which field does Densely Connected Convolutional Networks belong to?",
    "output": "Densely Connected Convolutional Networks is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "What category is the ControlNet: Adding Conditional Control to Text-to-Image Diffusion paper in?",
    "output": "ControlNet: Adding Conditional Control to Text-to-Image Diffusion is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "What year was Deep Graph Library: A Graph-Centric, Highly-Performant Package released?",
    "output": "The Deep Graph Library: A Graph-Centric, Highly-Performant Package paper was published in 2020."
  },
  {
    "input": "Explain the T5: Exploring the Limits of Transfer Learning paper.",
    "output": "T5: Exploring the Limits of Transfer Learning is a 2019 paper in Computation and Language (Natural Language Processing). The implementation is available at google-research/text-to-text-transfer-transformer which code for the paper \"exploring the limits of transfer learning with a unified text-to-text transformer\" It is written in Python. The repository has 6,453 stars, indicating high community interest."
  },
  {
    "input": "What language is the GAIR-NLP/LIMO repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2502.03387&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2303.03378&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2303.03378&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at NVlabs/prismer which the implementation of \"prismer: a vision-language model with multi-task experts\". It is written in Python. The repository has 1,307 stars, indicating high community interest."
  },
  {
    "input": "What paper has ArXiv ID 2104.08691?",
    "output": "Prefix-Tuning: Optimizing Continuous Prompts for Generation"
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2504.00698&start=0&max_results=10 implementation?",
    "output": "The allenai/codescientist repository has 301 stars and 37 forks on GitHub."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2004.08492?",
    "output": "arXiv Query: search_query=&id_list=2004.08492&start=0&max_results=10"
  },
  {
    "input": "How many stars does the Graph Attention Networks repository have?",
    "output": "The PetarV-/GAT repository has 3,460 stars and 673 forks on GitHub."
  },
  {
    "input": "Give me an overview of FLAN: Finetuned Language Models Are Zero-Shot Learners.",
    "output": "FLAN: Finetuned Language Models Are Zero-Shot Learners is a 2021 paper in Computation and Language (Natural Language Processing). The implementation is available at google-research/FLAN It is written in Python. The repository has 1,554 stars, indicating high community interest."
  },
  {
    "input": "In what year did ViT: An Image is Worth 16x16 Words come out?",
    "output": "The ViT: An Image is Worth 16x16 Words paper was published in 2021."
  },
  {
    "input": "What programming language is Swin Transformer: Hierarchical Vision Transformer using Shifted Windows implemented in?",
    "output": "The Swin Transformer: Hierarchical Vision Transformer using Shifted Windows implementation is written in Python."
  },
  {
    "input": "What does the PetarV-/GAT repository do?",
    "output": "Graph Attention Networks (https://arxiv.org/abs/1710.10903)"
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2302.12246&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2302.12246&start=0&max_results=10 is available at https://github.com/shizhediao/active-prompt (shizhediao/active-prompt)."
  },
  {
    "input": "What's the implementation repository for MiniGPT-4: Enhancing Vision-Language Understanding?",
    "output": "The code for MiniGPT-4: Enhancing Vision-Language Understanding is available at https://github.com/Vision-CAIR/MiniGPT-4 (Vision-CAIR/MiniGPT-4)."
  },
  {
    "input": "Summarize the XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale paper.",
    "output": "XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale is a 2021 paper in Computation and Language (Natural Language Processing). The implementation is available at facebookresearch/fairseq which facebook ai research sequence-to-sequence toolkit written in python. It is written in Python. The repository has 31,985 stars, indicating high community interest."
  },
  {
    "input": "What is Mamba: Linear-Time Sequence Modeling with Selective State Spaces about?",
    "output": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces is a 2023 paper in Machine Learning. The implementation is available at state-spaces/mamba which mamba ssm architecture It is written in Python. The repository has 16,525 stars, indicating high community interest."
  },
  {
    "input": "What year was WizardCoder: Empowering Code LLMs with Evol-Instruct released?",
    "output": "The WizardCoder: Empowering Code LLMs with Evol-Instruct paper was published in 2023."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2406.09403&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2406.09403&start=0&max_results=10 is a 2024 paper in Machine Learning. The implementation is available at lamini-ai/Lamini-Memory-Tuning which banishing llm hallucinations requires rethinking generalization The repository has 275 stars, indicating high community interest."
  },
  {
    "input": "Summarize the XLNet: Generalized Autoregressive Pretraining paper.",
    "output": "XLNet: Generalized Autoregressive Pretraining is a 2019 paper in Computation and Language (Natural Language Processing). The implementation is available at zihangdai/xlnet which xlnet: generalized autoregressive pretraining for language understanding It is written in Python. The repository has 6,179 stars, indicating high community interest."
  },
  {
    "input": "What does the facebookresearch/DPR repository do?",
    "output": "Dense Passage Retriever - is a set of tools and models for open domain Q&A task."
  },
  {
    "input": "Explain the google-research/bert implementation of BERT: Pre-training of Deep Bidirectional Transformers.",
    "output": "TensorFlow code and pre-trained models for BERT"
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2504.16084&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2504.16084&start=0&max_results=10 is available at https://github.com/Crista23/goal_directedness_llms (Crista23/goal_directedness_llms)."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2406.10209&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2406.10209&start=0&max_results=10 paper was published in 2024."
  },
  {
    "input": "Summarize the DreamFusion: Text-to-3D using 2D Diffusion paper.",
    "output": "DreamFusion: Text-to-3D using 2D Diffusion is a 2022 paper in Computer Vision and Pattern Recognition. The implementation is available at ashawkey/stable-dreamfusion which text-to-3d & image-to-3d & mesh exportation with nerf + diffusion. It is written in Python. The repository has 8,773 stars, indicating high community interest."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2503.01141&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2503.01141&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2301.04104&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2301.04104&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2301.04856&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2301.04856&start=0&max_results=10 is available at https://github.com/frankxu2004/knnlm-why (frankxu2004/knnlm-why)."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2209.00796?",
    "output": "DreamFusion: Text-to-3D using 2D Diffusion"
  },
  {
    "input": "When was the Animate Anyone: Consistent and Controllable Image-to-Video Synthesis paper published?",
    "output": "The Animate Anyone: Consistent and Controllable Image-to-Video Synthesis paper was published in 2023."
  },
  {
    "input": "Which field does Text Embeddings by Weakly-Supervised Contrastive Pre-training belong to?",
    "output": "Text Embeddings by Weakly-Supervised Contrastive Pre-training is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Tell me the title of ArXiv paper 2303.03378.",
    "output": "arXiv Query: search_query=&id_list=2303.03378&start=0&max_results=10"
  },
  {
    "input": "Explain the alphadl/lookahead.pytorch implementation of Lookahead Optimizer: k steps forward, 1 step back.",
    "output": "lookahead optimizer (Lookahead Optimizer: k steps forward, 1 step back) for pytorch "
  },
  {
    "input": "Describe the google/trax repository that implements Reformer: The Efficient Transformer.",
    "output": "Trax \u2014 Deep Learning with Clear Code and Speed"
  },
  {
    "input": "What category is the AlpacaEval: An Automatic Evaluator for Instruction-following paper in?",
    "output": "AlpacaEval: An Automatic Evaluator for Instruction-following is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "When was the Proximal Policy Optimization Algorithms paper published?",
    "output": "The Proximal Policy Optimization Algorithms paper was published in 2017."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2405.04324&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2405.04324&start=0&max_results=10 is available at https://github.com/ibm-granite/granite-code-models (ibm-granite/granite-code-models)."
  },
  {
    "input": "How many stars does the Neural Message Passing for Quantum Chemistry repository have?",
    "output": "The priba/nmp_qc repository has 343 stars and 84 forks on GitHub."
  },
  {
    "input": "Tell me the title of ArXiv paper 1710.10903.",
    "output": "Graph Attention Networks"
  },
  {
    "input": "What language is the google-research/pegasus repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2006.16779&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2501.13545&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2501.13545&start=0&max_results=10 is available at https://github.com/plurai-ai/intellagent (plurai-ai/intellagent)."
  },
  {
    "input": "What is TensorFlow: A System for Large-Scale Machine Learning about?",
    "output": "TensorFlow: A System for Large-Scale Machine Learning is a 2016 paper in Distributed, Parallel, and Cluster Computing. The implementation is available at tensorflow/tensorflow which an open source machine learning framework for everyone It is written in C++. The repository has 192,543 stars, indicating high community interest."
  },
  {
    "input": "Give me an overview of Training language models to follow instructions with human feedback.",
    "output": "Training language models to follow instructions with human feedback is a 2022 paper in Computation and Language (Natural Language Processing). The implementation is available at openai/following-instructions-human-feedback The repository has 1,249 stars, indicating high community interest."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2303.03378&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2303.03378&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What year was Reformer: The Efficient Transformer released?",
    "output": "The Reformer: The Efficient Transformer paper was published in 2020."
  },
  {
    "input": "What is LIMA: Less Is More for Alignment about?",
    "output": "LIMA: Less Is More for Alignment is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at meta-llama/llama which inference code for llama models It is written in Python. The repository has 58,946 stars, indicating high community interest."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2504.11844&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2504.11844&start=0&max_results=10 is available at https://github.com/Crista23/goal_directedness_llms (Crista23/goal_directedness_llms)."
  },
  {
    "input": "Which field does GPT-4 Technical Report belong to?",
    "output": "GPT-4 Technical Report is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2302.10866&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2302.10866&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at HazyResearch/safari which convolutions for sequence modeling It is written in Assembly. The repository has 904 stars, indicating high community interest."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2505.23006&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2505.23006&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at ruixin31/Spurious_Rewards It is written in Python. The repository has 342 stars, indicating high community interest."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2301.09515&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2301.09515&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2504.11844?",
    "output": "arXiv Query: search_query=&id_list=2504.11844&start=0&max_results=10"
  },
  {
    "input": "What topics does the Know What You Don't Know: Unanswerable Questions for SQuAD implementation cover?",
    "output": "The rajpurkar/SQuAD-explorer repository covers: dataset, visual-analysis, leaderboard."
  },
  {
    "input": "In what year did Linformer: Self-Attention with Linear Complexity come out?",
    "output": "The Linformer: Self-Attention with Linear Complexity paper was published in 2021."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2301.13688&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2301.13688&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What paper has ArXiv ID 1512.03385?",
    "output": "Deep Residual Learning for Image Recognition"
  },
  {
    "input": "Which field does BEiT: BERT Pre-Training of Image Transformers belong to?",
    "output": "BEiT: BERT Pre-Training of Image Transformers is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2302.00902?",
    "output": "arXiv Query: search_query=&id_list=2302.00902&start=0&max_results=10"
  },
  {
    "input": "What language is the huawei-noah/Pretrained-Language-Model repository written in?",
    "output": "The TinyBERT: Distilling BERT for Natural Language Understanding implementation is written in Python."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2502.17437?",
    "output": "arXiv Query: search_query=&id_list=2502.17437&start=0&max_results=10"
  },
  {
    "input": "When was the Training language models to follow instructions with human feedback paper published?",
    "output": "The Training language models to follow instructions with human feedback paper was published in 2022."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2303.03846&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2303.03846&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2301.10343&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2301.10343&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at autonomousvision/stylegan-t which [icml'23] stylegan-t: unlocking the power of gans for fast large-scale text-to-image synthesis It is written in Python. The repository has 1,196 stars, indicating high community interest."
  },
  {
    "input": "What is SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities about?",
    "output": "SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at 0nutation/SpeechGPT which speechgpt series: speech large language models It is written in Python. The repository has 1,396 stars, indicating high community interest."
  },
  {
    "input": "What programming language is Linformer: Self-Attention with Linear Complexity implemented in?",
    "output": "The Linformer: Self-Attention with Linear Complexity implementation is written in Python."
  },
  {
    "input": "What topics does the Grounding DINO: Marrying DINO with Grounded Pre-Training implementation cover?",
    "output": "The IDEA-Research/GroundingDINO repository covers: object-detection, open-world, open-world-detection, vision-language, vision-language-transformer."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2301.01947&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2301.01947&start=0&max_results=10 is available at https://github.com/oughtinc/ice (oughtinc/ice)."
  },
  {
    "input": "Give me an overview of Dream to Control: Learning Behaviors by Latent Imagination.",
    "output": "Dream to Control: Learning Behaviors by Latent Imagination is a 2019 paper in Machine Learning. The implementation is available at danijar/dreamer which dream to control: learning behaviors by latent imagination It is written in Python. The repository has 565 stars, indicating high community interest."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2301.00774&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2301.00774&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at facebookresearch/ConvNeXt-V2 which code release for convnext v2 model It is written in Python. The repository has 1,890 stars, indicating high community interest."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2303.02913&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2303.02913&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "Explain the MoonshotAI/Kimi-Audio implementation of arXiv Query: search_query=&id_list=2504.19413&start=0&max_results=10.",
    "output": "Kimi-Audio, an open-source audio foundation model excelling in audio understanding, generation, and conversation"
  },
  {
    "input": "Explain the huggingface/transformers implementation of DistilBERT: a distilled version of BERT.",
    "output": "\ud83e\udd17 Transformers: the model-definition framework for state-of-the-art machine learning models in text, vision, audio, and multimodal models, for both inference and training. "
  },
  {
    "input": "Give me an overview of Wav2Vec 2.0: A Framework for Self-Supervised Learning of Speech.",
    "output": "Wav2Vec 2.0: A Framework for Self-Supervised Learning of Speech is a 2021 paper in Computation and Language (Natural Language Processing). The implementation is available at facebookresearch/fairseq which facebook ai research sequence-to-sequence toolkit written in python. It is written in Python. The repository has 31,985 stars, indicating high community interest."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2503.22678&start=0&max_results=10 implementation?",
    "output": "The MAXNORM8650/MedAgentSim repository has 94 stars and 11 forks on GitHub."
  },
  {
    "input": "Describe the bowang-lab/MedSAM repository that implements Segment Anything in Medical Images.",
    "output": "Segment Anything in Medical Images"
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2301.11316?",
    "output": "arXiv Query: search_query=&id_list=2301.11316&start=0&max_results=10"
  },
  {
    "input": "What is Semi-Supervised Classification with Graph Convolutional Networks about?",
    "output": "Semi-Supervised Classification with Graph Convolutional Networks is a 2016 paper in Machine Learning. The implementation is available at tkipf/gcn which implementation of graph convolutional networks in tensorflow It is written in Python. The repository has 7,334 stars, indicating high community interest."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2406.09403&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2406.09403&start=0&max_results=10 paper was published in 2024."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2301.00808&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2301.00808&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What paper has ArXiv ID 2504.21801?",
    "output": "arXiv Query: search_query=&id_list=2504.21801&start=0&max_results=10"
  },
  {
    "input": "Describe the PetarV-/GAT repository that implements Graph Attention Networks.",
    "output": "Graph Attention Networks (https://arxiv.org/abs/1710.10903)"
  },
  {
    "input": "Where can I find the code for Distilling the Knowledge in a Neural Network?",
    "output": "The code for Distilling the Knowledge in a Neural Network is available at https://github.com/haitongli/knowledge-distillation-pytorch (haitongli/knowledge-distillation-pytorch)."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2107.07346&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2107.07346&start=0&max_results=10 is available at https://github.com/jacopotagliabue/you-dont-need-a-bigger-boat (jacopotagliabue/you-dont-need-a-bigger-boat)."
  },
  {
    "input": "Explain the Shark-NLP/OpenICL implementation of arXiv Query: search_query=&id_list=2303.02913&start=0&max_results=10.",
    "output": "OpenICL is an open-source framework to facilitate research, development, and prototyping of in-context learning."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2505.23006&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2505.23006&start=0&max_results=10 is available at https://github.com/ruixin31/Spurious_Rewards (ruixin31/Spurious_Rewards)."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2203.08913?",
    "output": "RetNet: Retentive Network: A Successor to Transformer"
  },
  {
    "input": "What year was Reflexion: Language Agents with Verbal Reinforcement Learning released?",
    "output": "The Reflexion: Language Agents with Verbal Reinforcement Learning paper was published in 2023."
  },
  {
    "input": "What topics does the Linformer: Self-Attention with Linear Complexity implementation cover?",
    "output": "The tatp22/linformer-pytorch repository covers: artificial-intelligence, deep-learning, attention-mechanism, pytorch, machine-learning, linformer, paper."
  },
  {
    "input": "What paper has ArXiv ID 2203.05556?",
    "output": "FlashAttention: Fast and Memory-Efficient Exact Attention"
  },
  {
    "input": "Explain the openai/baselines implementation of Proximal Policy Optimization Algorithms.",
    "output": "OpenAI Baselines: high-quality implementations of reinforcement learning algorithms"
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2302.12246&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2302.12246&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What language is the microsoft/unilm repository written in?",
    "output": "The Text Embeddings by Weakly-Supervised Contrastive Pre-training implementation is written in Python."
  },
  {
    "input": "What paper has ArXiv ID 2307.09288?",
    "output": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
  },
  {
    "input": "What paper has ArXiv ID 2009.06367?",
    "output": "arXiv Query: search_query=&id_list=2009.06367&start=0&max_results=10"
  },
  {
    "input": "Which field does BLIP-2: Bootstrapping Language-Image Pre-training belong to?",
    "output": "BLIP-2: Bootstrapping Language-Image Pre-training is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "What language is the HazyResearch/safari repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2303.04226&start=0&max_results=10 implementation is written in Assembly."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2005.11401?",
    "output": "RAG: Retrieval-Augmented Generation"
  },
  {
    "input": "What is the ArXiv category of FlashAttention: Fast and Memory-Efficient Exact Attention?",
    "output": "FlashAttention: Fast and Memory-Efficient Exact Attention is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "How many stars does the Denoising Diffusion Implicit Models repository have?",
    "output": "The ermongroup/ddim repository has 1,743 stars and 227 forks on GitHub."
  },
  {
    "input": "What are the key topics in the tatsu-lab/alpaca_eval repository?",
    "output": "The tatsu-lab/alpaca_eval repository covers: deep-learning, evaluation, foundation-models, instruction-following, large-language-models, leaderboard, nlp, rlhf."
  },
  {
    "input": "What language is the microsoft/JARVIS repository written in?",
    "output": "The HuggingGPT: Solving AI Tasks with ChatGPT implementation is written in Python."
  },
  {
    "input": "What paper has ArXiv ID 2502.12115?",
    "output": "arXiv Query: search_query=&id_list=2502.12115&start=0&max_results=10"
  },
  {
    "input": "What programming language is Analyzing the Structure of Attention in a Transformer Language Model implemented in?",
    "output": "The Analyzing the Structure of Attention in a Transformer Language Model implementation is written in Jupyter Notebook."
  },
  {
    "input": "What's the implementation repository for Code Llama: Open Foundation Models for Code?",
    "output": "The code for Code Llama: Open Foundation Models for Code is available at https://github.com/meta-llama/codellama (meta-llama/codellama)."
  },
  {
    "input": "How many stars does the ReAct: Synergizing Reasoning and Acting in Language Models repository have?",
    "output": "The ysymyth/ReAct repository has 3,225 stars and 325 forks on GitHub."
  },
  {
    "input": "Tell me the title of ArXiv paper 2303.04226.",
    "output": "arXiv Query: search_query=&id_list=2303.04226&start=0&max_results=10"
  },
  {
    "input": "What is the title of the paper with ArXiv ID 1503.02531?",
    "output": "Distilling the Knowledge in a Neural Network"
  },
  {
    "input": "When was the XLNet: Generalized Autoregressive Pretraining paper published?",
    "output": "The XLNet: Generalized Autoregressive Pretraining paper was published in 2019."
  },
  {
    "input": "What does the ibm-granite/granite-code-models repository do?",
    "output": "Granite Code Models: A Family of Open Foundation Models for Code Intelligence"
  },
  {
    "input": "Explain the plurai-ai/intellagent implementation of arXiv Query: search_query=&id_list=2501.11067&start=0&max_results=10.",
    "output": "A framework for comprehensive diagnosis and optimization of agents using simulated, realistic synthetic interactions"
  },
  {
    "input": "What is XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale about?",
    "output": "XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale is a 2021 paper in Computation and Language (Natural Language Processing). The implementation is available at facebookresearch/fairseq which facebook ai research sequence-to-sequence toolkit written in python. It is written in Python. The repository has 31,985 stars, indicating high community interest."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2301.09515&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2301.09515&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at autonomousvision/stylegan-t which [icml'23] stylegan-t: unlocking the power of gans for fast large-scale text-to-image synthesis It is written in Python. The repository has 1,196 stars, indicating high community interest."
  },
  {
    "input": "Which field does Denoising Diffusion Probabilistic Models belong to?",
    "output": "Denoising Diffusion Probabilistic Models is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 1607.01759?",
    "output": "Enriching Word Vectors with Subword Information"
  },
  {
    "input": "What does the openai/baselines repository do?",
    "output": "OpenAI Baselines: high-quality implementations of reinforcement learning algorithms"
  },
  {
    "input": "What does the bowang-lab/MedSAM repository do?",
    "output": "Segment Anything in Medical Images"
  },
  {
    "input": "Describe the autonomousvision/stylegan-t repository that implements arXiv Query: search_query=&id_list=2301.09515&start=0&max_results=10.",
    "output": "[ICML'23] StyleGAN-T: Unlocking the Power of GANs for Fast Large-Scale Text-to-Image Synthesis"
  },
  {
    "input": "Explain the hojonathanho/diffusion implementation of Denoising Diffusion Probabilistic Models.",
    "output": "Denoising Diffusion Probabilistic Models"
  },
  {
    "input": "How popular is the Wide & Deep Learning for Recommender Systems implementation?",
    "output": "The tensorflow/models repository has 77,677 stars and 45,409 forks on GitHub."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2405.02803&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2405.02803&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Give me an overview of LIMA: Less Is More for Alignment.",
    "output": "LIMA: Less Is More for Alignment is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at meta-llama/llama which inference code for llama models It is written in Python. The repository has 58,946 stars, indicating high community interest."
  },
  {
    "input": "What category is the DreamBooth: Fine Tuning Text-to-Image Diffusion Models paper in?",
    "output": "DreamBooth: Fine Tuning Text-to-Image Diffusion Models is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "Give me an overview of Dense Passage Retrieval for Open-Domain Question Answering.",
    "output": "Dense Passage Retrieval for Open-Domain Question Answering is a 2020 paper in Computation and Language (Natural Language Processing). The implementation is available at facebookresearch/DPR which dense passage retriever - is a set of tools and models for open domain q&a task. It is written in Python. The repository has 1,847 stars, indicating high community interest."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2302.12246&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2302.12246&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What does the state-spaces/mamba repository do?",
    "output": "Mamba SSM architecture"
  },
  {
    "input": "Where can I find the code for Sequence to Sequence Learning with Neural Networks?",
    "output": "The code for Sequence to Sequence Learning with Neural Networks is available at https://github.com/farizrahman4u/seq2seq (farizrahman4u/seq2seq)."
  },
  {
    "input": "Describe the google-research-datasets/natural-questions repository that implements Question and Answer Test-Train Overlap in Open-Domain QA Datasets.",
    "output": "Natural Questions (NQ) contains real user questions issued to Google search, and answers found from Wikipedia by annotators. NQ is designed for the training and evaluation of automatic question answering systems."
  },
  {
    "input": "Tell me the title of ArXiv paper 2305.10425.",
    "output": "Time-LLM: Time Series Forecasting by Reprogramming Large Language Models"
  },
  {
    "input": "Tell me the title of ArXiv paper 2212.10375.",
    "output": "InPars: Data Augmentation for Information Retrieval"
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2301.00303&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2301.00303&start=0&max_results=10 paper was published in 2022."
  },
  {
    "input": "When was the The Power of Scale for Parameter-Efficient Prompt Tuning paper published?",
    "output": "The The Power of Scale for Parameter-Efficient Prompt Tuning paper was published in 2021."
  },
  {
    "input": "When was the Text Embeddings by Weakly-Supervised Contrastive Pre-training paper published?",
    "output": "The Text Embeddings by Weakly-Supervised Contrastive Pre-training paper was published in 2022."
  },
  {
    "input": "What year was Training language models to follow instructions with human feedback released?",
    "output": "The Training language models to follow instructions with human feedback paper was published in 2022."
  },
  {
    "input": "Which field does BERT Rediscovers the Classical NLP Pipeline belong to?",
    "output": "BERT Rediscovers the Classical NLP Pipeline is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Explain the Faster R-CNN: Towards Real-Time Object Detection paper.",
    "output": "Faster R-CNN: Towards Real-Time Object Detection is a 2015 paper in Computer Vision and Pattern Recognition. The implementation is available at rbgirshick/py-faster-rcnn which faster r-cnn (python implementation) -- see https://github.com/shaoqingren/faster_rcnn for the official matlab version It is written in Python. The repository has 8,269 stars, indicating high community interest."
  },
  {
    "input": "Summarize the Time-LLM: Time Series Forecasting by Reprogramming Large Language Models paper.",
    "output": "Time-LLM: Time Series Forecasting by Reprogramming Large Language Models is a 2023 paper in Machine Learning. The implementation is available at KimMeen/Time-LLM which [iclr 2024] official implementation of \" \ud83e\udd99 time-llm: time series forecasting by reprogramming large language models\" It is written in Python. The repository has 2,377 stars, indicating high community interest."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2302.14838&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2302.14838&start=0&max_results=10 is available at https://github.com/ruiqi-zhong/D5 (ruiqi-zhong/D5)."
  },
  {
    "input": "Summarize the Playing Atari with Deep Reinforcement Learning paper.",
    "output": "Playing Atari with Deep Reinforcement Learning is a 2013 paper in Machine Learning. The implementation is available at google-deepmind/dqn which lua/torch implementation of dqn (nature, 2015) It is written in Lua. The repository has 618 stars, indicating high community interest."
  },
  {
    "input": "Explain the Distilling the Knowledge in a Neural Network paper.",
    "output": "Distilling the Knowledge in a Neural Network is a 2015 paper in Machine Learning. The implementation is available at haitongli/knowledge-distillation-pytorch which a pytorch implementation for exploring deep and shallow knowledge distillation (kd) experiments with flexibility It is written in Python. The repository has 1,973 stars, indicating high community interest."
  },
  {
    "input": "What's the implementation repository for FlashAttention-2: Faster Attention with Better Parallelism?",
    "output": "The code for FlashAttention-2: Faster Attention with Better Parallelism is available at https://github.com/Dao-AILab/flash-attention (Dao-AILab/flash-attention)."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2504.19413&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2504.19413&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2302.05206&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2302.05206&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "Tell me the title of ArXiv paper 1508.07909.",
    "output": "Neural Machine Translation by Jointly Learning to Align and Translate"
  },
  {
    "input": "Give me an overview of Llama 2: Open Foundation and Fine-Tuned Chat Models.",
    "output": "Llama 2: Open Foundation and Fine-Tuned Chat Models is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at meta-llama/llama which inference code for llama models It is written in Python. The repository has 58,946 stars, indicating high community interest."
  },
  {
    "input": "Summarize the Direct Preference Optimization: Your Language Model is a Reward Model paper.",
    "output": "Direct Preference Optimization: Your Language Model is a Reward Model is a 2023 paper in Machine Learning. The implementation is available at eric-mitchell/direct-preference-optimization which reference implementation for dpo (direct preference optimization) It is written in Python. The repository has 2,789 stars, indicating high community interest."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2501.13824&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2501.13824&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at plurai-ai/intellagent which a framework for comprehensive diagnosis and optimization of agents using simulated, realistic synthetic interactions It is written in Python. The repository has 1,148 stars, indicating high community interest."
  },
  {
    "input": "Give me an overview of ColBERT: Efficient Passage Search via Contextualized Late Interaction.",
    "output": "ColBERT: Efficient Passage Search via Contextualized Late Interaction is a 2021 paper in Information Retrieval. The implementation is available at stanford-futuredata/ColBERT which colbert: state-of-the-art neural search (sigir'20, tacl'21, neurips'21, naacl'22, cikm'22, acl'23, emnlp'23) It is written in Python. The repository has 3,707 stars, indicating high community interest."
  },
  {
    "input": "What is EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification about?",
    "output": "EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification is a 2019 paper in Computation and Language (Natural Language Processing). The implementation is available at jasonwei20/eda_nlp which data augmentation for nlp, presented at emnlp 2019 It is written in Python. The repository has 1,649 stars, indicating high community interest."
  },
  {
    "input": "Tell me the title of ArXiv paper 2201.12086.",
    "output": "BLIP: Bootstrapping Language-Image Pre-training"
  },
  {
    "input": "What language is the shizhediao/active-prompt repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2302.12246&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Describe the artidoro/qlora repository that implements QLoRA: Efficient Finetuning of Quantized LLMs.",
    "output": "QLoRA: Efficient Finetuning of Quantized LLMs"
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2301.10343&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2301.10343&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at autonomousvision/stylegan-t which [icml'23] stylegan-t: unlocking the power of gans for fast large-scale text-to-image synthesis It is written in Python. The repository has 1,196 stars, indicating high community interest."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2501.13545&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2501.13545&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at plurai-ai/intellagent which a framework for comprehensive diagnosis and optimization of agents using simulated, realistic synthetic interactions It is written in Python. The repository has 1,148 stars, indicating high community interest."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2503.00735&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2503.00735&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Describe the deepspeedai/DeepSpeed repository that implements ZeRO: Memory Optimizations Toward Training Trillion Parameter Models.",
    "output": "DeepSpeed is a deep learning optimization library that makes distributed training and inference easy, efficient, and effective."
  },
  {
    "input": "What's the implementation repository for MetaGPT: Meta Programming for Multi-Agent Systems?",
    "output": "The code for MetaGPT: Meta Programming for Multi-Agent Systems is available at https://github.com/FoundationAgents/MetaGPT (FoundationAgents/MetaGPT)."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2504.11844&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2504.11844&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "Describe the mit-han-lab/offsite-tuning repository that implements arXiv Query: search_query=&id_list=2302.04870&start=0&max_results=10.",
    "output": "Offsite-Tuning: Transfer Learning without Full Model"
  },
  {
    "input": "What topics does the arXiv Query: search_query=&id_list=2503.23513&start=0&max_results=10 implementation cover?",
    "output": "The MAXNORM8650/MedAgentSim repository covers: multi-agent-simulations."
  },
  {
    "input": "Which field does Neural Message Passing for Quantum Chemistry belong to?",
    "output": "Neural Message Passing for Quantum Chemistry is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2501.11067&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2501.11067&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What does the NVlabs/prismer repository do?",
    "output": "The implementation of \"Prismer: A Vision-Language Model with Multi-Task Experts\"."
  },
  {
    "input": "What programming language is LLaMA-Adapter: Efficient Fine-tuning of Language Models implemented in?",
    "output": "The LLaMA-Adapter: Efficient Fine-tuning of Language Models implementation is written in Python."
  },
  {
    "input": "What does the KaimingHe/deep-residual-networks repository do?",
    "output": "Deep Residual Learning for Image Recognition "
  },
  {
    "input": "What is the title of the paper with ArXiv ID 1910.01108?",
    "output": "TinyBERT: Distilling BERT for Natural Language Understanding"
  },
  {
    "input": "What paper has ArXiv ID 2106.09685?",
    "output": "LoRA: Low-Rank Adaptation of Large Language Models"
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2307.09288?",
    "output": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2303.08518&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2303.08518&start=0&max_results=10 is available at https://github.com/FMInference/FlexLLMGen (FMInference/FlexLLMGen)."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2301.13688&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2301.13688&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2301.09515&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2301.09515&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "How popular is the AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning implementation?",
    "output": "The QingruZhang/AdaLoRA repository has 361 stars and 36 forks on GitHub."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2404.03592&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2404.03592&start=0&max_results=10 is available at https://github.com/OpenBMB/Eurus (OpenBMB/Eurus)."
  },
  {
    "input": "What is the ArXiv category of LLM.int8(): 8-bit Matrix Multiplication for Transformers?",
    "output": "LLM.int8(): 8-bit Matrix Multiplication for Transformers is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Explain the lm-sys/FastChat implementation of Vicuna: An Open-Source Chatbot.",
    "output": "An open platform for training, serving, and evaluating large language models. Release repo for Vicuna and Chatbot Arena."
  },
  {
    "input": "Give me an overview of Generating Wikipedia by Summarizing Long Sequences.",
    "output": "Generating Wikipedia by Summarizing Long Sequences is a 2018 paper in Computation and Language (Natural Language Processing). The implementation is available at tensorflow/tensor2tensor which library of deep learning models and datasets designed to make deep learning more accessible and accelerate ml research. It is written in Python. The repository has 16,760 stars, indicating high community interest."
  },
  {
    "input": "What is the ArXiv category of Masked Autoencoders Are Scalable Vision Learners?",
    "output": "Masked Autoencoders Are Scalable Vision Learners is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2302.05206&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2302.05206&start=0&max_results=10 is available at https://github.com/tianjunz/HIR (tianjunz/HIR)."
  },
  {
    "input": "What are the key topics in the MAXNORM8650/MedAgentSim repository?",
    "output": "The MAXNORM8650/MedAgentSim repository covers: multi-agent-simulations."
  },
  {
    "input": "How popular is the The Power of Scale for Parameter-Efficient Prompt Tuning implementation?",
    "output": "The google-research/prompt-tuning repository has 698 stars and 62 forks on GitHub."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2504.19413&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2504.19413&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at MoonshotAI/Kimi-Audio which kimi-audio, an open-source audio foundation model excelling in audio understanding, generation, and conversation It is written in Python. The repository has 4,367 stars, indicating high community interest."
  },
  {
    "input": "How many stars does the Mamba: Linear-Time Sequence Modeling with Selective State Spaces repository have?",
    "output": "The state-spaces/mamba repository has 16,525 stars and 1,505 forks on GitHub."
  },
  {
    "input": "What are the key topics in the pytorch/pytorch repository?",
    "output": "The pytorch/pytorch repository covers: neural-network, autograd, gpu, numpy, deep-learning, tensor, python, machine-learning."
  },
  {
    "input": "In what year did PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel come out?",
    "output": "The PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel paper was published in 2022."
  },
  {
    "input": "What language is the Crista23/goal_directedness_llms repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2504.16084&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2504.02732&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2504.02732&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2501.19393&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2501.19393&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "How many stars does the MRKL Systems: A modular approach to AGI repository have?",
    "output": "The langchain-ai/langchain repository has 120,451 stars and 19,851 forks on GitHub."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2004.08492&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2004.08492&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2006.16779&start=0&max_results=10 implementation?",
    "output": "The google-research/pegasus repository has 1,644 stars and 318 forks on GitHub."
  },
  {
    "input": "What does the eric-mitchell/direct-preference-optimization repository do?",
    "output": "Reference implementation for DPO (Direct Preference Optimization)"
  },
  {
    "input": "Where can I find the code for Whisper: Robust Speech Recognition via Large-Scale Weak Supervision?",
    "output": "The code for Whisper: Robust Speech Recognition via Large-Scale Weak Supervision is available at https://github.com/openai/whisper (openai/whisper)."
  },
  {
    "input": "How popular is the Depthwise Separable Convolutions for Neural Machine Translation implementation?",
    "output": "The google-research/google-research repository has 36,800 stars and 8,255 forks on GitHub."
  },
  {
    "input": "What category is the Animate Anyone: Consistent and Controllable Image-to-Video Synthesis paper in?",
    "output": "Animate Anyone: Consistent and Controllable Image-to-Video Synthesis is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2301.00704&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2301.00704&start=0&max_results=10 is available at https://github.com/lucidrains/muse-maskgit-pytorch (lucidrains/muse-maskgit-pytorch)."
  },
  {
    "input": "What category is the Graph Attention Networks paper in?",
    "output": "Graph Attention Networks is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Describe the HumanAIGC/AnimateAnyone repository that implements Animate Anyone: Consistent and Controllable Image-to-Video Synthesis.",
    "output": "Animate Anyone: Consistent and Controllable Image-to-Video Synthesis for Character Animation"
  },
  {
    "input": "Tell me the title of ArXiv paper 2301.02828.",
    "output": "arXiv Query: search_query=&id_list=2301.02828&start=0&max_results=10"
  },
  {
    "input": "Summarize the LIMA: Less Is More for Alignment paper.",
    "output": "LIMA: Less Is More for Alignment is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at meta-llama/llama which inference code for llama models It is written in Python. The repository has 58,946 stars, indicating high community interest."
  },
  {
    "input": "Explain the Wav2Vec 2.0: A Framework for Self-Supervised Learning of Speech paper.",
    "output": "Wav2Vec 2.0: A Framework for Self-Supervised Learning of Speech is a 2021 paper in Computation and Language (Natural Language Processing). The implementation is available at facebookresearch/fairseq which facebook ai research sequence-to-sequence toolkit written in python. It is written in Python. The repository has 31,985 stars, indicating high community interest."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2410.12896&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2410.12896&start=0&max_results=10 paper was published in 2024."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2302.08242&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2302.08242&start=0&max_results=10 is available at https://github.com/haoliuhl/language-quantized-autoencoders (haoliuhl/language-quantized-autoencoders)."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2410.12896&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2410.12896&start=0&max_results=10 is a 2024 paper in Machine Learning. The implementation is available at ibm-granite/granite-3.0-language-models The repository has 267 stars, indicating high community interest."
  },
  {
    "input": "Describe the OpenGVLab/LLaMA-Adapter repository that implements LLaMA-Adapter: Efficient Fine-tuning of Language Models.",
    "output": "[ICLR 2024] Fine-tuning LLaMA to follow Instructions within 1 Hour and 1.2M Parameters"
  },
  {
    "input": "What topics does the RetNet: Retentive Network: A Successor to Transformer implementation cover?",
    "output": "The microsoft/torchscale repository covers: computer-vision, machine-learning, multimodal, natural-language-processing, pretrained-language-model, speech-processing, transformer, translation."
  },
  {
    "input": "What's the implementation repository for SimCSE: Simple Contrastive Learning of Sentence Embeddings?",
    "output": "The code for SimCSE: Simple Contrastive Learning of Sentence Embeddings is available at https://github.com/princeton-nlp/SimCSE (princeton-nlp/SimCSE)."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2302.04023?",
    "output": "arXiv Query: search_query=&id_list=2302.04023&start=0&max_results=10"
  },
  {
    "input": "How popular is the BERT Rediscovers the Classical NLP Pipeline implementation?",
    "output": "The clarkkev/attention-analysis repository has 469 stars and 82 forks on GitHub."
  },
  {
    "input": "How many stars does the Segment Anything in Medical Images repository have?",
    "output": "The bowang-lab/MedSAM repository has 3,930 stars and 532 forks on GitHub."
  },
  {
    "input": "What does the HazyResearch/safari repository do?",
    "output": "Convolutions for Sequence Modeling"
  },
  {
    "input": "Give me an overview of RAG: Retrieval-Augmented Generation.",
    "output": "RAG: Retrieval-Augmented Generation is a 2020 paper in Computation and Language (Natural Language Processing). The implementation is available at facebookresearch/DPR which dense passage retriever - is a set of tools and models for open domain q&a task. It is written in Python. The repository has 1,847 stars, indicating high community interest."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2406.09308&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2406.09308&start=0&max_results=10 is available at https://github.com/lamini-ai/Lamini-Memory-Tuning (lamini-ai/Lamini-Memory-Tuning)."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2302.12766&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2302.12766&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at siddk/voltron-robotics which voltron: language-driven representation learning for robotics It is written in Python. The repository has 227 stars, indicating high community interest."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2505.22101&start=0&max_results=10 implementation?",
    "output": "The ruixin31/Spurious_Rewards repository has 342 stars and 20 forks on GitHub."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2302.03011&start=0&max_results=10 implementation?",
    "output": "The mit-han-lab/offsite-tuning repository has 382 stars and 39 forks on GitHub."
  },
  {
    "input": "What are the key topics in the salesforce/BLIP repository?",
    "output": "The salesforce/BLIP repository covers: vision-language, vision-and-language-pre-training, image-text-retrieval, image-captioning, visual-question-answering, visual-reasoning, vision-language-transformer."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2505.23006&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2505.23006&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2302.12766&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2302.12766&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What does the mit-han-lab/smoothquant repository do?",
    "output": "[ICML 2023] SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models"
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2503.23513?",
    "output": "arXiv Query: search_query=&id_list=2503.23513&start=0&max_results=10"
  },
  {
    "input": "When was the Direct Preference Optimization: Your Language Model is a Reward Model paper published?",
    "output": "The Direct Preference Optimization: Your Language Model is a Reward Model paper was published in 2023."
  },
  {
    "input": "Explain the pytorch/pytorch implementation of PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel.",
    "output": "Tensors and Dynamic neural networks in Python with strong GPU acceleration"
  },
  {
    "input": "How many stars does the XLNet: Generalized Autoregressive Pretraining repository have?",
    "output": "The zihangdai/xlnet repository has 6,179 stars and 1,167 forks on GitHub."
  },
  {
    "input": "What category is the Horovod: fast and easy distributed deep learning in TensorFlow paper in?",
    "output": "Horovod: fast and easy distributed deep learning in TensorFlow is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2303.04671?",
    "output": "arXiv Query: search_query=&id_list=2303.04671&start=0&max_results=10"
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2405.03548&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2405.03548&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is the GitHub repository for Grounding DINO: Marrying DINO with Grounded Pre-Training?",
    "output": "The code for Grounding DINO: Marrying DINO with Grounded Pre-Training is available at https://github.com/IDEA-Research/GroundingDINO (IDEA-Research/GroundingDINO)."
  },
  {
    "input": "What paper has ArXiv ID 1711.00489?",
    "output": "Horovod: fast and easy distributed deep learning in TensorFlow"
  },
  {
    "input": "Which field does ChemCrow: Augmenting large-language models with chemistry tools belong to?",
    "output": "ChemCrow: Augmenting large-language models with chemistry tools is in the Artificial Intelligence (cs.AI) category."
  },
  {
    "input": "What are the key topics in the pytorch/pytorch repository?",
    "output": "The pytorch/pytorch repository covers: neural-network, autograd, gpu, numpy, deep-learning, tensor, python, machine-learning."
  },
  {
    "input": "In what year did RAG: Retrieval-Augmented Generation come out?",
    "output": "The RAG: Retrieval-Augmented Generation paper was published in 2020."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2301.00704&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2301.00704&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at lucidrains/muse-maskgit-pytorch which implementation of muse: text-to-image generation via masked generative transformers, in pytorch It is written in Python. The repository has 916 stars, indicating high community interest."
  },
  {
    "input": "What is the ArXiv category of XLNet: Generalized Autoregressive Pretraining?",
    "output": "XLNet: Generalized Autoregressive Pretraining is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Tell me the title of ArXiv paper 2505.23006.",
    "output": "arXiv Query: search_query=&id_list=2505.23006&start=0&max_results=10"
  },
  {
    "input": "What's the implementation repository for LLaMA: Open and Efficient Foundation Language Models?",
    "output": "The code for LLaMA: Open and Efficient Foundation Language Models is available at https://github.com/meta-llama/llama (meta-llama/llama)."
  },
  {
    "input": "What's the implementation repository for TensorFlow: A System for Large-Scale Machine Learning?",
    "output": "The code for TensorFlow: A System for Large-Scale Machine Learning is available at https://github.com/tensorflow/tensorflow (tensorflow/tensorflow)."
  },
  {
    "input": "What year was AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning released?",
    "output": "The AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning paper was published in 2022."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2210.17323?",
    "output": "GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers"
  },
  {
    "input": "What paper has ArXiv ID 2501.13545?",
    "output": "arXiv Query: search_query=&id_list=2501.13545&start=0&max_results=10"
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2303.04226&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2303.04226&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at HazyResearch/safari which convolutions for sequence modeling It is written in Assembly. The repository has 904 stars, indicating high community interest."
  },
  {
    "input": "What paper has ArXiv ID 2405.03520?",
    "output": "arXiv Query: search_query=&id_list=2405.03520&start=0&max_results=10"
  },
  {
    "input": "How popular is the Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning implementation?",
    "output": "The rail-berkeley/softlearning repository has 1,370 stars and 249 forks on GitHub."
  },
  {
    "input": "What language is the hojonathanho/diffusion repository written in?",
    "output": "The Denoising Diffusion Probabilistic Models implementation is written in Python."
  },
  {
    "input": "Give me an overview of MRKL Systems: A modular approach to AGI.",
    "output": "MRKL Systems: A modular approach to AGI is a 2022 paper in Artificial Intelligence. The implementation is available at langchain-ai/langchain which \ud83e\udd9c\ud83d\udd17 the platform for reliable agents. It is written in Python. The repository has 120,451 stars, indicating high community interest."
  },
  {
    "input": "What programming language is ChemCrow: Augmenting large-language models with chemistry tools implemented in?",
    "output": "The ChemCrow: Augmenting large-language models with chemistry tools implementation is written in Python."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2502.03387?",
    "output": "arXiv Query: search_query=&id_list=2502.03387&start=0&max_results=10"
  },
  {
    "input": "What is the GitHub repository for RoBERTa: A Robustly Optimized BERT Pretraining Approach?",
    "output": "The code for RoBERTa: A Robustly Optimized BERT Pretraining Approach is available at https://github.com/facebookresearch/fairseq (facebookresearch/fairseq)."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2301.01947&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2301.01947&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What paper has ArXiv ID 2303.02506?",
    "output": "arXiv Query: search_query=&id_list=2303.02506&start=0&max_results=10"
  },
  {
    "input": "Describe the machrisaa/tensorflow-vgg repository that implements Very Deep Convolutional Networks for Large-Scale Image Recognition.",
    "output": "VGG19 and VGG16 on Tensorflow"
  },
  {
    "input": "What language is the google-research/google-research repository written in?",
    "output": "The Depthwise Separable Convolutions for Neural Machine Translation implementation is written in Jupyter Notebook."
  },
  {
    "input": "What language is the ermongroup/ddim repository written in?",
    "output": "The Denoising Diffusion Implicit Models implementation is written in Python."
  },
  {
    "input": "Where can I find the code for StarCoder: May the source be with you!?",
    "output": "The code for StarCoder: May the source be with you! is available at https://github.com/bigcode-project/starcoder (bigcode-project/starcoder)."
  },
  {
    "input": "What topics does the Swin Transformer: Hierarchical Vision Transformer using Shifted Windows implementation cover?",
    "output": "The microsoft/Swin-Transformer repository covers: swin-transformer, image-classification, object-detection, semantic-segmentation, imagenet, mscoco, ade20k, mask-rcnn."
  },
  {
    "input": "What paper has ArXiv ID 2107.03374?",
    "output": "Codex: Evaluating Large Language Models Trained on Code"
  },
  {
    "input": "Give me an overview of RoBERTa: A Robustly Optimized BERT Pretraining Approach.",
    "output": "RoBERTa: A Robustly Optimized BERT Pretraining Approach is a 2019 paper in Computation and Language (Natural Language Processing). The implementation is available at facebookresearch/fairseq which facebook ai research sequence-to-sequence toolkit written in python. It is written in Python. The repository has 31,985 stars, indicating high community interest."
  },
  {
    "input": "What category is the Vicuna: An Open-Source Chatbot paper in?",
    "output": "Vicuna: An Open-Source Chatbot is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 1603.04467?",
    "output": "TensorFlow: A System for Large-Scale Machine Learning"
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2302.00923&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2302.00923&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2009.06367?",
    "output": "arXiv Query: search_query=&id_list=2009.06367&start=0&max_results=10"
  },
  {
    "input": "What is the ArXiv category of Segment Anything in Medical Images?",
    "output": "Segment Anything in Medical Images is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "Explain the lamini-ai/Lamini-Memory-Tuning implementation of arXiv Query: search_query=&id_list=2406.06326&start=0&max_results=10.",
    "output": "Banishing LLM Hallucinations Requires Rethinking Generalization"
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2301.01751&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2301.01751&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "Summarize the Question and Answer Test-Train Overlap in Open-Domain QA Datasets paper.",
    "output": "Question and Answer Test-Train Overlap in Open-Domain QA Datasets is a 2020 paper in Computation and Language (Natural Language Processing). The implementation is available at google-research-datasets/natural-questions which natural questions (nq) contains real user questions issued to google search, and answers found from wikipedia by annotators. nq is designed for the training and evaluation of automatic question answering systems. It is written in Python. The repository has 1,074 stars, indicating high community interest."
  },
  {
    "input": "What is the GitHub repository for MiniGPT-4: Enhancing Vision-Language Understanding?",
    "output": "The code for MiniGPT-4: Enhancing Vision-Language Understanding is available at https://github.com/Vision-CAIR/MiniGPT-4 (Vision-CAIR/MiniGPT-4)."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2301.04104&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2301.04104&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2302.14838&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2302.14838&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Where can I find the code for LLaVA: Large Language and Vision Assistant?",
    "output": "The code for LLaVA: Large Language and Vision Assistant is available at https://github.com/haotian-liu/LLaVA (haotian-liu/LLaVA)."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2410.18050&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2410.18050&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "When was the WizardCoder: Empowering Code LLMs with Evol-Instruct paper published?",
    "output": "The WizardCoder: Empowering Code LLMs with Evol-Instruct paper was published in 2023."
  },
  {
    "input": "How many stars does the Know What You Don't Know: Unanswerable Questions for SQuAD repository have?",
    "output": "The rajpurkar/SQuAD-explorer repository has 570 stars and 121 forks on GitHub."
  },
  {
    "input": "Summarize the EfficientDet: Scalable and Efficient Object Detection paper.",
    "output": "EfficientDet: Scalable and Efficient Object Detection is a 2020 paper in Computer Vision and Pattern Recognition. The implementation is available at google/automl which google brain automl It is written in Jupyter Notebook. The repository has 6,425 stars, indicating high community interest."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2302.14233&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2302.14233&start=0&max_results=10 is available at https://github.com/ruiqi-zhong/D5 (ruiqi-zhong/D5)."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2302.00923?",
    "output": "arXiv Query: search_query=&id_list=2302.00923&start=0&max_results=10"
  },
  {
    "input": "What's the implementation repository for XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale?",
    "output": "The code for XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale is available at https://github.com/facebookresearch/fairseq (facebookresearch/fairseq)."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2504.20734&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2504.20734&start=0&max_results=10 is available at https://github.com/MoonshotAI/Kimi-Audio (MoonshotAI/Kimi-Audio)."
  },
  {
    "input": "Describe the rajpurkar/SQuAD-explorer repository that implements Know What You Don't Know: Unanswerable Questions for SQuAD.",
    "output": "Visually Explore the Stanford Question Answering Dataset"
  },
  {
    "input": "What is the GitHub repository for Dense Passage Retrieval for Open-Domain Question Answering?",
    "output": "The code for Dense Passage Retrieval for Open-Domain Question Answering is available at https://github.com/facebookresearch/DPR (facebookresearch/DPR)."
  },
  {
    "input": "Which field does PaLM: Scaling Language Modeling with Pathways belong to?",
    "output": "PaLM: Scaling Language Modeling with Pathways is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2303.09431&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2303.09431&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at FMInference/FlexLLMGen which running large language models on a single gpu for throughput-oriented scenarios. It is written in Python. The repository has 9,379 stars, indicating high community interest."
  },
  {
    "input": "Give me an overview of FlashAttention: Fast and Memory-Efficient Exact Attention.",
    "output": "FlashAttention: Fast and Memory-Efficient Exact Attention is a 2022 paper in Machine Learning. The implementation is available at Dao-AILab/flash-attention which fast and memory-efficient exact attention It is written in Python. The repository has 20,719 stars, indicating high community interest."
  },
  {
    "input": "What category is the Training language models to follow instructions with human feedback paper in?",
    "output": "Training language models to follow instructions with human feedback is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What does the stanford-futuredata/ColBERT repository do?",
    "output": "ColBERT: state-of-the-art neural search (SIGIR'20, TACL'21, NeurIPS'21, NAACL'22, CIKM'22, ACL'23, EMNLP'23)"
  },
  {
    "input": "Tell me the title of ArXiv paper 2301.04856.",
    "output": "arXiv Query: search_query=&id_list=2301.04856&start=0&max_results=10"
  },
  {
    "input": "What is Adam: A Method for Stochastic Optimization about?",
    "output": "Adam: A Method for Stochastic Optimization is a 2014 paper in Machine Learning. The implementation is available at pytorch/pytorch which tensors and dynamic neural networks in python with strong gpu acceleration It is written in Python. The repository has 95,348 stars, indicating high community interest."
  },
  {
    "input": "Explain the TinyBERT: Distilling BERT for Natural Language Understanding paper.",
    "output": "TinyBERT: Distilling BERT for Natural Language Understanding is a 2019 paper in Computation and Language (Natural Language Processing). The implementation is available at huawei-noah/Pretrained-Language-Model which pretrained language model and its related optimization techniques developed by huawei noah's ark lab. It is written in Python. The repository has 3,149 stars, indicating high community interest."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2301.01947&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2301.01947&start=0&max_results=10 is available at https://github.com/oughtinc/ice (oughtinc/ice)."
  },
  {
    "input": "Summarize the Analyzing the Structure of Attention in a Transformer Language Model paper.",
    "output": "Analyzing the Structure of Attention in a Transformer Language Model is a 2020 paper in Computation and Language (Natural Language Processing). The implementation is available at clarkkev/attention-analysis It is written in Jupyter Notebook. The repository has 469 stars, indicating high community interest."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2312.02783&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2312.02783&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at ContextualAI/HALOs which a library with extensible implementations of dpo, kto, ppo, orpo, and other human-aware loss functions (halos). It is written in Python. The repository has 893 stars, indicating high community interest."
  },
  {
    "input": "What category is the HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction paper in?",
    "output": "HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What is the GitHub repository for Proximal Policy Optimization Algorithms?",
    "output": "The code for Proximal Policy Optimization Algorithms is available at https://github.com/openai/baselines (openai/baselines)."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2309.10305?",
    "output": "WizardCoder: Empowering Code LLMs with Evol-Instruct"
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2405.02803&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2405.02803&start=0&max_results=10 paper was published in 2024."
  },
  {
    "input": "Tell me the title of ArXiv paper 2201.08239.",
    "output": "Contriever: Unsupervised Dense Information Retrieval"
  },
  {
    "input": "Give me an overview of DeBERTa: Decoding-enhanced BERT with Disentangled Attention.",
    "output": "DeBERTa: Decoding-enhanced BERT with Disentangled Attention is a 2020 paper in Computation and Language (Natural Language Processing). The implementation is available at microsoft/DeBERTa which the implementation of deberta It is written in Python. The repository has 2,169 stars, indicating high community interest."
  },
  {
    "input": "What is DreamBooth: Fine Tuning Text-to-Image Diffusion Models about?",
    "output": "DreamBooth: Fine Tuning Text-to-Image Diffusion Models is a 2022 paper in Computer Vision and Pattern Recognition. The implementation is available at google/dreambooth The repository has 998 stars, indicating high community interest."
  },
  {
    "input": "Summarize the P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning paper.",
    "output": "P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning is a 2021 paper in Computation and Language (Natural Language Processing). The implementation is available at THUDM/P-tuning-v2 which an optimized deep prompt tuning strategy comparable to fine-tuning across scales and tasks It is written in Python. The repository has 2,067 stars, indicating high community interest."
  },
  {
    "input": "What topics does the PyTorch: An Imperative Style, High-Performance Deep Learning Library implementation cover?",
    "output": "The pytorch/pytorch repository covers: neural-network, autograd, gpu, numpy, deep-learning, tensor, python, machine-learning."
  },
  {
    "input": "When was the ControlNet: Adding Conditional Control to Text-to-Image Diffusion paper published?",
    "output": "The ControlNet: Adding Conditional Control to Text-to-Image Diffusion paper was published in 2023."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2302.09778&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2302.09778&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 1907.11692?",
    "output": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"
  },
  {
    "input": "How many stars does the LIMA: Less Is More for Alignment repository have?",
    "output": "The meta-llama/llama repository has 58,946 stars and 9,817 forks on GitHub."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2004.03270?",
    "output": "Analyzing the Structure of Attention in a Transformer Language Model"
  },
  {
    "input": "Give me an overview of Segment Anything in Medical Images.",
    "output": "Segment Anything in Medical Images is a 2023 paper in Computer Vision and Pattern Recognition. The implementation is available at bowang-lab/MedSAM which segment anything in medical images It is written in Jupyter Notebook. The repository has 3,930 stars, indicating high community interest."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 1804.09301?",
    "output": "Generating Wikipedia by Summarizing Long Sequences"
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2303.11366?",
    "output": "Reflexion: Language Agents with Verbal Reinforcement Learning"
  },
  {
    "input": "What language is the HazyResearch/safari repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2303.04129&start=0&max_results=10 implementation is written in Assembly."
  },
  {
    "input": "Tell me the title of ArXiv paper 1606.05250.",
    "output": "SQuAD: 100,000+ Questions for Machine Comprehension of Text"
  },
  {
    "input": "What paper has ArXiv ID 2002.05202?",
    "output": "Reformer: The Efficient Transformer"
  },
  {
    "input": "What is the ArXiv category of ChemCrow: Augmenting large-language models with chemistry tools?",
    "output": "ChemCrow: Augmenting large-language models with chemistry tools is in the Artificial Intelligence (cs.AI) category."
  },
  {
    "input": "What programming language is AlpacaEval: An Automatic Evaluator for Instruction-following implemented in?",
    "output": "The AlpacaEval: An Automatic Evaluator for Instruction-following implementation is written in Jupyter Notebook."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2301.02828&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2301.02828&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "How popular is the LLaVA: Large Language and Vision Assistant implementation?",
    "output": "The haotian-liu/LLaVA repository has 24,034 stars and 2,665 forks on GitHub."
  },
  {
    "input": "Describe the huggingface/transformers repository that implements DistilBERT: a distilled version of BERT.",
    "output": "\ud83e\udd17 Transformers: the model-definition framework for state-of-the-art machine learning models in text, vision, audio, and multimodal models, for both inference and training. "
  },
  {
    "input": "What's the implementation repository for LIMA: Less Is More for Alignment?",
    "output": "The code for LIMA: Less Is More for Alignment is available at https://github.com/meta-llama/llama (meta-llama/llama)."
  },
  {
    "input": "Describe the lucidrains/flamingo-pytorch repository that implements Flamingo: A Visual Language Model for Few-Shot Learning.",
    "output": "Implementation of \ud83e\udda9 Flamingo, state-of-the-art few-shot visual question answering attention net out of Deepmind, in Pytorch"
  },
  {
    "input": "What year was AlpacaEval: An Automatic Evaluator for Instruction-following released?",
    "output": "The AlpacaEval: An Automatic Evaluator for Instruction-following paper was published in 2023."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2502.17437&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2502.17437&start=0&max_results=10 is available at https://github.com/LTH14/fractalgen (LTH14/fractalgen)."
  },
  {
    "input": "In what year did FlashAttention: Fast and Memory-Efficient Exact Attention come out?",
    "output": "The FlashAttention: Fast and Memory-Efficient Exact Attention paper was published in 2022."
  },
  {
    "input": "When was the Code Llama: Open Foundation Models for Code paper published?",
    "output": "The Code Llama: Open Foundation Models for Code paper was published in 2023."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2501.11067&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2501.11067&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at plurai-ai/intellagent which a framework for comprehensive diagnosis and optimization of agents using simulated, realistic synthetic interactions It is written in Python. The repository has 1,148 stars, indicating high community interest."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2301.00774&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2301.00774&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What year was LLM.int8(): 8-bit Matrix Multiplication for Transformers released?",
    "output": "The LLM.int8(): 8-bit Matrix Multiplication for Transformers paper was published in 2022."
  },
  {
    "input": "What does the pytorch/pytorch repository do?",
    "output": "Tensors and Dynamic neural networks in Python with strong GPU acceleration"
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2303.03846&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2303.03846&start=0&max_results=10 is available at https://github.com/HazyResearch/safari (HazyResearch/safari)."
  },
  {
    "input": "What are the key topics in the lucidrains/PaLM-pytorch repository?",
    "output": "The lucidrains/PaLM-pytorch repository covers: deep-learning, transformers, attention-mechanism, artificial-general-intelligence."
  },
  {
    "input": "What category is the XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale paper in?",
    "output": "XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What language is the bitsandbytes-foundation/bitsandbytes repository written in?",
    "output": "The LLM.int8(): 8-bit Matrix Multiplication for Transformers implementation is written in Python."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2303.02913&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2303.02913&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Which field does ViT: An Image is Worth 16x16 Words belong to?",
    "output": "ViT: An Image is Worth 16x16 Words is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "What is Enriching Word Vectors with Subword Information about?",
    "output": "Enriching Word Vectors with Subword Information is a 2016 paper in Computation and Language (Natural Language Processing). The implementation is available at facebookresearch/fastText which library for fast text representation and classification. It is written in HTML. The repository has 26,432 stars, indicating high community interest."
  },
  {
    "input": "What paper has ArXiv ID 2305.14314?",
    "output": "QLoRA: Efficient Finetuning of Quantized LLMs"
  },
  {
    "input": "What year was High-Resolution Image Synthesis with Latent Diffusion Models released?",
    "output": "The High-Resolution Image Synthesis with Latent Diffusion Models paper was published in 2021."
  },
  {
    "input": "What is the ArXiv category of Playing Atari with Deep Reinforcement Learning?",
    "output": "Playing Atari with Deep Reinforcement Learning is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What programming language is InPars: Data Augmentation for Information Retrieval implemented in?",
    "output": "The InPars: Data Augmentation for Information Retrieval implementation is written in Python."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2505.22954&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2505.22954&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2303.01469&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2303.01469&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "How popular is the ColBERT: Efficient Passage Search via Contextualized Late Interaction implementation?",
    "output": "The stanford-futuredata/ColBERT repository has 3,707 stars and 462 forks on GitHub."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2302.09778?",
    "output": "arXiv Query: search_query=&id_list=2302.09778&start=0&max_results=10"
  },
  {
    "input": "What is the GitHub repository for The Power of Scale for Parameter-Efficient Prompt Tuning?",
    "output": "The code for The Power of Scale for Parameter-Efficient Prompt Tuning is available at https://github.com/google-research/prompt-tuning (google-research/prompt-tuning)."
  },
  {
    "input": "Explain the LTH14/fractalgen implementation of arXiv Query: search_query=&id_list=2503.01141&start=0&max_results=10.",
    "output": "PyTorch implementation of FractalGen https://arxiv.org/abs/2502.17437"
  },
  {
    "input": "What is the GitHub repository for S4: Efficiently Modeling Long Sequences?",
    "output": "The code for S4: Efficiently Modeling Long Sequences is available at https://github.com/state-spaces/s4 (state-spaces/s4)."
  },
  {
    "input": "How popular is the Playing Atari with Deep Reinforcement Learning implementation?",
    "output": "The google-deepmind/dqn repository has 618 stars and 164 forks on GitHub."
  },
  {
    "input": "What's the implementation repository for Depthwise Separable Convolutions for Neural Machine Translation?",
    "output": "The code for Depthwise Separable Convolutions for Neural Machine Translation is available at https://github.com/google-research/google-research (google-research/google-research)."
  },
  {
    "input": "What topics does the arXiv Query: search_query=&id_list=2312.02120&start=0&max_results=10 implementation cover?",
    "output": "The ContextualAI/HALOs repository covers: alignment, dpo, halos, kto, ppo, rlhf."
  },
  {
    "input": "What is the GitHub repository for Med-PaLM 2: Towards Expert-Level Medical Question Answering?",
    "output": "The code for Med-PaLM 2: Towards Expert-Level Medical Question Answering is available at https://github.com/google-research/google-research (google-research/google-research)."
  },
  {
    "input": "Explain the pytorch/pytorch implementation of Adam: A Method for Stochastic Optimization.",
    "output": "Tensors and Dynamic neural networks in Python with strong GPU acceleration"
  },
  {
    "input": "What category is the InPars: Data Augmentation for Information Retrieval paper in?",
    "output": "InPars: Data Augmentation for Information Retrieval is in the Information Retrieval (cs.IR) category."
  },
  {
    "input": "How many stars does the EfficientDet: Scalable and Efficient Object Detection repository have?",
    "output": "The google/automl repository has 6,425 stars and 1,463 forks on GitHub."
  },
  {
    "input": "What year was PyTorch: An Imperative Style, High-Performance Deep Learning Library released?",
    "output": "The PyTorch: An Imperative Style, High-Performance Deep Learning Library paper was published in 2019."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2504.01848&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2504.01848&start=0&max_results=10 is available at https://github.com/openai/frontier-evals (openai/frontier-evals)."
  },
  {
    "input": "What is the GitHub repository for BERT Rediscovers the Classical NLP Pipeline?",
    "output": "The code for BERT Rediscovers the Classical NLP Pipeline is available at https://github.com/clarkkev/attention-analysis (clarkkev/attention-analysis)."
  },
  {
    "input": "What category is the EfficientDet: Scalable and Efficient Object Detection paper in?",
    "output": "EfficientDet: Scalable and Efficient Object Detection is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "What is the GitHub repository for Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena?",
    "output": "The code for Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena is available at https://github.com/lm-sys/FastChat (lm-sys/FastChat)."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2504.11844&start=0&max_results=10 repository have?",
    "output": "The Crista23/goal_directedness_llms repository has 12 stars and 2 forks on GitHub."
  },
  {
    "input": "What's the implementation repository for ChemCrow: Augmenting large-language models with chemistry tools?",
    "output": "The code for ChemCrow: Augmenting large-language models with chemistry tools is available at https://github.com/ur-whitelab/chemcrow-public (ur-whitelab/chemcrow-public)."
  },
  {
    "input": "How many stars does the Code Llama: Open Foundation Models for Code repository have?",
    "output": "The meta-llama/codellama repository has 16,365 stars and 1,946 forks on GitHub."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2301.12652&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2301.12652&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2302.07459&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2302.07459&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at haoliuhl/language-quantized-autoencoders which language quantized autoencoders It is written in Python. The repository has 111 stars, indicating high community interest."
  },
  {
    "input": "Describe the ur-whitelab/chemcrow-public repository that implements ChemCrow: Augmenting large-language models with chemistry tools.",
    "output": "Chemcrow"
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2501.13824&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2501.13824&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What paper has ArXiv ID 2107.07346?",
    "output": "arXiv Query: search_query=&id_list=2107.07346&start=0&max_results=10"
  },
  {
    "input": "Give me an overview of YOLOX: Exceeding YOLO Series in 2021.",
    "output": "YOLOX: Exceeding YOLO Series in 2021 is a 2018 paper in Computer Vision and Pattern Recognition. The implementation is available at Megvii-BaseDetection/YOLOX which yolox is a high-performance anchor-free yolo, exceeding yolov3~v5 with megengine, onnx, tensorrt, ncnn, and openvino supported. documentation: https://yolox.readthedocs.io/ It is written in Python. The repository has 10,185 stars, indicating high community interest."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2405.02803&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2405.02803&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What year was Dream to Control: Learning Behaviors by Latent Imagination released?",
    "output": "The Dream to Control: Learning Behaviors by Latent Imagination paper was published in 2019."
  },
  {
    "input": "What topics does the PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel implementation cover?",
    "output": "The pytorch/pytorch repository covers: neural-network, autograd, gpu, numpy, deep-learning, tensor, python, machine-learning."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2312.02783&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2312.02783&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "Where can I find the code for Generating Wikipedia by Summarizing Long Sequences?",
    "output": "The code for Generating Wikipedia by Summarizing Long Sequences is available at https://github.com/tensorflow/tensor2tensor (tensorflow/tensor2tensor)."
  },
  {
    "input": "What language is the microsoft/unilm repository written in?",
    "output": "The BEiT: BERT Pre-Training of Image Transformers implementation is written in Python."
  },
  {
    "input": "Tell me the title of ArXiv paper 2101.00190.",
    "output": "The Power of Scale for Parameter-Efficient Prompt Tuning"
  },
  {
    "input": "What paper has ArXiv ID 2312.00752?",
    "output": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces"
  },
  {
    "input": "What topics does the ZeRO: Memory Optimizations Toward Training Trillion Parameter Models implementation cover?",
    "output": "The deepspeedai/DeepSpeed repository covers: deep-learning, pytorch, gpu, machine-learning, billion-parameters, data-parallelism, model-parallelism, inference, pipeline-parallelism, compression, mixture-of-experts, trillion-parameters, zero."
  },
  {
    "input": "What category is the Dense Passage Retrieval for Open-Domain Question Answering paper in?",
    "output": "Dense Passage Retrieval for Open-Domain Question Answering is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Describe the QingruZhang/AdaLoRA repository that implements AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning.",
    "output": "AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning (ICLR 2023). "
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2504.20734&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2504.20734&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2502.14815&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2502.14815&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at Open-Reasoner-Zero/Open-Reasoner-Zero which official repo for open-reasoner-zero It is written in Python. The repository has 2,068 stars, indicating high community interest."
  },
  {
    "input": "Where can I find the code for Flamingo: A Visual Language Model for Few-Shot Learning?",
    "output": "The code for Flamingo: A Visual Language Model for Few-Shot Learning is available at https://github.com/lucidrains/flamingo-pytorch (lucidrains/flamingo-pytorch)."
  },
  {
    "input": "Summarize the Med-PaLM 2: Towards Expert-Level Medical Question Answering paper.",
    "output": "Med-PaLM 2: Towards Expert-Level Medical Question Answering is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at google-research/google-research which google research It is written in Jupyter Notebook. The repository has 36,800 stars, indicating high community interest."
  },
  {
    "input": "When was the JAX: Composable transformations of Python+NumPy programs paper published?",
    "output": "The JAX: Composable transformations of Python+NumPy programs paper was published in 2019."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2502.19328&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2502.19328&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at LTH14/fractalgen which pytorch implementation of fractalgen https://arxiv.org/abs/2502.17437 It is written in Python. The repository has 1,196 stars, indicating high community interest."
  },
  {
    "input": "Which field does QLoRA: Efficient Finetuning of Quantized LLMs belong to?",
    "output": "QLoRA: Efficient Finetuning of Quantized LLMs is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Describe the microsoft/DeBERTa repository that implements DeBERTa: Decoding-enhanced BERT with Disentangled Attention.",
    "output": "The implementation of DeBERTa"
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2503.22708&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2503.22708&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What language is the mit-han-lab/offsite-tuning repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2302.04870&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What does the hpcaitech/Open-Sora repository do?",
    "output": "Open-Sora: Democratizing Efficient Video Production for All"
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2012.06009&start=0&max_results=10 repository have?",
    "output": "The facebookresearch/CovidPrognosis repository has 158 stars and 38 forks on GitHub."
  },
  {
    "input": "What year was GPT-4 Technical Report released?",
    "output": "The GPT-4 Technical Report paper was published in 2023."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2501.13824&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2501.13824&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "Tell me the title of ArXiv paper 2504.01848.",
    "output": "arXiv Query: search_query=&id_list=2504.01848&start=0&max_results=10"
  },
  {
    "input": "How popular is the Flamingo: A Visual Language Model for Few-Shot Learning implementation?",
    "output": "The lucidrains/flamingo-pytorch repository has 1,270 stars and 66 forks on GitHub."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2410.18050&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2410.18050&start=0&max_results=10 is available at https://github.com/ibm-granite/granite-3.0-language-models (ibm-granite/granite-3.0-language-models)."
  },
  {
    "input": "What are the key topics in the facebookresearch/fairseq repository?",
    "output": "The facebookresearch/fairseq repository covers: python, pytorch, artificial-intelligence."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2504.16084&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2504.16084&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at Crista23/goal_directedness_llms It is written in Python. The repository has 12 stars, indicating high community interest."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2503.00735&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2503.00735&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at LTH14/fractalgen which pytorch implementation of fractalgen https://arxiv.org/abs/2502.17437 It is written in Python. The repository has 1,196 stars, indicating high community interest."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2102.01192&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2102.01192&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is the GitHub repository for LLaVA: Large Language and Vision Assistant?",
    "output": "The code for LLaVA: Large Language and Vision Assistant is available at https://github.com/haotian-liu/LLaVA (haotian-liu/LLaVA)."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2406.06326&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2406.06326&start=0&max_results=10 is a 2024 paper in Machine Learning. The implementation is available at lamini-ai/Lamini-Memory-Tuning which banishing llm hallucinations requires rethinking generalization The repository has 275 stars, indicating high community interest."
  },
  {
    "input": "What's the implementation repository for HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction?",
    "output": "The code for HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction is available at https://github.com/facebookresearch/fairseq (facebookresearch/fairseq)."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2006.11239?",
    "output": "Denoising Diffusion Probabilistic Models"
  },
  {
    "input": "In what year did Training language models to follow instructions with human feedback come out?",
    "output": "The Training language models to follow instructions with human feedback paper was published in 2022."
  },
  {
    "input": "In what year did The Flan Collection: Designing Data and Methods for Effective Instruction Tuning come out?",
    "output": "The The Flan Collection: Designing Data and Methods for Effective Instruction Tuning paper was published in 2023."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2205.00445?",
    "output": "MRKL Systems: A modular approach to AGI"
  },
  {
    "input": "What category is the T5: Exploring the Limits of Transfer Learning paper in?",
    "output": "T5: Exploring the Limits of Transfer Learning is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What is PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel about?",
    "output": "PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel is a 2022 paper in Distributed, Parallel, and Cluster Computing. The implementation is available at pytorch/pytorch which tensors and dynamic neural networks in python with strong gpu acceleration It is written in Python. The repository has 95,348 stars, indicating high community interest."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2004.13637?",
    "output": "arXiv Query: search_query=&id_list=2004.13637&start=0&max_results=10"
  },
  {
    "input": "What is AWQ: Activation-aware Weight Quantization for LLM Compression about?",
    "output": "AWQ: Activation-aware Weight Quantization for LLM Compression is a 2023 paper in Machine Learning. The implementation is available at mit-han-lab/llm-awq which [mlsys 2024 best paper award] awq: activation-aware weight quantization for llm compression and acceleration It is written in Python. The repository has 3,357 stars, indicating high community interest."
  },
  {
    "input": "How popular is the ELECTRA: Pre-training Text Encoders as Discriminators implementation?",
    "output": "The google-research/electra repository has 2,365 stars and 350 forks on GitHub."
  },
  {
    "input": "Explain the CompVis/latent-diffusion implementation of High-Resolution Image Synthesis with Latent Diffusion Models.",
    "output": "High-Resolution Image Synthesis with Latent Diffusion Models"
  },
  {
    "input": "Explain the ali-vilab/composer implementation of arXiv Query: search_query=&id_list=2302.09778&start=0&max_results=10.",
    "output": "Official implementation of \"Composer: Creative and Controllable Image Synthesis with Composable Conditions\""
  },
  {
    "input": "In what year did MiniGPT-4: Enhancing Vision-Language Understanding come out?",
    "output": "The MiniGPT-4: Enhancing Vision-Language Understanding paper was published in 2023."
  },
  {
    "input": "Tell me the title of ArXiv paper 2205.00445.",
    "output": "MRKL Systems: A modular approach to AGI"
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2303.04226&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2303.04226&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2303.03378&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2303.03378&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "When was the High-Resolution Image Synthesis with Latent Diffusion Models paper published?",
    "output": "The High-Resolution Image Synthesis with Latent Diffusion Models paper was published in 2021."
  },
  {
    "input": "What's the implementation repository for ControlNet: Adding Conditional Control to Text-to-Image Diffusion?",
    "output": "The code for ControlNet: Adding Conditional Control to Text-to-Image Diffusion is available at https://github.com/lllyasviel/ControlNet (lllyasviel/ControlNet)."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2301.11316&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2301.11316&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "How popular is the Attention Is All You Need implementation?",
    "output": "The tensorflow/tensor2tensor repository has 16,760 stars and 3,702 forks on GitHub."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2301.11305&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2301.11305&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at autonomousvision/stylegan-t which [icml'23] stylegan-t: unlocking the power of gans for fast large-scale text-to-image synthesis It is written in Python. The repository has 1,196 stars, indicating high community interest."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2302.10866&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2302.10866&start=0&max_results=10 is available at https://github.com/HazyResearch/safari (HazyResearch/safari)."
  },
  {
    "input": "Tell me the title of ArXiv paper 1609.02907.",
    "output": "Semi-Supervised Classification with Graph Convolutional Networks"
  },
  {
    "input": "What is the ArXiv category of WizardCoder: Empowering Code LLMs with Evol-Instruct?",
    "output": "WizardCoder: Empowering Code LLMs with Evol-Instruct is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What paper has ArXiv ID 2203.07814?",
    "output": "Competition-Level Code Generation with AlphaCode"
  },
  {
    "input": "What paper has ArXiv ID 2303.09431?",
    "output": "arXiv Query: search_query=&id_list=2303.09431&start=0&max_results=10"
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2301.13188&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2301.13188&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Which field does Codex: Evaluating Large Language Models Trained on Code belong to?",
    "output": "Codex: Evaluating Large Language Models Trained on Code is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Explain the microsoft/unilm implementation of BEiT: BERT Pre-Training of Image Transformers.",
    "output": "Large-scale Self-supervised Pre-training Across Tasks, Languages, and Modalities"
  },
  {
    "input": "Explain the dmlc/dgl implementation of Deep Graph Library: A Graph-Centric, Highly-Performant Package.",
    "output": "Python package built to ease deep learning on graph, on top of existing DL frameworks."
  },
  {
    "input": "What does the facebookresearch/ConvNeXt-V2 repository do?",
    "output": "Code release for ConvNeXt V2 model"
  },
  {
    "input": "Tell me the title of ArXiv paper 2004.08492.",
    "output": "arXiv Query: search_query=&id_list=2004.08492&start=0&max_results=10"
  },
  {
    "input": "When was the RoFormer: Enhanced Transformer with Rotary Position Embedding paper published?",
    "output": "The RoFormer: Enhanced Transformer with Rotary Position Embedding paper was published in 2021."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2302.04023&start=0&max_results=10 implementation?",
    "output": "The mit-han-lab/offsite-tuning repository has 382 stars and 39 forks on GitHub."
  },
  {
    "input": "What paper has ArXiv ID 2201.08239?",
    "output": "Contriever: Unsupervised Dense Information Retrieval"
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2303.02506&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2303.02506&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at NVlabs/prismer which the implementation of \"prismer: a vision-language model with multi-task experts\". It is written in Python. The repository has 1,307 stars, indicating high community interest."
  },
  {
    "input": "What is the ArXiv category of ViT: An Image is Worth 16x16 Words?",
    "output": "ViT: An Image is Worth 16x16 Words is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "Tell me the title of ArXiv paper 2303.17580.",
    "output": "HuggingGPT: Solving AI Tasks with ChatGPT"
  },
  {
    "input": "What are the key topics in the tensorflow/tensor2tensor repository?",
    "output": "The tensorflow/tensor2tensor repository covers: machine-learning, machine-translation, deep-learning, reinforcement-learning, tpu."
  },
  {
    "input": "How many stars does the TensorFlow: A System for Large-Scale Machine Learning repository have?",
    "output": "The tensorflow/tensorflow repository has 192,543 stars and 74,998 forks on GitHub."
  },
  {
    "input": "What topics does the arXiv Query: search_query=&id_list=2004.08492&start=0&max_results=10 implementation cover?",
    "output": "The uber/orbit repository covers: python, forecasting, bayesian, exponential-smoothing, pyro, stan, pystan, probabilistic-programming, probabilistic, forecast, orbit, time-series, regression, arima, changepoint, bayesian-methods, bayesian-statistics, machine-learning, regression-models, pytorch."
  },
  {
    "input": "Where can I find the code for AWQ: Activation-aware Weight Quantization for LLM Compression?",
    "output": "The code for AWQ: Activation-aware Weight Quantization for LLM Compression is available at https://github.com/mit-han-lab/llm-awq (mit-han-lab/llm-awq)."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2404.03592&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2404.03592&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Where can I find the code for DistilBERT: a distilled version of BERT?",
    "output": "The code for DistilBERT: a distilled version of BERT is available at https://github.com/huggingface/transformers (huggingface/transformers)."
  },
  {
    "input": "Explain the openai/CLIP implementation of CLIP: Learning Transferable Visual Models From Natural Language.",
    "output": "CLIP (Contrastive Language-Image Pretraining),  Predict the most relevant text snippet given an image"
  },
  {
    "input": "What paper has ArXiv ID 1503.02531?",
    "output": "Distilling the Knowledge in a Neural Network"
  },
  {
    "input": "In what year did Graph Attention Networks come out?",
    "output": "The Graph Attention Networks paper was published in 2017."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2504.21801&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2504.21801&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "Explain the MoonshotAI/Kimi-Audio implementation of arXiv Query: search_query=&id_list=2504.20734&start=0&max_results=10.",
    "output": "Kimi-Audio, an open-source audio foundation model excelling in audio understanding, generation, and conversation"
  },
  {
    "input": "What does the nlpxucan/WizardLM repository do?",
    "output": "LLMs build upon Evol Insturct: WizardLM, WizardCoder, WizardMath"
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2302.04870&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2302.04870&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at mit-han-lab/offsite-tuning which offsite-tuning: transfer learning without full model It is written in Python. The repository has 382 stars, indicating high community interest."
  },
  {
    "input": "What year was Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks released?",
    "output": "The Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks paper was published in 2019."
  },
  {
    "input": "When was the Billion-scale similarity search with GPUs paper published?",
    "output": "The Billion-scale similarity search with GPUs paper was published in 2019."
  },
  {
    "input": "What category is the Faster R-CNN: Towards Real-Time Object Detection paper in?",
    "output": "Faster R-CNN: Towards Real-Time Object Detection is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2502.14815&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2502.14815&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "Describe the openai/CLIP repository that implements CLIP: Learning Transferable Visual Models From Natural Language.",
    "output": "CLIP (Contrastive Language-Image Pretraining),  Predict the most relevant text snippet given an image"
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2301.10343&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2301.10343&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "When was the FlashAttention: Fast and Memory-Efficient Exact Attention paper published?",
    "output": "The FlashAttention: Fast and Memory-Efficient Exact Attention paper was published in 2022."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2106.07447?",
    "output": "HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction"
  },
  {
    "input": "What paper has ArXiv ID 2403.17887?",
    "output": "arXiv Query: search_query=&id_list=2403.17887&start=0&max_results=10"
  },
  {
    "input": "What programming language is TinyBERT: Distilling BERT for Natural Language Understanding implemented in?",
    "output": "The TinyBERT: Distilling BERT for Natural Language Understanding implementation is written in Python."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2009.06367&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2009.06367&start=0&max_results=10 is a 2020 paper in Machine Learning. The implementation is available at salesforce/GeDi which gedi: generative discriminator guided sequence generation It is written in Python. The repository has 209 stars, indicating high community interest."
  },
  {
    "input": "What programming language is Time-LLM: Time Series Forecasting by Reprogramming Large Language Models implemented in?",
    "output": "The Time-LLM: Time Series Forecasting by Reprogramming Large Language Models implementation is written in Python."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2301.11305&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2301.11305&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2405.04324?",
    "output": "arXiv Query: search_query=&id_list=2405.04324&start=0&max_results=10"
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2302.10866&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2302.10866&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What paper has ArXiv ID 2103.00020?",
    "output": "CLIP: Learning Transferable Visual Models From Natural Language"
  },
  {
    "input": "Which field does LLaVA-1.5: Improved Baselines with Visual Instruction Tuning belong to?",
    "output": "LLaVA-1.5: Improved Baselines with Visual Instruction Tuning is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2501.19393&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2501.19393&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What language is the facebookresearch/mae repository written in?",
    "output": "The Masked Autoencoders Are Scalable Vision Learners implementation is written in Python."
  },
  {
    "input": "What does the openai/gpt-3 repository do?",
    "output": "GPT-3: Language Models are Few-Shot Learners"
  },
  {
    "input": "What does the mistralai/mistral-inference repository do?",
    "output": "Official inference library for Mistral models"
  },
  {
    "input": "Explain the ashawkey/stable-dreamfusion implementation of DreamFusion: Text-to-3D using 2D Diffusion.",
    "output": "Text-to-3D & Image-to-3D & Mesh Exportation with NeRF + Diffusion."
  },
  {
    "input": "Give me an overview of LLaVA-1.5: Improved Baselines with Visual Instruction Tuning.",
    "output": "LLaVA-1.5: Improved Baselines with Visual Instruction Tuning is a 2023 paper in Computer Vision and Pattern Recognition. The implementation is available at haotian-liu/LLaVA which [neurips'23 oral] visual instruction tuning (llava) built towards gpt-4v level capabilities and beyond. It is written in Python. The repository has 24,034 stars, indicating high community interest."
  },
  {
    "input": "What is the ArXiv category of Faster R-CNN: Towards Real-Time Object Detection?",
    "output": "Faster R-CNN: Towards Real-Time Object Detection is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "Tell me the title of ArXiv paper 2205.01917.",
    "output": "Masked Autoencoders Are Scalable Vision Learners"
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2504.02732&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2504.02732&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at MAXNORM8650/MedAgentSim which medagentsim: self-evolving multi-agent simulations for realistic clinical interactions, miccai 2025 (oral and early accepted) It is written in Python. The repository has 94 stars, indicating high community interest."
  },
  {
    "input": "What year was Prefix-Tuning: Optimizing Continuous Prompts for Generation released?",
    "output": "The Prefix-Tuning: Optimizing Continuous Prompts for Generation paper was published in 2021."
  },
  {
    "input": "When was the GPT-4 Technical Report paper published?",
    "output": "The GPT-4 Technical Report paper was published in 2023."
  },
  {
    "input": "What language is the tmikolov/word2vec repository written in?",
    "output": "The Efficient Estimation of Word Representations in Vector Space implementation is written in C."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2303.03378&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2303.03378&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at NVlabs/prismer which the implementation of \"prismer: a vision-language model with multi-task experts\". It is written in Python. The repository has 1,307 stars, indicating high community interest."
  },
  {
    "input": "Explain the Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context paper.",
    "output": "Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context is a 2019 paper in Computation and Language (Natural Language Processing). The implementation is available at kimiyoung/transformer-xl It is written in Python. The repository has 3,679 stars, indicating high community interest."
  },
  {
    "input": "Explain the sentient-agi/OpenDeepSearch implementation of arXiv Query: search_query=&id_list=2503.20201&start=0&max_results=10.",
    "output": "SOTA search powered LLM"
  },
  {
    "input": "Summarize the Semi-Supervised Classification with Graph Convolutional Networks paper.",
    "output": "Semi-Supervised Classification with Graph Convolutional Networks is a 2016 paper in Machine Learning. The implementation is available at tkipf/gcn which implementation of graph convolutional networks in tensorflow It is written in Python. The repository has 7,334 stars, indicating high community interest."
  },
  {
    "input": "What does the lucidrains/flamingo-pytorch repository do?",
    "output": "Implementation of \ud83e\udda9 Flamingo, state-of-the-art few-shot visual question answering attention net out of Deepmind, in Pytorch"
  },
  {
    "input": "How popular is the TinyBERT: Distilling BERT for Natural Language Understanding implementation?",
    "output": "The huawei-noah/Pretrained-Language-Model repository has 3,149 stars and 644 forks on GitHub."
  },
  {
    "input": "In what year did Dream to Control: Learning Behaviors by Latent Imagination come out?",
    "output": "The Dream to Control: Learning Behaviors by Latent Imagination paper was published in 2019."
  },
  {
    "input": "What year was Zephyr: Direct Distillation of LM Alignment released?",
    "output": "The Zephyr: Direct Distillation of LM Alignment paper was published in 2023."
  },
  {
    "input": "How many stars does the DistilBERT: a distilled version of BERT repository have?",
    "output": "The huggingface/transformers repository has 152,994 stars and 31,218 forks on GitHub."
  },
  {
    "input": "What language is the JohnNay/llm-lobbyist repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2301.01181&start=0&max_results=10 implementation is written in Jupyter Notebook."
  },
  {
    "input": "How many stars does the ViT: An Image is Worth 16x16 Words repository have?",
    "output": "The google-research/vision_transformer repository has 12,040 stars and 1,432 forks on GitHub."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2405.03548&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2405.03548&start=0&max_results=10 is available at https://github.com/ibm-granite/granite-code-models (ibm-granite/granite-code-models)."
  },
  {
    "input": "What does the facebookresearch/ConvNeXt-V2 repository do?",
    "output": "Code release for ConvNeXt V2 model"
  },
  {
    "input": "Describe the google-research/electra repository that implements ELECTRA: Pre-training Text Encoders as Discriminators.",
    "output": "ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators"
  },
  {
    "input": "What language is the hexiangnan/neural_collaborative_filtering repository written in?",
    "output": "The Neural Collaborative Filtering implementation is written in Python."
  },
  {
    "input": "What is the GitHub repository for Self-RAG: Learning to Retrieve, Generate, and Critique?",
    "output": "The code for Self-RAG: Learning to Retrieve, Generate, and Critique is available at https://github.com/AkariAsai/self-rag (AkariAsai/self-rag)."
  },
  {
    "input": "What paper has ArXiv ID 1909.10351?",
    "output": "DistilBERT: a distilled version of BERT"
  },
  {
    "input": "What paper has ArXiv ID 2303.06865?",
    "output": "arXiv Query: search_query=&id_list=2303.06865&start=0&max_results=10"
  },
  {
    "input": "Explain the tensorflow/tensorflow implementation of TensorFlow: A System for Large-Scale Machine Learning.",
    "output": "An Open Source Machine Learning Framework for Everyone"
  },
  {
    "input": "What topics does the arXiv Query: search_query=&id_list=2302.07459&start=0&max_results=10 implementation cover?",
    "output": "The haoliuhl/language-quantized-autoencoders repository covers: autoencoders, bert, large-language-models, multimodal, roberta, vq, vqvae."
  },
  {
    "input": "What paper has ArXiv ID 2208.01618?",
    "output": "DreamBooth: Fine Tuning Text-to-Image Diffusion Models"
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2301.00303&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2301.00303&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What does the MAXNORM8650/MedAgentSim repository do?",
    "output": "MedAgentSim: Self-Evolving Multi-Agent Simulations for Realistic Clinical Interactions, MICCAI 2025 (oral and early accepted)"
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2310.03744?",
    "output": "LLaVA-1.5: Improved Baselines with Visual Instruction Tuning"
  },
  {
    "input": "Explain the BERT Rediscovers the Classical NLP Pipeline paper.",
    "output": "BERT Rediscovers the Classical NLP Pipeline is a 2019 paper in Computation and Language (Natural Language Processing). The implementation is available at clarkkev/attention-analysis It is written in Jupyter Notebook. The repository has 469 stars, indicating high community interest."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2303.04671&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2303.04671&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2502.17437&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2502.17437&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at LTH14/fractalgen which pytorch implementation of fractalgen https://arxiv.org/abs/2502.17437 It is written in Python. The repository has 1,196 stars, indicating high community interest."
  },
  {
    "input": "What is the ArXiv category of Multi-Task Deep Neural Networks for Natural Language Understanding?",
    "output": "Multi-Task Deep Neural Networks for Natural Language Understanding is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Where can I find the code for Longformer: The Long-Document Transformer?",
    "output": "The code for Longformer: The Long-Document Transformer is available at https://github.com/allenai/longformer (allenai/longformer)."
  },
  {
    "input": "What programming language is Know What You Don't Know: Unanswerable Questions for SQuAD implemented in?",
    "output": "The Know What You Don't Know: Unanswerable Questions for SQuAD implementation is written in JavaScript."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2501.13545&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2501.13545&start=0&max_results=10 is available at https://github.com/plurai-ai/intellagent (plurai-ai/intellagent)."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2405.02803&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2405.02803&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Tell me the title of ArXiv paper 1909.10351.",
    "output": "DistilBERT: a distilled version of BERT"
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2301.13188&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2301.13188&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at amazon-science/mm-cot which official implementation for \"multimodal chain-of-thought reasoning in language models\" (stay tuned and more will be updated) It is written in Python. The repository has 3,982 stars, indicating high community interest."
  },
  {
    "input": "How popular is the High-Resolution Image Synthesis with Latent Diffusion Models implementation?",
    "output": "The CompVis/latent-diffusion repository has 13,589 stars and 1,697 forks on GitHub."
  },
  {
    "input": "What programming language is SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities implemented in?",
    "output": "The SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities implementation is written in Python."
  },
  {
    "input": "Explain the MRKL Systems: A modular approach to AGI paper.",
    "output": "MRKL Systems: A modular approach to AGI is a 2022 paper in Artificial Intelligence. The implementation is available at langchain-ai/langchain which \ud83e\udd9c\ud83d\udd17 the platform for reliable agents. It is written in Python. The repository has 120,451 stars, indicating high community interest."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2303.01469&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2303.01469&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at ruiqi-zhong/D5 which the github repo for goal driven discovery of distributional differences via language descriptions It is written in Python. The repository has 71 stars, indicating high community interest."
  },
  {
    "input": "Summarize the Efficient Estimation of Word Representations in Vector Space paper.",
    "output": "Efficient Estimation of Word Representations in Vector Space is a 2013 paper in Computation and Language (Natural Language Processing). The implementation is available at tmikolov/word2vec which automatically exported from code.google.com/p/word2vec It is written in C. The repository has 1,569 stars, indicating high community interest."
  },
  {
    "input": "Where can I find the code for YOLOX: Exceeding YOLO Series in 2021?",
    "output": "The code for YOLOX: Exceeding YOLO Series in 2021 is available at https://github.com/Megvii-BaseDetection/YOLOX (Megvii-BaseDetection/YOLOX)."
  },
  {
    "input": "Give me an overview of Self-RAG: Learning to Retrieve, Generate, and Critique.",
    "output": "Self-RAG: Learning to Retrieve, Generate, and Critique is a 2022 paper in Computation and Language (Natural Language Processing). The implementation is available at AkariAsai/self-rag which this includes the original implementation of self-rag: learning to retrieve, generate and critique through self-reflection by akari asai, zeqiu wu, yizhong wang, avirup sil, and hannaneh hajishirzi. It is written in Python. The repository has 2,252 stars, indicating high community interest."
  },
  {
    "input": "What's the implementation repository for High-Resolution Image Synthesis with Latent Diffusion Models?",
    "output": "The code for High-Resolution Image Synthesis with Latent Diffusion Models is available at https://github.com/CompVis/latent-diffusion (CompVis/latent-diffusion)."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2109.03264&start=0&max_results=10 repository have?",
    "output": "The facebookresearch/fairseq repository has 31,988 stars and 6,630 forks on GitHub."
  },
  {
    "input": "Summarize the Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks paper.",
    "output": "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks is a 2019 paper in Computation and Language (Natural Language Processing). The implementation is available at huggingface/sentence-transformers which state-of-the-art text embeddings It is written in Python. The repository has 17,904 stars, indicating high community interest."
  },
  {
    "input": "What language is the MoonshotAI/Kimi-Audio repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2504.19413&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Explain the Segment Anything in Medical Images paper.",
    "output": "Segment Anything in Medical Images is a 2023 paper in Computer Vision and Pattern Recognition. The implementation is available at bowang-lab/MedSAM which segment anything in medical images It is written in Jupyter Notebook. The repository has 3,930 stars, indicating high community interest."
  },
  {
    "input": "Where can I find the code for Llama 2: Open Foundation and Fine-Tuned Chat Models?",
    "output": "The code for Llama 2: Open Foundation and Fine-Tuned Chat Models is available at https://github.com/meta-llama/llama (meta-llama/llama)."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2103.00020?",
    "output": "CLIP: Learning Transferable Visual Models From Natural Language"
  },
  {
    "input": "Describe the ermongroup/ddim repository that implements Denoising Diffusion Implicit Models.",
    "output": "Denoising Diffusion Implicit Models"
  },
  {
    "input": "When was the Sequence to Sequence Learning with Neural Networks paper published?",
    "output": "The Sequence to Sequence Learning with Neural Networks paper was published in 2014."
  },
  {
    "input": "What paper has ArXiv ID 2301.04856?",
    "output": "arXiv Query: search_query=&id_list=2301.04856&start=0&max_results=10"
  },
  {
    "input": "Summarize the MMLU: Measuring Massive Multitask Language Understanding paper.",
    "output": "MMLU: Measuring Massive Multitask Language Understanding is a 2021 paper in cs.CY. The implementation is available at hendrycks/test which measuring massive multitask language understanding | iclr 2021 It is written in Python. The repository has 1,523 stars, indicating high community interest."
  },
  {
    "input": "What is LLM.int8(): 8-bit Matrix Multiplication for Transformers about?",
    "output": "LLM.int8(): 8-bit Matrix Multiplication for Transformers is a 2022 paper in Machine Learning. The implementation is available at bitsandbytes-foundation/bitsandbytes which accessible large language models via k-bit quantization for pytorch. It is written in Python. The repository has 7,771 stars, indicating high community interest."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2004.13637&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2004.13637&start=0&max_results=10 is available at https://github.com/facebookresearch/TransCoder (facebookresearch/TransCoder)."
  },
  {
    "input": "What is Deep Residual Learning for Image Recognition about?",
    "output": "Deep Residual Learning for Image Recognition is a 2015 paper in Computer Vision and Pattern Recognition. The implementation is available at KaimingHe/deep-residual-networks which deep residual learning for image recognition  The repository has 6,664 stars, indicating high community interest."
  },
  {
    "input": "Explain the GAIR-NLP/LIMO implementation of arXiv Query: search_query=&id_list=2502.01061&start=0&max_results=10.",
    "output": "[COLM 2025] LIMO: Less is More for Reasoning"
  },
  {
    "input": "Tell me the title of ArXiv paper 2204.02311.",
    "output": "PaLM: Scaling Language Modeling with Pathways"
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2303.04129&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2303.04129&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Describe the facebookresearch/fairseq repository that implements RoBERTa: A Robustly Optimized BERT Pretraining Approach.",
    "output": "Facebook AI Research Sequence-to-Sequence Toolkit written in Python."
  },
  {
    "input": "What is Generating Wikipedia by Summarizing Long Sequences about?",
    "output": "Generating Wikipedia by Summarizing Long Sequences is a 2018 paper in Computation and Language (Natural Language Processing). The implementation is available at tensorflow/tensor2tensor which library of deep learning models and datasets designed to make deep learning more accessible and accelerate ml research. It is written in Python. The repository has 16,760 stars, indicating high community interest."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2301.01181&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2301.01181&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Which field does RoFormer: Enhanced Transformer with Rotary Position Embedding belong to?",
    "output": "RoFormer: Enhanced Transformer with Rotary Position Embedding is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What's the implementation repository for YOLOX: Exceeding YOLO Series in 2021?",
    "output": "The code for YOLOX: Exceeding YOLO Series in 2021 is available at https://github.com/Megvii-BaseDetection/YOLOX (Megvii-BaseDetection/YOLOX)."
  },
  {
    "input": "What language is the Megvii-BaseDetection/YOLOX repository written in?",
    "output": "The YOLOX: Exceeding YOLO Series in 2021 implementation is written in Python."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2101.04909&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2101.04909&start=0&max_results=10 paper was published in 2021."
  },
  {
    "input": "Which field does Dream to Control: Learning Behaviors by Latent Imagination belong to?",
    "output": "Dream to Control: Learning Behaviors by Latent Imagination is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Where can I find the code for Graph Attention Networks?",
    "output": "The code for Graph Attention Networks is available at https://github.com/PetarV-/GAT (PetarV-/GAT)."
  },
  {
    "input": "What paper has ArXiv ID 2301.02828?",
    "output": "arXiv Query: search_query=&id_list=2301.02828&start=0&max_results=10"
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2502.17437&start=0&max_results=10 repository have?",
    "output": "The LTH14/fractalgen repository has 1,196 stars and 65 forks on GitHub."
  },
  {
    "input": "What is Neural Message Passing for Quantum Chemistry about?",
    "output": "Neural Message Passing for Quantum Chemistry is a 2017 paper in Machine Learning. The implementation is available at priba/nmp_qc which our own implementation of neural message passing for computer vision paper It is written in Python. The repository has 343 stars, indicating high community interest."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2301.09515&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2301.09515&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What paper has ArXiv ID 2205.01917?",
    "output": "Masked Autoencoders Are Scalable Vision Learners"
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2504.00698&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2504.00698&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "How popular is the Neural Machine Translation by Jointly Learning to Align and Translate implementation?",
    "output": "The lisa-groundhog/GroundHog repository has 613 stars and 230 forks on GitHub."
  },
  {
    "input": "What does the facebookresearch/fairseq repository do?",
    "output": "Facebook AI Research Sequence-to-Sequence Toolkit written in Python."
  },
  {
    "input": "Tell me the title of ArXiv paper 2103.14899.",
    "output": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows"
  },
  {
    "input": "Tell me the title of ArXiv paper 2302.13007.",
    "output": "Synthetic Data Generation with Large Language Models"
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2504.20734&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2504.20734&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "What programming language is AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning implemented in?",
    "output": "The AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning implementation is written in Python."
  },
  {
    "input": "Summarize the FlashAttention-2: Faster Attention with Better Parallelism paper.",
    "output": "FlashAttention-2: Faster Attention with Better Parallelism is a 2023 paper in Machine Learning. The implementation is available at Dao-AILab/flash-attention which fast and memory-efficient exact attention It is written in Python. The repository has 20,719 stars, indicating high community interest."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2312.02120&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2312.02120&start=0&max_results=10 is available at https://github.com/ContextualAI/HALOs (ContextualAI/HALOs)."
  },
  {
    "input": "What paper has ArXiv ID 1409.1556?",
    "output": "Very Deep Convolutional Networks for Large-Scale Image Recognition"
  },
  {
    "input": "What does the MAXNORM8650/MedAgentSim repository do?",
    "output": "MedAgentSim: Self-Evolving Multi-Agent Simulations for Realistic Clinical Interactions, MICCAI 2025 (oral and early accepted)"
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2301.01181?",
    "output": "arXiv Query: search_query=&id_list=2301.01181&start=0&max_results=10"
  },
  {
    "input": "Explain the google-research/google-research implementation of Depthwise Separable Convolutions for Neural Machine Translation.",
    "output": "Google Research"
  },
  {
    "input": "Explain the allenai/codescientist implementation of arXiv Query: search_query=&id_list=2503.22708&start=0&max_results=10.",
    "output": "CodeScientist: An automated scientific discovery system for code-based experiments"
  },
  {
    "input": "What is the ArXiv category of Depthwise Separable Convolutions for Neural Machine Translation?",
    "output": "Depthwise Separable Convolutions for Neural Machine Translation is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2503.23513&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2503.23513&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What topics does the SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding implementation cover?",
    "output": "The nyu-mll/jiant repository covers: nlp, sentence-representation, bert, multitask-learning, transformers, transfer-learning."
  },
  {
    "input": "Tell me the title of ArXiv paper 2302.05543.",
    "output": "ControlNet: Adding Conditional Control to Text-to-Image Diffusion"
  },
  {
    "input": "Tell me the title of ArXiv paper 2502.03387.",
    "output": "arXiv Query: search_query=&id_list=2502.03387&start=0&max_results=10"
  },
  {
    "input": "Explain the williamleif/GraphSAGE implementation of Inductive Representation Learning on Large Graphs.",
    "output": "Representation learning on large graphs using stochastic graph convolutions."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2301.02828&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2301.02828&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Where can I find the code for RAG: Retrieval-Augmented Generation?",
    "output": "The code for RAG: Retrieval-Augmented Generation is available at https://github.com/facebookresearch/DPR (facebookresearch/DPR)."
  },
  {
    "input": "What is the ArXiv category of MRKL Systems: A modular approach to AGI?",
    "output": "MRKL Systems: A modular approach to AGI is in the Artificial Intelligence (cs.AI) category."
  },
  {
    "input": "Explain the Learning to summarize from human feedback paper.",
    "output": "Learning to summarize from human feedback is a 2020 paper in Computation and Language (Natural Language Processing). The implementation is available at openai/summarize-from-feedback which code for \"learning to summarize from human feedback\" It is written in Python. The repository has 1,055 stars, indicating high community interest."
  },
  {
    "input": "How popular is the ReAct: Synergizing Reasoning and Acting in Language Models implementation?",
    "output": "The ysymyth/ReAct repository has 3,225 stars and 325 forks on GitHub."
  },
  {
    "input": "What does the mit-han-lab/offsite-tuning repository do?",
    "output": "Offsite-Tuning: Transfer Learning without Full Model"
  },
  {
    "input": "What's the implementation repository for Feature Pyramid Networks for Object Detection?",
    "output": "The code for Feature Pyramid Networks for Object Detection is available at https://github.com/facebookresearch/Detectron (facebookresearch/Detectron)."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2109.03264&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2109.03264&start=0&max_results=10 paper was published in 2021."
  },
  {
    "input": "How popular is the Deep Graph Library: A Graph-Centric, Highly-Performant Package implementation?",
    "output": "The dmlc/dgl repository has 14,149 stars and 3,057 forks on GitHub."
  },
  {
    "input": "How many stars does the ControlNet: Adding Conditional Control to Text-to-Image Diffusion repository have?",
    "output": "The lllyasviel/ControlNet repository has 33,344 stars and 2,987 forks on GitHub."
  },
  {
    "input": "What language is the ur-whitelab/chemcrow-public repository written in?",
    "output": "The ChemCrow: Augmenting large-language models with chemistry tools implementation is written in Python."
  },
  {
    "input": "Tell me the title of ArXiv paper 1707.06347.",
    "output": "Proximal Policy Optimization Algorithms"
  },
  {
    "input": "Give me an overview of AWQ: Activation-aware Weight Quantization for LLM Compression.",
    "output": "AWQ: Activation-aware Weight Quantization for LLM Compression is a 2023 paper in Machine Learning. The implementation is available at mit-han-lab/llm-awq which [mlsys 2024 best paper award] awq: activation-aware weight quantization for llm compression and acceleration It is written in Python. The repository has 3,357 stars, indicating high community interest."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 1807.11164?",
    "output": "YOLOX: Exceeding YOLO Series in 2021"
  },
  {
    "input": "What does the lm-sys/FastChat repository do?",
    "output": "An open platform for training, serving, and evaluating large language models. Release repo for Vicuna and Chatbot Arena."
  },
  {
    "input": "What's the implementation repository for PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel?",
    "output": "The code for PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel is available at https://github.com/pytorch/pytorch (pytorch/pytorch)."
  },
  {
    "input": "What are the key topics in the mistralai/mistral-inference repository?",
    "output": "The mistralai/mistral-inference repository covers: llm, llm-inference, mistralai."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2301.13688&start=0&max_results=10 repository have?",
    "output": "The amazon-science/mm-cot repository has 3,982 stars and 333 forks on GitHub."
  },
  {
    "input": "In what year did Swin Transformer: Hierarchical Vision Transformer using Shifted Windows come out?",
    "output": "The Swin Transformer: Hierarchical Vision Transformer using Shifted Windows paper was published in 2021."
  },
  {
    "input": "What topics does the arXiv Query: search_query=&id_list=2301.01751&start=0&max_results=10 implementation cover?",
    "output": "The oughtinc/ice repository covers: debugging, gpt-3, python, language-model."
  },
  {
    "input": "What language is the MoonshotAI/Kimi-Audio repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2504.20734&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2302.03917&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2302.03917&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What are the key topics in the uber/orbit repository?",
    "output": "The uber/orbit repository covers: python, forecasting, bayesian, exponential-smoothing, pyro, stan, pystan, probabilistic-programming, probabilistic, forecast, orbit, time-series, regression, arima, changepoint, bayesian-methods, bayesian-statistics, machine-learning, regression-models, pytorch."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2505.22101&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2505.22101&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "Where can I find the code for Self-RAG: Learning to Retrieve, Generate, and Critique?",
    "output": "The code for Self-RAG: Learning to Retrieve, Generate, and Critique is available at https://github.com/AkariAsai/self-rag (AkariAsai/self-rag)."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2302.00902&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2302.00902&start=0&max_results=10 is available at https://github.com/haoliuhl/language-quantized-autoencoders (haoliuhl/language-quantized-autoencoders)."
  },
  {
    "input": "How many stars does the Deep Residual Learning for Image Recognition repository have?",
    "output": "The KaimingHe/deep-residual-networks repository has 6,664 stars and 2,231 forks on GitHub."
  },
  {
    "input": "What does the tkipf/gcn repository do?",
    "output": "Implementation of Graph Convolutional Networks in TensorFlow"
  },
  {
    "input": "How many stars does the The Power of Scale for Parameter-Efficient Prompt Tuning repository have?",
    "output": "The google-research/prompt-tuning repository has 698 stars and 62 forks on GitHub."
  },
  {
    "input": "What is the ArXiv category of GPT-3: Language Models are Few-Shot Learners?",
    "output": "GPT-3: Language Models are Few-Shot Learners is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2403.17887&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2403.17887&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2502.12115&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2502.12115&start=0&max_results=10 is available at https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero (Open-Reasoner-Zero/Open-Reasoner-Zero)."
  },
  {
    "input": "Tell me the title of ArXiv paper 2002.04745.",
    "output": "Lookahead Optimizer: k steps forward, 1 step back"
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2012.06009&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2012.06009&start=0&max_results=10 paper was published in 2020."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2405.03520&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2405.03520&start=0&max_results=10 is a 2024 paper in Machine Learning. The implementation is available at ibm-granite/granite-code-models which granite code models: a family of open foundation models for code intelligence The repository has 1,240 stars, indicating high community interest."
  },
  {
    "input": "What paper has ArXiv ID 1607.01759?",
    "output": "Enriching Word Vectors with Subword Information"
  },
  {
    "input": "Tell me the title of ArXiv paper 2004.04906.",
    "output": "Dense Passage Retrieval for Open-Domain Question Answering"
  },
  {
    "input": "What programming language is TensorFlow: A System for Large-Scale Machine Learning implemented in?",
    "output": "The TensorFlow: A System for Large-Scale Machine Learning implementation is written in C++."
  },
  {
    "input": "What is BERT Rediscovers the Classical NLP Pipeline about?",
    "output": "BERT Rediscovers the Classical NLP Pipeline is a 2019 paper in Computation and Language (Natural Language Processing). The implementation is available at clarkkev/attention-analysis It is written in Jupyter Notebook. The repository has 469 stars, indicating high community interest."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2006.03511&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2006.03511&start=0&max_results=10 is available at https://github.com/facebookresearch/TransCoder (facebookresearch/TransCoder)."
  },
  {
    "input": "Tell me the title of ArXiv paper 2503.20201.",
    "output": "arXiv Query: search_query=&id_list=2503.20201&start=0&max_results=10"
  },
  {
    "input": "When was the LLaMA-Adapter: Efficient Fine-tuning of Language Models paper published?",
    "output": "The LLaMA-Adapter: Efficient Fine-tuning of Language Models paper was published in 2023."
  },
  {
    "input": "Tell me the title of ArXiv paper 2203.05556.",
    "output": "FlashAttention: Fast and Memory-Efficient Exact Attention"
  },
  {
    "input": "How popular is the Faster R-CNN: Towards Real-Time Object Detection implementation?",
    "output": "The rbgirshick/py-faster-rcnn repository has 8,269 stars and 4,102 forks on GitHub."
  },
  {
    "input": "What is ControlNet: Adding Conditional Control to Text-to-Image Diffusion about?",
    "output": "ControlNet: Adding Conditional Control to Text-to-Image Diffusion is a 2023 paper in Computer Vision and Pattern Recognition. The implementation is available at lllyasviel/ControlNet which let us control diffusion models! It is written in Python. The repository has 33,344 stars, indicating high community interest."
  },
  {
    "input": "Tell me the title of ArXiv paper 1910.10683.",
    "output": "T5: Exploring the Limits of Transfer Learning"
  },
  {
    "input": "What language is the allenai/codescientist repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2504.00698&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2406.06326&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2406.06326&start=0&max_results=10 is a 2024 paper in Machine Learning. The implementation is available at lamini-ai/Lamini-Memory-Tuning which banishing llm hallucinations requires rethinking generalization The repository has 275 stars, indicating high community interest."
  },
  {
    "input": "What topics does the CLIP: Learning Transferable Visual Models From Natural Language implementation cover?",
    "output": "The openai/CLIP repository covers: deep-learning, machine-learning."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2504.19413&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2504.19413&start=0&max_results=10 is available at https://github.com/MoonshotAI/Kimi-Audio (MoonshotAI/Kimi-Audio)."
  },
  {
    "input": "Tell me the title of ArXiv paper 2301.12597.",
    "output": "BLIP-2: Bootstrapping Language-Image Pre-training"
  },
  {
    "input": "What category is the SimCSE: Simple Contrastive Learning of Sentence Embeddings paper in?",
    "output": "SimCSE: Simple Contrastive Learning of Sentence Embeddings is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What does the plurai-ai/intellagent repository do?",
    "output": "A framework for comprehensive diagnosis and optimization of agents using simulated, realistic synthetic interactions"
  },
  {
    "input": "Tell me the title of ArXiv paper 2302.00902.",
    "output": "arXiv Query: search_query=&id_list=2302.00902&start=0&max_results=10"
  },
  {
    "input": "Describe the openai/gpt-3 repository that implements GPT-3: Language Models are Few-Shot Learners.",
    "output": "GPT-3: Language Models are Few-Shot Learners"
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2502.01061&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2502.01061&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Summarize the S4: Efficiently Modeling Long Sequences paper.",
    "output": "S4: Efficiently Modeling Long Sequences is a 2021 paper in Machine Learning. The implementation is available at state-spaces/s4 which structured state space sequence models It is written in Jupyter Notebook. The repository has 2,783 stars, indicating high community interest."
  },
  {
    "input": "How popular is the Neural Message Passing for Quantum Chemistry implementation?",
    "output": "The priba/nmp_qc repository has 343 stars and 84 forks on GitHub."
  },
  {
    "input": "What is the ArXiv category of Dream to Control: Learning Behaviors by Latent Imagination?",
    "output": "Dream to Control: Learning Behaviors by Latent Imagination is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2007.16122&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2007.16122&start=0&max_results=10 is a 2020 paper in Machine Learning. The implementation is available at linkedin/gdmix which a deep ranking personalization framework It is written in Python. The repository has 133 stars, indicating high community interest."
  },
  {
    "input": "Where can I find the code for Lookahead Optimizer: k steps forward, 1 step back?",
    "output": "The code for Lookahead Optimizer: k steps forward, 1 step back is available at https://github.com/alphadl/lookahead.pytorch (alphadl/lookahead.pytorch)."
  },
  {
    "input": "When was the Mistral 7B paper published?",
    "output": "The Mistral 7B paper was published in 2023."
  },
  {
    "input": "In what year did Feature Pyramid Networks for Object Detection come out?",
    "output": "The Feature Pyramid Networks for Object Detection paper was published in 2016."
  },
  {
    "input": "What is the ArXiv category of Semi-Supervised Classification with Graph Convolutional Networks?",
    "output": "Semi-Supervised Classification with Graph Convolutional Networks is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2301.10343&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2301.10343&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at autonomousvision/stylegan-t which [icml'23] stylegan-t: unlocking the power of gans for fast large-scale text-to-image synthesis It is written in Python. The repository has 1,196 stars, indicating high community interest."
  },
  {
    "input": "Tell me the title of ArXiv paper 2303.03846.",
    "output": "arXiv Query: search_query=&id_list=2303.03846&start=0&max_results=10"
  },
  {
    "input": "Tell me the title of ArXiv paper 1706.02216.",
    "output": "Inductive Representation Learning on Large Graphs"
  },
  {
    "input": "Explain the FoundationAgents/MetaGPT implementation of MetaGPT: Meta Programming for Multi-Agent Systems.",
    "output": "\ud83c\udf1f The Multi-Agent Framework: First AI Software Company, Towards Natural Language Programming"
  },
  {
    "input": "What programming language is ZeRO: Memory Optimizations Toward Training Trillion Parameter Models implemented in?",
    "output": "The ZeRO: Memory Optimizations Toward Training Trillion Parameter Models implementation is written in Python."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2104.00355&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2104.00355&start=0&max_results=10 is a 2021 paper in Machine Learning. The implementation is available at facebookresearch/fairseq which facebook ai research sequence-to-sequence toolkit written in python. It is written in Python. The repository has 31,988 stars, indicating high community interest."
  },
  {
    "input": "How many stars does the Proximal Policy Optimization Algorithms repository have?",
    "output": "The openai/baselines repository has 16,566 stars and 4,951 forks on GitHub."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2404.03592&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2404.03592&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Tell me the title of ArXiv paper 2302.07459.",
    "output": "arXiv Query: search_query=&id_list=2302.07459&start=0&max_results=10"
  },
  {
    "input": "What is SimCSE: Simple Contrastive Learning of Sentence Embeddings about?",
    "output": "SimCSE: Simple Contrastive Learning of Sentence Embeddings is a 2021 paper in Computation and Language (Natural Language Processing). The implementation is available at princeton-nlp/SimCSE which [emnlp 2021] simcse: simple contrastive learning of sentence embeddings https://arxiv.org/abs/2104.08821 It is written in Python. The repository has 3,615 stars, indicating high community interest."
  },
  {
    "input": "How many stars does the EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification repository have?",
    "output": "The jasonwei20/eda_nlp repository has 1,649 stars and 317 forks on GitHub."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2502.09992&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2502.09992&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2405.04324&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2405.04324&start=0&max_results=10 is a 2024 paper in Machine Learning. The implementation is available at ibm-granite/granite-code-models which granite code models: a family of open foundation models for code intelligence The repository has 1,240 stars, indicating high community interest."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2504.21801&start=0&max_results=10 implementation?",
    "output": "The MoonshotAI/Kimi-Audio repository has 4,367 stars and 316 forks on GitHub."
  },
  {
    "input": "When was the Mamba: Linear-Time Sequence Modeling with Selective State Spaces paper published?",
    "output": "The Mamba: Linear-Time Sequence Modeling with Selective State Spaces paper was published in 2023."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2502.14815&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2502.14815&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2302.10866&start=0&max_results=10 repository have?",
    "output": "The HazyResearch/safari repository has 904 stars and 70 forks on GitHub."
  },
  {
    "input": "What language is the google-research/vision_transformer repository written in?",
    "output": "The An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale implementation is written in Jupyter Notebook."
  },
  {
    "input": "What category is the Question and Answer Test-Train Overlap in Open-Domain QA Datasets paper in?",
    "output": "Question and Answer Test-Train Overlap in Open-Domain QA Datasets is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2107.07346&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2107.07346&start=0&max_results=10 paper was published in 2021."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 1606.05250?",
    "output": "SQuAD: 100,000+ Questions for Machine Comprehension of Text"
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2501.11067&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2501.11067&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at plurai-ai/intellagent which a framework for comprehensive diagnosis and optimization of agents using simulated, realistic synthetic interactions It is written in Python. The repository has 1,148 stars, indicating high community interest."
  },
  {
    "input": "What paper has ArXiv ID 2004.03270?",
    "output": "Analyzing the Structure of Attention in a Transformer Language Model"
  },
  {
    "input": "Describe the google/automl repository that implements EfficientDet: Scalable and Efficient Object Detection.",
    "output": "Google Brain AutoML"
  },
  {
    "input": "What does the mit-han-lab/offsite-tuning repository do?",
    "output": "Offsite-Tuning: Transfer Learning without Full Model"
  },
  {
    "input": "What is P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning about?",
    "output": "P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning is a 2021 paper in Computation and Language (Natural Language Processing). The implementation is available at THUDM/P-tuning-v2 which an optimized deep prompt tuning strategy comparable to fine-tuning across scales and tasks It is written in Python. The repository has 2,067 stars, indicating high community interest."
  },
  {
    "input": "Where can I find the code for Denoising Diffusion Implicit Models?",
    "output": "The code for Denoising Diffusion Implicit Models is available at https://github.com/ermongroup/ddim (ermongroup/ddim)."
  },
  {
    "input": "Describe the plurai-ai/intellagent repository that implements arXiv Query: search_query=&id_list=2501.11067&start=0&max_results=10.",
    "output": "A framework for comprehensive diagnosis and optimization of agents using simulated, realistic synthetic interactions"
  },
  {
    "input": "Summarize the HuggingGPT: Solving AI Tasks with ChatGPT paper.",
    "output": "HuggingGPT: Solving AI Tasks with ChatGPT is a 2023 paper in Artificial Intelligence. The implementation is available at microsoft/JARVIS which jarvis, a system to connect llms with ml community. paper: https://arxiv.org/pdf/2303.17580.pdf It is written in Python. The repository has 24,460 stars, indicating high community interest."
  },
  {
    "input": "What paper has ArXiv ID 1703.04691?",
    "output": "Neural Message Passing for Quantum Chemistry"
  },
  {
    "input": "Where can I find the code for ControlNet: Adding Conditional Control to Text-to-Image Diffusion?",
    "output": "The code for ControlNet: Adding Conditional Control to Text-to-Image Diffusion is available at https://github.com/lllyasviel/ControlNet (lllyasviel/ControlNet)."
  },
  {
    "input": "What paper has ArXiv ID 2303.08518?",
    "output": "arXiv Query: search_query=&id_list=2303.08518&start=0&max_results=10"
  },
  {
    "input": "Explain the allenai/codescientist implementation of arXiv Query: search_query=&id_list=2504.00698&start=0&max_results=10.",
    "output": "CodeScientist: An automated scientific discovery system for code-based experiments"
  },
  {
    "input": "What programming language is Deep Graph Library: A Graph-Centric, Highly-Performant Package implemented in?",
    "output": "The Deep Graph Library: A Graph-Centric, Highly-Performant Package implementation is written in Python."
  },
  {
    "input": "What paper has ArXiv ID 2503.01141?",
    "output": "arXiv Query: search_query=&id_list=2503.01141&start=0&max_results=10"
  },
  {
    "input": "Describe the LTH14/fractalgen repository that implements arXiv Query: search_query=&id_list=2502.17437&start=0&max_results=10.",
    "output": "PyTorch implementation of FractalGen https://arxiv.org/abs/2502.17437"
  },
  {
    "input": "What language is the google/automl repository written in?",
    "output": "The EfficientDet: Scalable and Efficient Object Detection implementation is written in Jupyter Notebook."
  },
  {
    "input": "What paper has ArXiv ID 1804.09301?",
    "output": "Generating Wikipedia by Summarizing Long Sequences"
  },
  {
    "input": "What topics does the arXiv Query: search_query=&id_list=2501.11067&start=0&max_results=10 implementation cover?",
    "output": "The plurai-ai/intellagent repository covers: agent, evaluation, llmops, simulator, synthetic-data."
  },
  {
    "input": "Give me an overview of Densely Connected Convolutional Networks.",
    "output": "Densely Connected Convolutional Networks is a 2016 paper in Computer Vision and Pattern Recognition. The implementation is available at liuzhuang13/DenseNet which densely connected convolutional networks, in cvpr 2017 (best paper award). It is written in Lua. The repository has 4,843 stars, indicating high community interest."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2502.03387&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2502.03387&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "How many stars does the S4: Efficiently Modeling Long Sequences repository have?",
    "output": "The state-spaces/s4 repository has 2,783 stars and 349 forks on GitHub."
  },
  {
    "input": "What programming language is LLaVA-1.5: Improved Baselines with Visual Instruction Tuning implemented in?",
    "output": "The LLaVA-1.5: Improved Baselines with Visual Instruction Tuning implementation is written in Python."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2503.23513&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2503.23513&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at MAXNORM8650/MedAgentSim which medagentsim: self-evolving multi-agent simulations for realistic clinical interactions, miccai 2025 (oral and early accepted) It is written in Python. The repository has 94 stars, indicating high community interest."
  },
  {
    "input": "Explain the LLaVA: Large Language and Vision Assistant paper.",
    "output": "LLaVA: Large Language and Vision Assistant is a 2023 paper in Computer Vision and Pattern Recognition. The implementation is available at haotian-liu/LLaVA which [neurips'23 oral] visual instruction tuning (llava) built towards gpt-4v level capabilities and beyond. It is written in Python. The repository has 24,034 stars, indicating high community interest."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2301.09515&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2301.09515&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Where can I find the code for Contriever: Unsupervised Dense Information Retrieval?",
    "output": "The code for Contriever: Unsupervised Dense Information Retrieval is available at https://github.com/facebookresearch/contriever (facebookresearch/contriever)."
  },
  {
    "input": "In what year did Faster R-CNN: Towards Real-Time Object Detection come out?",
    "output": "The Faster R-CNN: Towards Real-Time Object Detection paper was published in 2015."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2303.08518&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2303.08518&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at FMInference/FlexLLMGen which running large language models on a single gpu for throughput-oriented scenarios. It is written in Python. The repository has 9,379 stars, indicating high community interest."
  },
  {
    "input": "Where can I find the code for ELECTRA: Pre-training Text Encoders as Discriminators?",
    "output": "The code for ELECTRA: Pre-training Text Encoders as Discriminators is available at https://github.com/google-research/electra (google-research/electra)."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2312.02783&start=0&max_results=10 implementation?",
    "output": "The ContextualAI/HALOs repository has 893 stars and 49 forks on GitHub."
  },
  {
    "input": "When was the Adam: A Method for Stochastic Optimization paper published?",
    "output": "The Adam: A Method for Stochastic Optimization paper was published in 2014."
  },
  {
    "input": "Give me an overview of FlashAttention-2: Faster Attention with Better Parallelism.",
    "output": "FlashAttention-2: Faster Attention with Better Parallelism is a 2023 paper in Machine Learning. The implementation is available at Dao-AILab/flash-attention which fast and memory-efficient exact attention It is written in Python. The repository has 20,719 stars, indicating high community interest."
  },
  {
    "input": "What category is the Inductive Representation Learning on Large Graphs paper in?",
    "output": "Inductive Representation Learning on Large Graphs is in the cs.SI (cs.SI) category."
  },
  {
    "input": "What are the key topics in the google/trax repository?",
    "output": "The google/trax repository covers: jax, numpy, deep-learning, deep-reinforcement-learning, machine-learning, transformer, reinforcement-learning."
  },
  {
    "input": "What paper has ArXiv ID 2301.11305?",
    "output": "arXiv Query: search_query=&id_list=2301.11305&start=0&max_results=10"
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2502.09992&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2502.09992&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at Open-Reasoner-Zero/Open-Reasoner-Zero which official repo for open-reasoner-zero It is written in Python. The repository has 2,068 stars, indicating high community interest."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2301.00808&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2301.00808&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What year was Very Deep Convolutional Networks for Large-Scale Image Recognition released?",
    "output": "The Very Deep Convolutional Networks for Large-Scale Image Recognition paper was published in 2014."
  },
  {
    "input": "Give me an overview of High-Resolution Image Synthesis with Latent Diffusion Models.",
    "output": "High-Resolution Image Synthesis with Latent Diffusion Models is a 2021 paper in Computer Vision and Pattern Recognition. The implementation is available at CompVis/latent-diffusion which high-resolution image synthesis with latent diffusion models It is written in Jupyter Notebook. The repository has 13,589 stars, indicating high community interest."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2312.02783&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2312.02783&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at ContextualAI/HALOs which a library with extensible implementations of dpo, kto, ppo, orpo, and other human-aware loss functions (halos). It is written in Python. The repository has 893 stars, indicating high community interest."
  },
  {
    "input": "Explain the facebookresearch/fairseq implementation of arXiv Query: search_query=&id_list=2109.03264&start=0&max_results=10.",
    "output": "Facebook AI Research Sequence-to-Sequence Toolkit written in Python."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2301.00774&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2301.00774&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "How popular is the PaLM: Scaling Language Modeling with Pathways implementation?",
    "output": "The lucidrains/PaLM-pytorch repository has 826 stars and 82 forks on GitHub."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2504.20734&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2504.20734&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2502.03387&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2502.03387&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at GAIR-NLP/LIMO which [colm 2025] limo: less is more for reasoning It is written in Python. The repository has 1,052 stars, indicating high community interest."
  },
  {
    "input": "Describe the mit-han-lab/smoothquant repository that implements SmoothQuant: Accurate and Efficient Post-Training Quantization.",
    "output": "[ICML 2023] SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models"
  },
  {
    "input": "Describe the hpcaitech/Open-Sora repository that implements arXiv Query: search_query=&id_list=2406.07394&start=0&max_results=10.",
    "output": "Open-Sora: Democratizing Efficient Video Production for All"
  },
  {
    "input": "Summarize the MetaGPT: Meta Programming for Multi-Agent Systems paper.",
    "output": "MetaGPT: Meta Programming for Multi-Agent Systems is a 2023 paper in Artificial Intelligence. The implementation is available at FoundationAgents/MetaGPT which \ud83c\udf1f the multi-agent framework: first ai software company, towards natural language programming It is written in Python. The repository has 59,669 stars, indicating high community interest."
  },
  {
    "input": "Tell me the title of ArXiv paper 2304.15004.",
    "output": "Vicuna: An Open-Source Chatbot"
  },
  {
    "input": "Tell me the title of ArXiv paper 2005.12872.",
    "output": "EfficientDet: Scalable and Efficient Object Detection"
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2503.23513&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2503.23513&start=0&max_results=10 is available at https://github.com/MAXNORM8650/MedAgentSim (MAXNORM8650/MedAgentSim)."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2404.03592?",
    "output": "arXiv Query: search_query=&id_list=2404.03592&start=0&max_results=10"
  },
  {
    "input": "Summarize the Attention Is All You Need paper.",
    "output": "Attention Is All You Need is a 2017 paper in Computation and Language (Natural Language Processing). The implementation is available at tensorflow/tensor2tensor which library of deep learning models and datasets designed to make deep learning more accessible and accelerate ml research. It is written in Python. The repository has 16,760 stars, indicating high community interest."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2406.10209&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2406.10209&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "How many stars does the JAX: Composable transformations of Python+NumPy programs repository have?",
    "output": "The jax-ml/jax repository has 34,091 stars and 3,264 forks on GitHub."
  },
  {
    "input": "What category is the LLaVA-1.5: Improved Baselines with Visual Instruction Tuning paper in?",
    "output": "LLaVA-1.5: Improved Baselines with Visual Instruction Tuning is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "What category is the PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel paper in?",
    "output": "PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel is in the Distributed, Parallel, and Cluster Computing (cs.DC) category."
  },
  {
    "input": "What does the farizrahman4u/seq2seq repository do?",
    "output": "Sequence to Sequence Learning with Keras"
  },
  {
    "input": "Summarize the Decoupled Weight Decay Regularization paper.",
    "output": "Decoupled Weight Decay Regularization is a 2017 paper in Machine Learning. The implementation is available at pytorch/pytorch which tensors and dynamic neural networks in python with strong gpu acceleration It is written in Python. The repository has 95,348 stars, indicating high community interest."
  },
  {
    "input": "What language is the beir-cellar/beir repository written in?",
    "output": "The Beir: A Heterogeneous Benchmark for IR implementation is written in Python."
  },
  {
    "input": "Tell me the title of ArXiv paper 2504.00698.",
    "output": "arXiv Query: search_query=&id_list=2504.00698&start=0&max_results=10"
  },
  {
    "input": "What is the GitHub repository for GPT-3: Language Models are Few-Shot Learners?",
    "output": "The code for GPT-3: Language Models are Few-Shot Learners is available at https://github.com/openai/gpt-3 (openai/gpt-3)."
  },
  {
    "input": "What does the ContextualAI/HALOs repository do?",
    "output": "A library with extensible implementations of DPO, KTO, PPO, ORPO, and other human-aware loss functions (HALOs)."
  },
  {
    "input": "Explain the pytorch/pytorch implementation of PyTorch: An Imperative Style, High-Performance Deep Learning Library.",
    "output": "Tensors and Dynamic neural networks in Python with strong GPU acceleration"
  },
  {
    "input": "What category is the Very Deep Convolutional Networks for Large-Scale Image Recognition paper in?",
    "output": "Very Deep Convolutional Networks for Large-Scale Image Recognition is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2502.01061&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2502.01061&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What paper has ArXiv ID 2301.00704?",
    "output": "arXiv Query: search_query=&id_list=2301.00704&start=0&max_results=10"
  },
  {
    "input": "Explain the Enriching Word Vectors with Subword Information paper.",
    "output": "Enriching Word Vectors with Subword Information is a 2016 paper in Computation and Language (Natural Language Processing). The implementation is available at facebookresearch/fastText which library for fast text representation and classification. It is written in HTML. The repository has 26,432 stars, indicating high community interest."
  },
  {
    "input": "Summarize the Linformer: Self-Attention with Linear Complexity paper.",
    "output": "Linformer: Self-Attention with Linear Complexity is a 2021 paper in Machine Learning. The implementation is available at tatp22/linformer-pytorch which my take on a practical implementation of linformer for pytorch. It is written in Python. The repository has 421 stars, indicating high community interest."
  },
  {
    "input": "Explain the BLIP: Bootstrapping Language-Image Pre-training paper.",
    "output": "BLIP: Bootstrapping Language-Image Pre-training is a 2022 paper in Computer Vision and Pattern Recognition. The implementation is available at salesforce/BLIP which pytorch code for blip: bootstrapping language-image pre-training for unified vision-language understanding and generation   It is written in Jupyter Notebook. The repository has 5,583 stars, indicating high community interest."
  },
  {
    "input": "What is the GitHub repository for ZeRO: Memory Optimizations Toward Training Trillion Parameter Models?",
    "output": "The code for ZeRO: Memory Optimizations Toward Training Trillion Parameter Models is available at https://github.com/deepspeedai/DeepSpeed (deepspeedai/DeepSpeed)."
  },
  {
    "input": "What does the princeton-nlp/SimCSE repository do?",
    "output": "[EMNLP 2021] SimCSE: Simple Contrastive Learning of Sentence Embeddings https://arxiv.org/abs/2104.08821"
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2301.05062&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2301.05062&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What are the key topics in the google-research/google-research repository?",
    "output": "The google-research/google-research repository covers: machine-learning, ai, research."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2503.20201&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2503.20201&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "How popular is the Self-RAG: Learning to Retrieve, Generate, and Critique implementation?",
    "output": "The AkariAsai/self-rag repository has 2,252 stars and 214 forks on GitHub."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2302.08242&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2302.08242&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at haoliuhl/language-quantized-autoencoders which language quantized autoencoders It is written in Python. The repository has 111 stars, indicating high community interest."
  },
  {
    "input": "What paper has ArXiv ID 2302.00923?",
    "output": "arXiv Query: search_query=&id_list=2302.00923&start=0&max_results=10"
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2302.14838&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2302.14838&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Summarize the Reflexion: Language Agents with Verbal Reinforcement Learning paper.",
    "output": "Reflexion: Language Agents with Verbal Reinforcement Learning is a 2023 paper in Artificial Intelligence. The implementation is available at noahshinn/reflexion which [neurips 2023] reflexion: language agents with verbal reinforcement learning It is written in Python. The repository has 2,959 stars, indicating high community interest."
  },
  {
    "input": "What does the facebookresearch/CovidPrognosis repository do?",
    "output": "COVID deterioration prediction based on chest X-ray radiographs via MoCo-trained image representations"
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2301.09515&start=0&max_results=10 implementation?",
    "output": "The autonomousvision/stylegan-t repository has 1,196 stars and 61 forks on GitHub."
  },
  {
    "input": "What language is the hpcaitech/Open-Sora repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2406.07394&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2503.20201&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2503.20201&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at sentient-agi/OpenDeepSearch which sota search powered llm It is written in Python. The repository has 3,723 stars, indicating high community interest."
  },
  {
    "input": "Which field does MRKL Systems: A modular approach to AGI belong to?",
    "output": "MRKL Systems: A modular approach to AGI is in the Artificial Intelligence (cs.AI) category."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2301.13188&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2301.13188&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What language is the salesforce/GeDi repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2009.06367&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What programming language is SQuAD: 100,000+ Questions for Machine Comprehension of Text implemented in?",
    "output": "The SQuAD: 100,000+ Questions for Machine Comprehension of Text implementation is written in JavaScript."
  },
  {
    "input": "How popular is the XLNet: Generalized Autoregressive Pretraining implementation?",
    "output": "The zihangdai/xlnet repository has 6,179 stars and 1,167 forks on GitHub."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2303.01469&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2303.01469&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at ruiqi-zhong/D5 which the github repo for goal driven discovery of distributional differences via language descriptions It is written in Python. The repository has 71 stars, indicating high community interest."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2303.04129&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2303.04129&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2004.13637&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2004.13637&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "How many stars does the Self-RAG: Learning to Retrieve, Generate, and Critique repository have?",
    "output": "The AkariAsai/self-rag repository has 2,252 stars and 214 forks on GitHub."
  },
  {
    "input": "What language is the kimiyoung/transformer-xl repository written in?",
    "output": "The Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context implementation is written in Python."
  },
  {
    "input": "What paper has ArXiv ID 1409.0473?",
    "output": "Sequence to Sequence Learning with Neural Networks"
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2302.03917&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2302.03917&start=0&max_results=10 is available at https://github.com/mit-han-lab/offsite-tuning (mit-han-lab/offsite-tuning)."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2505.22954&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2505.22954&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Explain the lucidrains/muse-maskgit-pytorch implementation of arXiv Query: search_query=&id_list=2301.00704&start=0&max_results=10.",
    "output": "Implementation of Muse: Text-to-Image Generation via Masked Generative Transformers, in Pytorch"
  },
  {
    "input": "When was the Dense Passage Retrieval for Open-Domain Question Answering paper published?",
    "output": "The Dense Passage Retrieval for Open-Domain Question Answering paper was published in 2020."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2505.23006&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2505.23006&start=0&max_results=10 is available at https://github.com/ruixin31/Spurious_Rewards (ruixin31/Spurious_Rewards)."
  },
  {
    "input": "What does the simplescaling/s1 repository do?",
    "output": "s1: Simple test-time scaling"
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2405.04324&start=0&max_results=10 repository have?",
    "output": "The ibm-granite/granite-code-models repository has 1,240 stars and 88 forks on GitHub."
  },
  {
    "input": "Tell me the title of ArXiv paper 2501.13545.",
    "output": "arXiv Query: search_query=&id_list=2501.13545&start=0&max_results=10"
  },
  {
    "input": "What does the rajpurkar/SQuAD-explorer repository do?",
    "output": "Visually Explore the Stanford Question Answering Dataset"
  },
  {
    "input": "What is JAX: Composable transformations of Python+NumPy programs about?",
    "output": "JAX: Composable transformations of Python+NumPy programs is a 2019 paper in cs.MS. The implementation is available at jax-ml/jax which composable transformations of python+numpy programs: differentiate, vectorize, jit to gpu/tpu, and more It is written in Python. The repository has 34,091 stars, indicating high community interest."
  },
  {
    "input": "Explain the facebookresearch/mae implementation of Masked Autoencoders Are Scalable Vision Learners.",
    "output": "PyTorch implementation of MAE https//arxiv.org/abs/2111.06377"
  },
  {
    "input": "What language is the MAXNORM8650/MedAgentSim repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2503.22678&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "In what year did BEiT: BERT Pre-Training of Image Transformers come out?",
    "output": "The BEiT: BERT Pre-Training of Image Transformers paper was published in 2021."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2504.19413&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2504.19413&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Summarize the Very Deep Convolutional Networks for Large-Scale Image Recognition paper.",
    "output": "Very Deep Convolutional Networks for Large-Scale Image Recognition is a 2014 paper in Computer Vision and Pattern Recognition. The implementation is available at machrisaa/tensorflow-vgg which vgg19 and vgg16 on tensorflow It is written in Python. The repository has 2,239 stars, indicating high community interest."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2301.04104&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2301.04104&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at google-deepmind/tracr It is written in Python. The repository has 548 stars, indicating high community interest."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2301.01947&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2301.01947&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2006.16779&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2006.16779&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "When was the RetNet: Retentive Network: A Successor to Transformer paper published?",
    "output": "The RetNet: Retentive Network: A Successor to Transformer paper was published in 2022."
  },
  {
    "input": "How popular is the LoRA: Low-Rank Adaptation of Large Language Models implementation?",
    "output": "The microsoft/LoRA repository has 12,979 stars and 863 forks on GitHub."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2109.03264&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2109.03264&start=0&max_results=10 is available at https://github.com/facebookresearch/fairseq (facebookresearch/fairseq)."
  },
  {
    "input": "What is the GitHub repository for DreamBooth: Fine Tuning Text-to-Image Diffusion Models?",
    "output": "The code for DreamBooth: Fine Tuning Text-to-Image Diffusion Models is available at https://github.com/google/dreambooth (google/dreambooth)."
  },
  {
    "input": "Which field does The Flan Collection: Designing Data and Methods for Effective Instruction Tuning belong to?",
    "output": "The Flan Collection: Designing Data and Methods for Effective Instruction Tuning is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What category is the Mamba: Linear-Time Sequence Modeling with Selective State Spaces paper in?",
    "output": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Which field does AlpacaEval: An Automatic Evaluator for Instruction-following belong to?",
    "output": "AlpacaEval: An Automatic Evaluator for Instruction-following is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What language is the OpenBMB/Eurus repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2404.03592&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What are the key topics in the haotian-liu/LLaVA repository?",
    "output": "The haotian-liu/LLaVA repository covers: gpt-4, chatbot, chatgpt, llama, multimodal, llava, foundation-models, instruction-tuning, multi-modality, visual-language-learning, llama-2, llama2, vision-language-model."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2301.11305&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2301.11305&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at autonomousvision/stylegan-t which [icml'23] stylegan-t: unlocking the power of gans for fast large-scale text-to-image synthesis It is written in Python. The repository has 1,196 stars, indicating high community interest."
  },
  {
    "input": "Explain the openai/whisper implementation of Whisper: Robust Speech Recognition via Large-Scale Weak Supervision.",
    "output": "Robust Speech Recognition via Large-Scale Weak Supervision"
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2302.14233&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2302.14233&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at ruiqi-zhong/D5 which the github repo for goal driven discovery of distributional differences via language descriptions It is written in Python. The repository has 71 stars, indicating high community interest."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2302.07459&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2302.07459&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What category is the Semi-Supervised Classification with Graph Convolutional Networks paper in?",
    "output": "Semi-Supervised Classification with Graph Convolutional Networks is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What are the key topics in the THUDM/P-tuning-v2 repository?",
    "output": "The THUDM/P-tuning-v2 repository covers: natural-language-processing, prompt-tuning, pretrained-language-model, p-tuning, parameter-efficient-learning."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2504.01848&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2504.01848&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "In what year did Question and Answer Test-Train Overlap in Open-Domain QA Datasets come out?",
    "output": "The Question and Answer Test-Train Overlap in Open-Domain QA Datasets paper was published in 2020."
  },
  {
    "input": "In what year did Very Deep Convolutional Networks for Large-Scale Image Recognition come out?",
    "output": "The Very Deep Convolutional Networks for Large-Scale Image Recognition paper was published in 2014."
  },
  {
    "input": "What's the implementation repository for SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding?",
    "output": "The code for SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding is available at https://github.com/nyu-mll/jiant (nyu-mll/jiant)."
  },
  {
    "input": "When was the Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks paper published?",
    "output": "The Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks paper was published in 2019."
  },
  {
    "input": "What is the ArXiv category of Densely Connected Convolutional Networks?",
    "output": "Densely Connected Convolutional Networks is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2503.20201&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2503.20201&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2302.12766&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2302.12766&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Explain the haoliuhl/language-quantized-autoencoders implementation of arXiv Query: search_query=&id_list=2302.07459&start=0&max_results=10.",
    "output": "Language Quantized AutoEncoders"
  },
  {
    "input": "Summarize the Faster R-CNN: Towards Real-Time Object Detection paper.",
    "output": "Faster R-CNN: Towards Real-Time Object Detection is a 2015 paper in Computer Vision and Pattern Recognition. The implementation is available at rbgirshick/py-faster-rcnn which faster r-cnn (python implementation) -- see https://github.com/shaoqingren/faster_rcnn for the official matlab version It is written in Python. The repository has 8,269 stars, indicating high community interest."
  },
  {
    "input": "What topics does the Attention Is All You Need implementation cover?",
    "output": "The tensorflow/tensor2tensor repository covers: machine-learning, machine-translation, deep-learning, reinforcement-learning, tpu."
  },
  {
    "input": "What language is the Dao-AILab/flash-attention repository written in?",
    "output": "The FlashAttention: Fast and Memory-Efficient Exact Attention implementation is written in Python."
  },
  {
    "input": "In what year did LLaMA-Adapter: Efficient Fine-tuning of Language Models come out?",
    "output": "The LLaMA-Adapter: Efficient Fine-tuning of Language Models paper was published in 2023."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2303.08518&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2303.08518&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What programming language is Direct Preference Optimization: Your Language Model is a Reward Model implemented in?",
    "output": "The Direct Preference Optimization: Your Language Model is a Reward Model implementation is written in Python."
  },
  {
    "input": "What is Inductive Representation Learning on Large Graphs about?",
    "output": "Inductive Representation Learning on Large Graphs is a 2017 paper in cs.SI. The implementation is available at williamleif/GraphSAGE which representation learning on large graphs using stochastic graph convolutions. It is written in Python. The repository has 3,630 stars, indicating high community interest."
  },
  {
    "input": "What paper has ArXiv ID 2307.08691?",
    "output": "FlashAttention-2: Faster Attention with Better Parallelism"
  },
  {
    "input": "What is Dense Passage Retrieval for Open-Domain Question Answering about?",
    "output": "Dense Passage Retrieval for Open-Domain Question Answering is a 2020 paper in Computation and Language (Natural Language Processing). The implementation is available at facebookresearch/DPR which dense passage retriever - is a set of tools and models for open domain q&a task. It is written in Python. The repository has 1,847 stars, indicating high community interest."
  },
  {
    "input": "How many stars does the Zephyr: Direct Distillation of LM Alignment repository have?",
    "output": "The huggingface/alignment-handbook repository has 5,429 stars and 464 forks on GitHub."
  },
  {
    "input": "What does the facebookresearch/fairseq repository do?",
    "output": "Facebook AI Research Sequence-to-Sequence Toolkit written in Python."
  },
  {
    "input": "What category is the Competition-Level Code Generation with AlphaCode paper in?",
    "output": "Competition-Level Code Generation with AlphaCode is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What does the openai/CLIP repository do?",
    "output": "CLIP (Contrastive Language-Image Pretraining),  Predict the most relevant text snippet given an image"
  },
  {
    "input": "When was the QLoRA: Efficient Finetuning of Quantized LLMs paper published?",
    "output": "The QLoRA: Efficient Finetuning of Quantized LLMs paper was published in 2023."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2303.04671&start=0&max_results=10 repository have?",
    "output": "The chenfei-wu/TaskMatrix repository has 34,352 stars and 3,268 forks on GitHub."
  },
  {
    "input": "In what year did Segment Anything come out?",
    "output": "The Segment Anything paper was published in 2023."
  },
  {
    "input": "What year was ChemCrow: Augmenting large-language models with chemistry tools released?",
    "output": "The ChemCrow: Augmenting large-language models with chemistry tools paper was published in 2023."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2303.01469&start=0&max_results=10 repository have?",
    "output": "The ruiqi-zhong/D5 repository has 71 stars and 13 forks on GitHub."
  },
  {
    "input": "What is the ArXiv category of An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale?",
    "output": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2504.20734&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2504.20734&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at MoonshotAI/Kimi-Audio which kimi-audio, an open-source audio foundation model excelling in audio understanding, generation, and conversation It is written in Python. The repository has 4,367 stars, indicating high community interest."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2504.11844&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2504.11844&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at Crista23/goal_directedness_llms It is written in Python. The repository has 12 stars, indicating high community interest."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2302.00923&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2302.00923&start=0&max_results=10 is available at https://github.com/amazon-science/mm-cot (amazon-science/mm-cot)."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2406.09308&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2406.09308&start=0&max_results=10 paper was published in 2024."
  },
  {
    "input": "What's the implementation repository for AlpacaEval: An Automatic Evaluator for Instruction-following?",
    "output": "The code for AlpacaEval: An Automatic Evaluator for Instruction-following is available at https://github.com/tatsu-lab/alpaca_eval (tatsu-lab/alpaca_eval)."
  },
  {
    "input": "Explain the mit-han-lab/offsite-tuning implementation of arXiv Query: search_query=&id_list=2302.04023&start=0&max_results=10.",
    "output": "Offsite-Tuning: Transfer Learning without Full Model"
  },
  {
    "input": "What paper has ArXiv ID 2303.11331?",
    "output": "Grounding DINO: Marrying DINO with Grounded Pre-Training"
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2303.06865&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2303.06865&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Tell me the title of ArXiv paper 2305.06500.",
    "output": "InstructBLIP: Towards General-purpose Vision-Language Models"
  },
  {
    "input": "What category is the Neural Machine Translation by Jointly Learning to Align and Translate paper in?",
    "output": "Neural Machine Translation by Jointly Learning to Align and Translate is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What programming language is ColBERT: Efficient Passage Search via Contextualized Late Interaction implemented in?",
    "output": "The ColBERT: Efficient Passage Search via Contextualized Late Interaction implementation is written in Python."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2301.04104&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2301.04104&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What topics does the arXiv Query: search_query=&id_list=2303.03378&start=0&max_results=10 implementation cover?",
    "output": "The NVlabs/prismer repository covers: image-captioning, language-model, multi-modal-learning, multi-task-learning, vision-language-model, vision-and-language, vqa."
  },
  {
    "input": "Which field does Mamba: Linear-Time Sequence Modeling with Selective State Spaces belong to?",
    "output": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "In what year did S4: Efficiently Modeling Long Sequences come out?",
    "output": "The S4: Efficiently Modeling Long Sequences paper was published in 2021."
  },
  {
    "input": "What's the implementation repository for Training language models to follow instructions with human feedback?",
    "output": "The code for Training language models to follow instructions with human feedback is available at https://github.com/openai/following-instructions-human-feedback (openai/following-instructions-human-feedback)."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2303.04129&start=0&max_results=10 implementation?",
    "output": "The HazyResearch/safari repository has 904 stars and 70 forks on GitHub."
  },
  {
    "input": "What language is the QingruZhang/AdaLoRA repository written in?",
    "output": "The AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning implementation is written in Python."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2501.19393?",
    "output": "arXiv Query: search_query=&id_list=2501.19393&start=0&max_results=10"
  },
  {
    "input": "What language is the pytorch/pytorch repository written in?",
    "output": "The PyTorch: An Imperative Style, High-Performance Deep Learning Library implementation is written in Python."
  },
  {
    "input": "How many stars does the Prefix-Tuning: Optimizing Continuous Prompts for Generation repository have?",
    "output": "The XiangLi1999/PrefixTuning repository has 954 stars and 164 forks on GitHub."
  },
  {
    "input": "Describe the haotian-liu/LLaVA repository that implements LLaVA: Large Language and Vision Assistant.",
    "output": "[NeurIPS'23 Oral] Visual Instruction Tuning (LLaVA) built towards GPT-4V level capabilities and beyond."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2502.19328?",
    "output": "arXiv Query: search_query=&id_list=2502.19328&start=0&max_results=10"
  },
  {
    "input": "Explain the farizrahman4u/seq2seq implementation of Sequence to Sequence Learning with Neural Networks.",
    "output": "Sequence to Sequence Learning with Keras"
  },
  {
    "input": "What topics does the PaLM: Scaling Language Modeling with Pathways implementation cover?",
    "output": "The lucidrains/PaLM-pytorch repository covers: deep-learning, transformers, attention-mechanism, artificial-general-intelligence."
  },
  {
    "input": "Which field does Gorilla: Large Language Model Connected with Massive APIs belong to?",
    "output": "Gorilla: Large Language Model Connected with Massive APIs is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Which field does Analyzing the Structure of Attention in a Transformer Language Model belong to?",
    "output": "Analyzing the Structure of Attention in a Transformer Language Model is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What does the MoonshotAI/Kimi-Audio repository do?",
    "output": "Kimi-Audio, an open-source audio foundation model excelling in audio understanding, generation, and conversation"
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2502.19328&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2502.19328&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What paper has ArXiv ID 2503.20201?",
    "output": "arXiv Query: search_query=&id_list=2503.20201&start=0&max_results=10"
  },
  {
    "input": "Explain the google-research-datasets/natural-questions implementation of Question and Answer Test-Train Overlap in Open-Domain QA Datasets.",
    "output": "Natural Questions (NQ) contains real user questions issued to Google search, and answers found from Wikipedia by annotators. NQ is designed for the training and evaluation of automatic question answering systems."
  },
  {
    "input": "What language is the HazyResearch/safari repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2303.03846&start=0&max_results=10 implementation is written in Assembly."
  },
  {
    "input": "Tell me the title of ArXiv paper 1909.11942.",
    "output": "ELECTRA: Pre-training Text Encoders as Discriminators"
  },
  {
    "input": "What language is the salesforce/LAVIS repository written in?",
    "output": "The BLIP-2: Bootstrapping Language-Image Pre-training implementation is written in Jupyter Notebook."
  },
  {
    "input": "Describe the Open-Reasoner-Zero/Open-Reasoner-Zero repository that implements arXiv Query: search_query=&id_list=2502.14815&start=0&max_results=10.",
    "output": "Official Repo for Open-Reasoner-Zero"
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2301.12652&start=0&max_results=10 implementation?",
    "output": "The amazon-science/mm-cot repository has 3,982 stars and 333 forks on GitHub."
  },
  {
    "input": "What's the implementation repository for Densely Connected Convolutional Networks?",
    "output": "The code for Densely Connected Convolutional Networks is available at https://github.com/liuzhuang13/DenseNet (liuzhuang13/DenseNet)."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2303.08518&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2303.08518&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What year was Vicuna: An Open-Source Chatbot released?",
    "output": "The Vicuna: An Open-Source Chatbot paper was published in 2023."
  },
  {
    "input": "Explain the mistralai/mistral-inference implementation of Mistral 7B.",
    "output": "Official inference library for Mistral models"
  },
  {
    "input": "Summarize the Generating Wikipedia by Summarizing Long Sequences paper.",
    "output": "Generating Wikipedia by Summarizing Long Sequences is a 2018 paper in Computation and Language (Natural Language Processing). The implementation is available at tensorflow/tensor2tensor which library of deep learning models and datasets designed to make deep learning more accessible and accelerate ml research. It is written in Python. The repository has 16,760 stars, indicating high community interest."
  },
  {
    "input": "How popular is the BEiT: BERT Pre-Training of Image Transformers implementation?",
    "output": "The microsoft/unilm repository has 21,848 stars and 2,674 forks on GitHub."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2301.13688&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2301.13688&start=0&max_results=10 is available at https://github.com/amazon-science/mm-cot (amazon-science/mm-cot)."
  },
  {
    "input": "What does the google-research/electra repository do?",
    "output": "ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators"
  },
  {
    "input": "When was the Masked Autoencoders Are Scalable Vision Learners paper published?",
    "output": "The Masked Autoencoders Are Scalable Vision Learners paper was published in 2022."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2301.00704&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2301.00704&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "Summarize the SmoothQuant: Accurate and Efficient Post-Training Quantization paper.",
    "output": "SmoothQuant: Accurate and Efficient Post-Training Quantization is a 2022 paper in Machine Learning. The implementation is available at mit-han-lab/smoothquant which [icml 2023] smoothquant: accurate and efficient post-training quantization for large language models It is written in Python. The repository has 1,561 stars, indicating high community interest."
  },
  {
    "input": "In what year did Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena come out?",
    "output": "The Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena paper was published in 2023."
  },
  {
    "input": "How popular is the YOLOX: Exceeding YOLO Series in 2021 implementation?",
    "output": "The Megvii-BaseDetection/YOLOX repository has 10,185 stars and 2,413 forks on GitHub."
  },
  {
    "input": "What is the GitHub repository for Wav2Vec 2.0: A Framework for Self-Supervised Learning of Speech?",
    "output": "The code for Wav2Vec 2.0: A Framework for Self-Supervised Learning of Speech is available at https://github.com/facebookresearch/fairseq (facebookresearch/fairseq)."
  },
  {
    "input": "When was the BLIP-2: Bootstrapping Language-Image Pre-training paper published?",
    "output": "The BLIP-2: Bootstrapping Language-Image Pre-training paper was published in 2023."
  },
  {
    "input": "What is the ArXiv category of Question and Answer Test-Train Overlap in Open-Domain QA Datasets?",
    "output": "Question and Answer Test-Train Overlap in Open-Domain QA Datasets is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2302.09778&start=0&max_results=10 repository have?",
    "output": "The ali-vilab/composer repository has 1,560 stars and 49 forks on GitHub."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2406.09308&start=0&max_results=10 repository have?",
    "output": "The lamini-ai/Lamini-Memory-Tuning repository has 275 stars and 12 forks on GitHub."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2302.14233&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2302.14233&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Tell me the title of ArXiv paper 1802.05365.",
    "output": "Universal Language Model Fine-tuning for Text Classification"
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2502.09992&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2502.09992&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is WizardCoder: Empowering Code LLMs with Evol-Instruct about?",
    "output": "WizardCoder: Empowering Code LLMs with Evol-Instruct is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at nlpxucan/WizardLM which llms build upon evol insturct: wizardlm, wizardcoder, wizardmath It is written in Python. The repository has 9,460 stars, indicating high community interest."
  },
  {
    "input": "Describe the uber/orbit repository that implements arXiv Query: search_query=&id_list=2004.08492&start=0&max_results=10.",
    "output": "A Python package for Bayesian forecasting with object-oriented design and probabilistic models under the hood."
  },
  {
    "input": "Where can I find the code for Gorilla: Large Language Model Connected with Massive APIs?",
    "output": "The code for Gorilla: Large Language Model Connected with Massive APIs is available at https://github.com/ShishirPatil/gorilla (ShishirPatil/gorilla)."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2405.03548&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2405.03548&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Explain the InstructBLIP: Towards General-purpose Vision-Language Models paper.",
    "output": "InstructBLIP: Towards General-purpose Vision-Language Models is a 2023 paper in Computer Vision and Pattern Recognition. The implementation is available at salesforce/LAVIS which lavis - a one-stop library for language-vision intelligence It is written in Jupyter Notebook. The repository has 11,040 stars, indicating high community interest."
  },
  {
    "input": "What topics does the arXiv Query: search_query=&id_list=2104.00355&start=0&max_results=10 implementation cover?",
    "output": "The facebookresearch/fairseq repository covers: python, pytorch, artificial-intelligence."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2106.05237?",
    "output": "Wav2Vec 2.0: A Framework for Self-Supervised Learning of Speech"
  },
  {
    "input": "What language is the salesforce/LAVIS repository written in?",
    "output": "The InstructBLIP: Towards General-purpose Vision-Language Models implementation is written in Jupyter Notebook."
  },
  {
    "input": "What is the ArXiv category of SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding?",
    "output": "SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Tell me the title of ArXiv paper 2102.01192.",
    "output": "arXiv Query: search_query=&id_list=2102.01192&start=0&max_results=10"
  },
  {
    "input": "What's the implementation repository for Segment Anything?",
    "output": "The code for Segment Anything is available at https://github.com/facebookresearch/segment-anything (facebookresearch/segment-anything)."
  },
  {
    "input": "What category is the Know What You Don't Know: Unanswerable Questions for SQuAD paper in?",
    "output": "Know What You Don't Know: Unanswerable Questions for SQuAD is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What does the openai/frontier-evals repository do?",
    "output": "OpenAI Frontier Evals"
  },
  {
    "input": "What category is the LLM.int8(): 8-bit Matrix Multiplication for Transformers paper in?",
    "output": "LLM.int8(): 8-bit Matrix Multiplication for Transformers is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What programming language is Enriching Word Vectors with Subword Information implemented in?",
    "output": "The Enriching Word Vectors with Subword Information implementation is written in HTML."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2004.08492&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2004.08492&start=0&max_results=10 paper was published in 2020."
  },
  {
    "input": "Tell me the title of ArXiv paper 2307.00109.",
    "output": "Med-PaLM 2: Towards Expert-Level Medical Question Answering"
  },
  {
    "input": "What is the GitHub repository for EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification?",
    "output": "The code for EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification is available at https://github.com/jasonwei20/eda_nlp (jasonwei20/eda_nlp)."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2405.02803&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2405.02803&start=0&max_results=10 is a 2024 paper in Machine Learning. The implementation is available at ibm-granite/granite-code-models which granite code models: a family of open foundation models for code intelligence The repository has 1,240 stars, indicating high community interest."
  },
  {
    "input": "What language is the FMInference/FlexLLMGen repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2303.08518&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Explain the autonomousvision/stylegan-t implementation of arXiv Query: search_query=&id_list=2301.09515&start=0&max_results=10.",
    "output": "[ICML'23] StyleGAN-T: Unlocking the Power of GANs for Fast Large-Scale Text-to-Image Synthesis"
  },
  {
    "input": "When was the TinyBERT: Distilling BERT for Natural Language Understanding paper published?",
    "output": "The TinyBERT: Distilling BERT for Natural Language Understanding paper was published in 2019."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2303.02913&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2303.02913&start=0&max_results=10 is available at https://github.com/Shark-NLP/OpenICL (Shark-NLP/OpenICL)."
  },
  {
    "input": "Give me an overview of Segment Anything.",
    "output": "Segment Anything is a 2023 paper in Computer Vision and Pattern Recognition. The implementation is available at facebookresearch/segment-anything which the repository provides code for running inference with the segmentanything model (sam), links for downloading the trained model checkpoints, and example notebooks that show how to use the model. It is written in Jupyter Notebook. The repository has 52,657 stars, indicating high community interest."
  },
  {
    "input": "Describe the dmlc/dgl repository that implements Deep Graph Library: A Graph-Centric, Highly-Performant Package.",
    "output": "Python package built to ease deep learning on graph, on top of existing DL frameworks."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2301.13688&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2301.13688&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "In what year did GPT-4 Technical Report come out?",
    "output": "The GPT-4 Technical Report paper was published in 2023."
  },
  {
    "input": "What is the ArXiv category of YOLOX: Exceeding YOLO Series in 2021?",
    "output": "YOLOX: Exceeding YOLO Series in 2021 is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "What is the GitHub repository for Contriever: Unsupervised Dense Information Retrieval?",
    "output": "The code for Contriever: Unsupervised Dense Information Retrieval is available at https://github.com/facebookresearch/contriever (facebookresearch/contriever)."
  },
  {
    "input": "Explain the google-research/text-to-text-transfer-transformer implementation of T5: Exploring the Limits of Transfer Learning.",
    "output": "Code for the paper \"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\""
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2504.16084&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2504.16084&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at Crista23/goal_directedness_llms It is written in Python. The repository has 12 stars, indicating high community interest."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2502.14815&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2502.14815&start=0&max_results=10 is available at https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero (Open-Reasoner-Zero/Open-Reasoner-Zero)."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2503.00735&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2503.00735&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at LTH14/fractalgen which pytorch implementation of fractalgen https://arxiv.org/abs/2502.17437 It is written in Python. The repository has 1,196 stars, indicating high community interest."
  },
  {
    "input": "Where can I find the code for Inductive Representation Learning on Large Graphs?",
    "output": "The code for Inductive Representation Learning on Large Graphs is available at https://github.com/williamleif/GraphSAGE (williamleif/GraphSAGE)."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2303.09431&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2303.09431&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at FMInference/FlexLLMGen which running large language models on a single gpu for throughput-oriented scenarios. It is written in Python. The repository has 9,379 stars, indicating high community interest."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2301.00774&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2301.00774&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2301.00704&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2301.00704&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at lucidrains/muse-maskgit-pytorch which implementation of muse: text-to-image generation via masked generative transformers, in pytorch It is written in Python. The repository has 916 stars, indicating high community interest."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2005.00687?",
    "output": "Deep Graph Library: A Graph-Centric, Highly-Performant Package"
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2006.16779?",
    "output": "arXiv Query: search_query=&id_list=2006.16779&start=0&max_results=10"
  },
  {
    "input": "What is the GitHub repository for HuggingGPT: Solving AI Tasks with ChatGPT?",
    "output": "The code for HuggingGPT: Solving AI Tasks with ChatGPT is available at https://github.com/microsoft/JARVIS (microsoft/JARVIS)."
  },
  {
    "input": "What paper has ArXiv ID 2406.07394?",
    "output": "arXiv Query: search_query=&id_list=2406.07394&start=0&max_results=10"
  },
  {
    "input": "Which field does ALBERT: A Lite BERT belong to?",
    "output": "ALBERT: A Lite BERT is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2303.09431&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2303.09431&start=0&max_results=10 is available at https://github.com/FMInference/FlexLLMGen (FMInference/FlexLLMGen)."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 1802.05365?",
    "output": "Universal Language Model Fine-tuning for Text Classification"
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2302.09778&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2302.09778&start=0&max_results=10 is available at https://github.com/ali-vilab/composer (ali-vilab/composer)."
  },
  {
    "input": "Tell me the title of ArXiv paper 1905.05583.",
    "output": "Billion-scale similarity search with GPUs"
  },
  {
    "input": "In what year did Analyzing the Structure of Attention in a Transformer Language Model come out?",
    "output": "The Analyzing the Structure of Attention in a Transformer Language Model paper was published in 2020."
  },
  {
    "input": "Explain the namisan/mt-dnn implementation of Multi-Task Deep Neural Networks for Natural Language Understanding.",
    "output": "Multi-Task Deep Neural Networks for Natural Language Understanding"
  },
  {
    "input": "In what year did GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers come out?",
    "output": "The GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers paper was published in 2022."
  },
  {
    "input": "How popular is the StarCoder: May the source be with you! implementation?",
    "output": "The bigcode-project/starcoder repository has 7,473 stars and 530 forks on GitHub."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2502.12115&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2502.12115&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at Open-Reasoner-Zero/Open-Reasoner-Zero which official repo for open-reasoner-zero It is written in Python. The repository has 2,068 stars, indicating high community interest."
  },
  {
    "input": "Give me an overview of Reflexion: Language Agents with Verbal Reinforcement Learning.",
    "output": "Reflexion: Language Agents with Verbal Reinforcement Learning is a 2023 paper in Artificial Intelligence. The implementation is available at noahshinn/reflexion which [neurips 2023] reflexion: language agents with verbal reinforcement learning It is written in Python. The repository has 2,959 stars, indicating high community interest."
  },
  {
    "input": "What language is the pytorch/pytorch repository written in?",
    "output": "The PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel implementation is written in Python."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2301.11305&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2301.11305&start=0&max_results=10 is available at https://github.com/autonomousvision/stylegan-t (autonomousvision/stylegan-t)."
  },
  {
    "input": "What topics does the Reformer: The Efficient Transformer implementation cover?",
    "output": "The google/trax repository covers: jax, numpy, deep-learning, deep-reinforcement-learning, machine-learning, transformer, reinforcement-learning."
  },
  {
    "input": "What does the microsoft/unilm repository do?",
    "output": "Large-scale Self-supervised Pre-training Across Tasks, Languages, and Modalities"
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2301.01751&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2301.01751&start=0&max_results=10 is available at https://github.com/oughtinc/ice (oughtinc/ice)."
  },
  {
    "input": "What does the haoliuhl/language-quantized-autoencoders repository do?",
    "output": "Language Quantized AutoEncoders"
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2303.04226&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2303.04226&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at HazyResearch/safari which convolutions for sequence modeling It is written in Assembly. The repository has 904 stars, indicating high community interest."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2301.11316&start=0&max_results=10 implementation?",
    "output": "The autonomousvision/stylegan-t repository has 1,196 stars and 61 forks on GitHub."
  },
  {
    "input": "What language is the dmlc/dgl repository written in?",
    "output": "The Deep Graph Library: A Graph-Centric, Highly-Performant Package implementation is written in Python."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2504.11844&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2504.11844&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at Crista23/goal_directedness_llms It is written in Python. The repository has 12 stars, indicating high community interest."
  },
  {
    "input": "What topics does the Universal Language Model Fine-tuning for Text Classification implementation cover?",
    "output": "The fastai/fastai repository covers: deep-learning, machine-learning, pytorch, python, gpu, fastai, notebooks, colab."
  },
  {
    "input": "Which field does Beir: A Heterogeneous Benchmark for IR belong to?",
    "output": "Beir: A Heterogeneous Benchmark for IR is in the Information Retrieval (cs.IR) category."
  },
  {
    "input": "Summarize the PyTorch Geometric: Library for Geometric Deep Learning paper.",
    "output": "PyTorch Geometric: Library for Geometric Deep Learning is a 2019 paper in Machine Learning. The implementation is available at pyg-team/pytorch_geometric which graph neural network library for pytorch It is written in Python. The repository has 23,186 stars, indicating high community interest."
  },
  {
    "input": "How popular is the ZeRO: Memory Optimizations Toward Training Trillion Parameter Models implementation?",
    "output": "The deepspeedai/DeepSpeed repository has 40,796 stars and 4,646 forks on GitHub."
  },
  {
    "input": "What is SQuAD: 100,000+ Questions for Machine Comprehension of Text about?",
    "output": "SQuAD: 100,000+ Questions for Machine Comprehension of Text is a 2016 paper in Computation and Language (Natural Language Processing). The implementation is available at rajpurkar/SQuAD-explorer which visually explore the stanford question answering dataset It is written in JavaScript. The repository has 570 stars, indicating high community interest."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2302.12766&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2302.12766&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What language is the ruixin31/Spurious_Rewards repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2505.22101&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2504.02732&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2504.02732&start=0&max_results=10 is available at https://github.com/MAXNORM8650/MedAgentSim (MAXNORM8650/MedAgentSim)."
  },
  {
    "input": "What paper has ArXiv ID 2502.09992?",
    "output": "arXiv Query: search_query=&id_list=2502.09992&start=0&max_results=10"
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2405.04324&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2405.04324&start=0&max_results=10 paper was published in 2024."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2302.04023&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2302.04023&start=0&max_results=10 is available at https://github.com/mit-han-lab/offsite-tuning (mit-han-lab/offsite-tuning)."
  },
  {
    "input": "How many stars does the SQuAD: 100,000+ Questions for Machine Comprehension of Text repository have?",
    "output": "The rajpurkar/SQuAD-explorer repository has 570 stars and 121 forks on GitHub."
  },
  {
    "input": "What language is the hpcaitech/Open-Sora repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2406.12824&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What's the implementation repository for MRKL Systems: A modular approach to AGI?",
    "output": "The code for MRKL Systems: A modular approach to AGI is available at https://github.com/langchain-ai/langchain (langchain-ai/langchain)."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2505.22101&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2505.22101&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Describe the state-spaces/s4 repository that implements S4: Efficiently Modeling Long Sequences.",
    "output": "Structured state space sequence models"
  },
  {
    "input": "What language is the allenai/codescientist repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2503.22708&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What language is the mit-han-lab/smoothquant repository written in?",
    "output": "The SmoothQuant: Accurate and Efficient Post-Training Quantization implementation is written in Python."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2505.22101?",
    "output": "arXiv Query: search_query=&id_list=2505.22101&start=0&max_results=10"
  },
  {
    "input": "What programming language is BLIP: Bootstrapping Language-Image Pre-training implemented in?",
    "output": "The BLIP: Bootstrapping Language-Image Pre-training implementation is written in Jupyter Notebook."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2502.14815&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2502.14815&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at Open-Reasoner-Zero/Open-Reasoner-Zero which official repo for open-reasoner-zero It is written in Python. The repository has 2,068 stars, indicating high community interest."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2301.00303&start=0&max_results=10 repository have?",
    "output": "The facebookresearch/ConvNeXt-V2 repository has 1,890 stars and 155 forks on GitHub."
  },
  {
    "input": "In what year did ELECTRA: Pre-training Text Encoders as Discriminators come out?",
    "output": "The ELECTRA: Pre-training Text Encoders as Discriminators paper was published in 2019."
  },
  {
    "input": "Give me an overview of Analyzing the Structure of Attention in a Transformer Language Model.",
    "output": "Analyzing the Structure of Attention in a Transformer Language Model is a 2020 paper in Computation and Language (Natural Language Processing). The implementation is available at clarkkev/attention-analysis It is written in Jupyter Notebook. The repository has 469 stars, indicating high community interest."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2301.00808&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2301.00808&start=0&max_results=10 is available at https://github.com/facebookresearch/ConvNeXt-V2 (facebookresearch/ConvNeXt-V2)."
  },
  {
    "input": "When was the P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning paper published?",
    "output": "The P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning paper was published in 2021."
  },
  {
    "input": "What's the implementation repository for TinyBERT: Distilling BERT for Natural Language Understanding?",
    "output": "The code for TinyBERT: Distilling BERT for Natural Language Understanding is available at https://github.com/huawei-noah/Pretrained-Language-Model (huawei-noah/Pretrained-Language-Model)."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2104.00355?",
    "output": "arXiv Query: search_query=&id_list=2104.00355&start=0&max_results=10"
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2405.03520&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2405.03520&start=0&max_results=10 paper was published in 2024."
  },
  {
    "input": "Describe the google-research/google-research repository that implements Synthetic Data Generation with Large Language Models.",
    "output": "Google Research"
  },
  {
    "input": "In what year did LLaMA: Open and Efficient Foundation Language Models come out?",
    "output": "The LLaMA: Open and Efficient Foundation Language Models paper was published in 2023."
  },
  {
    "input": "What are the key topics in the NVlabs/prismer repository?",
    "output": "The NVlabs/prismer repository covers: image-captioning, language-model, multi-modal-learning, multi-task-learning, vision-language-model, vision-and-language, vqa."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2004.13637&start=0&max_results=10 implementation?",
    "output": "The facebookresearch/TransCoder repository has 1,722 stars and 261 forks on GitHub."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2302.12246&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2302.12246&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at shizhediao/active-prompt which source code for the paper \"active prompting with chain-of-thought for large language models\" It is written in Python. The repository has 247 stars, indicating high community interest."
  },
  {
    "input": "What are the key topics in the microsoft/Swin-Transformer repository?",
    "output": "The microsoft/Swin-Transformer repository covers: swin-transformer, image-classification, object-detection, semantic-segmentation, imagenet, mscoco, ade20k, mask-rcnn."
  },
  {
    "input": "Where can I find the code for BLIP-2: Bootstrapping Language-Image Pre-training?",
    "output": "The code for BLIP-2: Bootstrapping Language-Image Pre-training is available at https://github.com/salesforce/LAVIS (salesforce/LAVIS)."
  },
  {
    "input": "Give me an overview of Deep Graph Library: A Graph-Centric, Highly-Performant Package.",
    "output": "Deep Graph Library: A Graph-Centric, Highly-Performant Package is a 2020 paper in Machine Learning. The implementation is available at dmlc/dgl which python package built to ease deep learning on graph, on top of existing dl frameworks. It is written in Python. The repository has 14,149 stars, indicating high community interest."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2302.12766&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2302.12766&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What is the GitHub repository for LLaMA-Adapter: Efficient Fine-tuning of Language Models?",
    "output": "The code for LLaMA-Adapter: Efficient Fine-tuning of Language Models is available at https://github.com/OpenGVLab/LLaMA-Adapter (OpenGVLab/LLaMA-Adapter)."
  },
  {
    "input": "How many stars does the Whisper: Robust Speech Recognition via Large-Scale Weak Supervision repository have?",
    "output": "The openai/whisper repository has 91,236 stars and 11,444 forks on GitHub."
  },
  {
    "input": "Give me an overview of TensorFlow: A System for Large-Scale Machine Learning.",
    "output": "TensorFlow: A System for Large-Scale Machine Learning is a 2016 paper in Distributed, Parallel, and Cluster Computing. The implementation is available at tensorflow/tensorflow which an open source machine learning framework for everyone It is written in C++. The repository has 192,543 stars, indicating high community interest."
  },
  {
    "input": "What category is the Proximal Policy Optimization Algorithms paper in?",
    "output": "Proximal Policy Optimization Algorithms is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Describe the amazon-science/mm-cot repository that implements arXiv Query: search_query=&id_list=2302.00923&start=0&max_results=10.",
    "output": "Official implementation for \"Multimodal Chain-of-Thought Reasoning in Language Models\" (stay tuned and more will be updated)"
  },
  {
    "input": "What is the GitHub repository for Adam: A Method for Stochastic Optimization?",
    "output": "The code for Adam: A Method for Stochastic Optimization is available at https://github.com/pytorch/pytorch (pytorch/pytorch)."
  },
  {
    "input": "What programming language is Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context implemented in?",
    "output": "The Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context implementation is written in Python."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2302.10866&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2302.10866&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at HazyResearch/safari which convolutions for sequence modeling It is written in Assembly. The repository has 904 stars, indicating high community interest."
  },
  {
    "input": "What language is the frankxu2004/knnlm-why repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2301.02828&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What category is the Adam: A Method for Stochastic Optimization paper in?",
    "output": "Adam: A Method for Stochastic Optimization is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Which field does Longformer: The Long-Document Transformer belong to?",
    "output": "Longformer: The Long-Document Transformer is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2009.06367&start=0&max_results=10 implementation?",
    "output": "The salesforce/GeDi repository has 209 stars and 46 forks on GitHub."
  },
  {
    "input": "What does the haoliuhl/language-quantized-autoencoders repository do?",
    "output": "Language Quantized AutoEncoders"
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2302.08242&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2302.08242&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Explain the microsoft/torchscale implementation of RetNet: Retentive Network: A Successor to Transformer.",
    "output": "Foundation Architecture for (M)LLMs"
  },
  {
    "input": "What paper has ArXiv ID 1910.01108?",
    "output": "TinyBERT: Distilling BERT for Natural Language Understanding"
  },
  {
    "input": "What paper has ArXiv ID 2104.05556?",
    "output": "Multi-Task Deep Neural Networks for Natural Language Understanding"
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2301.04104?",
    "output": "arXiv Query: search_query=&id_list=2301.04104&start=0&max_results=10"
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2109.07958?",
    "output": "MMLU: Measuring Massive Multitask Language Understanding"
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2303.02506&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2303.02506&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What programming language is LLaVA: Large Language and Vision Assistant implemented in?",
    "output": "The LLaVA: Large Language and Vision Assistant implementation is written in Python."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2503.01141&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2503.01141&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at LTH14/fractalgen which pytorch implementation of fractalgen https://arxiv.org/abs/2502.17437 It is written in Python. The repository has 1,196 stars, indicating high community interest."
  },
  {
    "input": "What topics does the BEiT: BERT Pre-Training of Image Transformers implementation cover?",
    "output": "The microsoft/unilm repository covers: nlp, pre-trained-model, unilm, minilm, layoutlm, layoutxlm, beit, document-ai, trocr, beit-3, foundation-models, xlm-e, deepnet, llm, multimodal, mllm, kosmos, kosmos-1, textdiffuser, bitnet."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 1706.02216?",
    "output": "Inductive Representation Learning on Large Graphs"
  },
  {
    "input": "Explain the S4: Efficiently Modeling Long Sequences paper.",
    "output": "S4: Efficiently Modeling Long Sequences is a 2021 paper in Machine Learning. The implementation is available at state-spaces/s4 which structured state space sequence models It is written in Jupyter Notebook. The repository has 2,783 stars, indicating high community interest."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2302.04870&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2302.04870&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What language is the zihangdai/xlnet repository written in?",
    "output": "The XLNet: Generalized Autoregressive Pretraining implementation is written in Python."
  },
  {
    "input": "Which field does SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities belong to?",
    "output": "SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2301.11316&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2301.11316&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What's the implementation repository for DeBERTa: Decoding-enhanced BERT with Disentangled Attention?",
    "output": "The code for DeBERTa: Decoding-enhanced BERT with Disentangled Attention is available at https://github.com/microsoft/DeBERTa (microsoft/DeBERTa)."
  },
  {
    "input": "How popular is the Llama 2: Open Foundation and Fine-Tuned Chat Models implementation?",
    "output": "The meta-llama/llama repository has 58,946 stars and 9,817 forks on GitHub."
  },
  {
    "input": "What paper has ArXiv ID 2110.07602?",
    "output": "P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning"
  },
  {
    "input": "When was the AlpacaEval: An Automatic Evaluator for Instruction-following paper published?",
    "output": "The AlpacaEval: An Automatic Evaluator for Instruction-following paper was published in 2023."
  },
  {
    "input": "What is the GitHub repository for LIMA: Less Is More for Alignment?",
    "output": "The code for LIMA: Less Is More for Alignment is available at https://github.com/meta-llama/llama (meta-llama/llama)."
  },
  {
    "input": "Tell me the title of ArXiv paper 2203.02155.",
    "output": "Training language models to follow instructions with human feedback"
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2006.16779&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2006.16779&start=0&max_results=10 is available at https://github.com/google-research/pegasus (google-research/pegasus)."
  },
  {
    "input": "Give me an overview of XLNet: Generalized Autoregressive Pretraining.",
    "output": "XLNet: Generalized Autoregressive Pretraining is a 2019 paper in Computation and Language (Natural Language Processing). The implementation is available at zihangdai/xlnet which xlnet: generalized autoregressive pretraining for language understanding It is written in Python. The repository has 6,179 stars, indicating high community interest."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2503.20201&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2503.20201&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What's the implementation repository for Dense Passage Retrieval for Open-Domain Question Answering?",
    "output": "The code for Dense Passage Retrieval for Open-Domain Question Answering is available at https://github.com/facebookresearch/DPR (facebookresearch/DPR)."
  },
  {
    "input": "Describe the farizrahman4u/seq2seq repository that implements Sequence to Sequence Learning with Neural Networks.",
    "output": "Sequence to Sequence Learning with Keras"
  },
  {
    "input": "What language is the facebookresearch/fairseq repository written in?",
    "output": "The HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction implementation is written in Python."
  },
  {
    "input": "Describe the fastai/fastai repository that implements Universal Language Model Fine-tuning for Text Classification.",
    "output": "The fastai deep learning library"
  },
  {
    "input": "What paper has ArXiv ID 2301.13688?",
    "output": "arXiv Query: search_query=&id_list=2301.13688&start=0&max_results=10"
  },
  {
    "input": "What paper has ArXiv ID 1905.09418?",
    "output": "BERT Rediscovers the Classical NLP Pipeline"
  },
  {
    "input": "What is Efficient Estimation of Word Representations in Vector Space about?",
    "output": "Efficient Estimation of Word Representations in Vector Space is a 2013 paper in Computation and Language (Natural Language Processing). The implementation is available at tmikolov/word2vec which automatically exported from code.google.com/p/word2vec It is written in C. The repository has 1,569 stars, indicating high community interest."
  },
  {
    "input": "What paper has ArXiv ID 2504.00698?",
    "output": "arXiv Query: search_query=&id_list=2504.00698&start=0&max_results=10"
  },
  {
    "input": "What does the bigcode-project/starcoder repository do?",
    "output": "Home of StarCoder: fine-tuning & inference!"
  },
  {
    "input": "Where can I find the code for Efficient Estimation of Word Representations in Vector Space?",
    "output": "The code for Efficient Estimation of Word Representations in Vector Space is available at https://github.com/tmikolov/word2vec (tmikolov/word2vec)."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2301.04104&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2301.04104&start=0&max_results=10 is available at https://github.com/google-deepmind/tracr (google-deepmind/tracr)."
  },
  {
    "input": "Which field does Finetuned Language Models Are Zero-Shot Learners belong to?",
    "output": "Finetuned Language Models Are Zero-Shot Learners is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2301.11305&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2301.11305&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "Explain the Reflexion: Language Agents with Verbal Reinforcement Learning paper.",
    "output": "Reflexion: Language Agents with Verbal Reinforcement Learning is a 2023 paper in Artificial Intelligence. The implementation is available at noahshinn/reflexion which [neurips 2023] reflexion: language agents with verbal reinforcement learning It is written in Python. The repository has 2,959 stars, indicating high community interest."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2503.23513&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2503.23513&start=0&max_results=10 is available at https://github.com/MAXNORM8650/MedAgentSim (MAXNORM8650/MedAgentSim)."
  },
  {
    "input": "What is the ArXiv category of Denoising Diffusion Implicit Models?",
    "output": "Denoising Diffusion Implicit Models is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "When was the ChemCrow: Augmenting large-language models with chemistry tools paper published?",
    "output": "The ChemCrow: Augmenting large-language models with chemistry tools paper was published in 2023."
  },
  {
    "input": "Summarize the ColBERT: Efficient Passage Search via Contextualized Late Interaction paper.",
    "output": "ColBERT: Efficient Passage Search via Contextualized Late Interaction is a 2021 paper in Information Retrieval. The implementation is available at stanford-futuredata/ColBERT which colbert: state-of-the-art neural search (sigir'20, tacl'21, neurips'21, naacl'22, cikm'22, acl'23, emnlp'23) It is written in Python. The repository has 3,707 stars, indicating high community interest."
  },
  {
    "input": "Which field does Reformer: The Efficient Transformer belong to?",
    "output": "Reformer: The Efficient Transformer is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2303.02506&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2303.02506&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at NVlabs/prismer which the implementation of \"prismer: a vision-language model with multi-task experts\". It is written in Python. The repository has 1,307 stars, indicating high community interest."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2302.12766&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2302.12766&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What's the implementation repository for Neural Collaborative Filtering?",
    "output": "The code for Neural Collaborative Filtering is available at https://github.com/hexiangnan/neural_collaborative_filtering (hexiangnan/neural_collaborative_filtering)."
  },
  {
    "input": "What category is the Reformer: The Efficient Transformer paper in?",
    "output": "Reformer: The Efficient Transformer is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2109.03264&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2109.03264&start=0&max_results=10 is a 2021 paper in Machine Learning. The implementation is available at facebookresearch/fairseq which facebook ai research sequence-to-sequence toolkit written in python. It is written in Python. The repository has 31,988 stars, indicating high community interest."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2302.04870&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2302.04870&start=0&max_results=10 is available at https://github.com/mit-han-lab/offsite-tuning (mit-han-lab/offsite-tuning)."
  },
  {
    "input": "What is the ArXiv category of Denoising Diffusion Probabilistic Models?",
    "output": "Denoising Diffusion Probabilistic Models is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2503.22678&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2503.22678&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at MAXNORM8650/MedAgentSim which medagentsim: self-evolving multi-agent simulations for realistic clinical interactions, miccai 2025 (oral and early accepted) It is written in Python. The repository has 94 stars, indicating high community interest."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2006.03511&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2006.03511&start=0&max_results=10 is available at https://github.com/facebookresearch/TransCoder (facebookresearch/TransCoder)."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 1711.00489?",
    "output": "Horovod: fast and easy distributed deep learning in TensorFlow"
  },
  {
    "input": "What's the implementation repository for Text Embeddings by Weakly-Supervised Contrastive Pre-training?",
    "output": "The code for Text Embeddings by Weakly-Supervised Contrastive Pre-training is available at https://github.com/microsoft/unilm (microsoft/unilm)."
  },
  {
    "input": "What topics does the Time-LLM: Time Series Forecasting by Reprogramming Large Language Models implementation cover?",
    "output": "The KimMeen/Time-LLM repository covers: cross-modal-learning, cross-modality, deep-learning, language-model, large-language-models, machine-learning, multimodal-deep-learning, multimodal-time-series, prompt-tuning, time-series, time-series-analysis, time-series-forecast, time-series-forecasting."
  },
  {
    "input": "How popular is the Reformer: The Efficient Transformer implementation?",
    "output": "The google/trax repository has 8,293 stars and 828 forks on GitHub."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2301.01751&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2301.01751&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2505.22101&start=0&max_results=10 repository have?",
    "output": "The ruixin31/Spurious_Rewards repository has 342 stars and 20 forks on GitHub."
  },
  {
    "input": "What are the key topics in the rajpurkar/SQuAD-explorer repository?",
    "output": "The rajpurkar/SQuAD-explorer repository covers: dataset, visual-analysis, leaderboard."
  },
  {
    "input": "Give me an overview of MMLU: Measuring Massive Multitask Language Understanding.",
    "output": "MMLU: Measuring Massive Multitask Language Understanding is a 2021 paper in cs.CY. The implementation is available at hendrycks/test which measuring massive multitask language understanding | iclr 2021 It is written in Python. The repository has 1,523 stars, indicating high community interest."
  },
  {
    "input": "What does the Open-Reasoner-Zero/Open-Reasoner-Zero repository do?",
    "output": "Official Repo for Open-Reasoner-Zero"
  },
  {
    "input": "What topics does the MMLU: Measuring Massive Multitask Language Understanding implementation cover?",
    "output": "The hendrycks/test repository covers: muti-task, transfer-learning, gpt-3, few-shot-learning."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2302.07459&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2302.07459&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at haoliuhl/language-quantized-autoencoders which language quantized autoencoders It is written in Python. The repository has 111 stars, indicating high community interest."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2312.02120&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2312.02120&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at ContextualAI/HALOs which a library with extensible implementations of dpo, kto, ppo, orpo, and other human-aware loss functions (halos). It is written in Python. The repository has 893 stars, indicating high community interest."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2405.04324&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2405.04324&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "How many stars does the Grounding DINO: Marrying DINO with Grounded Pre-Training repository have?",
    "output": "The IDEA-Research/GroundingDINO repository has 9,324 stars and 969 forks on GitHub."
  },
  {
    "input": "In what year did Wide & Deep Learning for Recommender Systems come out?",
    "output": "The Wide & Deep Learning for Recommender Systems paper was published in 2016."
  },
  {
    "input": "How popular is the An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale implementation?",
    "output": "The google-research/vision_transformer repository has 12,040 stars and 1,432 forks on GitHub."
  },
  {
    "input": "In what year did LLM.int8(): 8-bit Matrix Multiplication for Transformers come out?",
    "output": "The LLM.int8(): 8-bit Matrix Multiplication for Transformers paper was published in 2022."
  },
  {
    "input": "How popular is the BERT: Pre-training of Deep Bidirectional Transformers implementation?",
    "output": "The google-research/bert repository has 39,687 stars and 9,711 forks on GitHub."
  },
  {
    "input": "What does the LTH14/fractalgen repository do?",
    "output": "PyTorch implementation of FractalGen https://arxiv.org/abs/2502.17437"
  },
  {
    "input": "What is the title of the paper with ArXiv ID 1910.01741?",
    "output": "Dream to Control: Learning Behaviors by Latent Imagination"
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2303.04129&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2303.04129&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at HazyResearch/safari which convolutions for sequence modeling It is written in Assembly. The repository has 904 stars, indicating high community interest."
  },
  {
    "input": "Where can I find the code for XLNet: Generalized Autoregressive Pretraining?",
    "output": "The code for XLNet: Generalized Autoregressive Pretraining is available at https://github.com/zihangdai/xlnet (zihangdai/xlnet)."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2302.14838&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2302.14838&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What paper has ArXiv ID 1606.07792?",
    "output": "Wide & Deep Learning for Recommender Systems"
  },
  {
    "input": "Which field does HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction belong to?",
    "output": "HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "How popular is the Codex: Evaluating Large Language Models Trained on Code implementation?",
    "output": "The openai/human-eval repository has 3,030 stars and 422 forks on GitHub."
  },
  {
    "input": "What language is the artidoro/qlora repository written in?",
    "output": "The QLoRA: Efficient Finetuning of Quantized LLMs implementation is written in Jupyter Notebook."
  },
  {
    "input": "How popular is the EfficientDet: Scalable and Efficient Object Detection implementation?",
    "output": "The google/automl repository has 6,425 stars and 1,463 forks on GitHub."
  },
  {
    "input": "What are the key topics in the MAXNORM8650/MedAgentSim repository?",
    "output": "The MAXNORM8650/MedAgentSim repository covers: multi-agent-simulations."
  },
  {
    "input": "What paper has ArXiv ID 2404.03592?",
    "output": "arXiv Query: search_query=&id_list=2404.03592&start=0&max_results=10"
  },
  {
    "input": "What language is the facebookresearch/fastText repository written in?",
    "output": "The Enriching Word Vectors with Subword Information implementation is written in HTML."
  },
  {
    "input": "Tell me the title of ArXiv paper 1506.02640.",
    "output": "Faster R-CNN: Towards Real-Time Object Detection"
  },
  {
    "input": "What's the implementation repository for EfficientDet: Scalable and Efficient Object Detection?",
    "output": "The code for EfficientDet: Scalable and Efficient Object Detection is available at https://github.com/google/automl (google/automl)."
  },
  {
    "input": "What is the GitHub repository for Prefix-Tuning: Optimizing Continuous Prompts for Generation?",
    "output": "The code for Prefix-Tuning: Optimizing Continuous Prompts for Generation is available at https://github.com/XiangLi1999/PrefixTuning (XiangLi1999/PrefixTuning)."
  },
  {
    "input": "What is the GitHub repository for Code Llama: Open Foundation Models for Code?",
    "output": "The code for Code Llama: Open Foundation Models for Code is available at https://github.com/meta-llama/codellama (meta-llama/codellama)."
  },
  {
    "input": "Where can I find the code for S4: Efficiently Modeling Long Sequences?",
    "output": "The code for S4: Efficiently Modeling Long Sequences is available at https://github.com/state-spaces/s4 (state-spaces/s4)."
  },
  {
    "input": "What topics does the Dream to Control: Learning Behaviors by Latent Imagination implementation cover?",
    "output": "The danijar/dreamer repository covers: reinforcement-learning, artificial-intelligence, deep-learning, robotics, world-models."
  },
  {
    "input": "What is the GitHub repository for SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding?",
    "output": "The code for SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding is available at https://github.com/nyu-mll/jiant (nyu-mll/jiant)."
  },
  {
    "input": "How popular is the Mamba: Linear-Time Sequence Modeling with Selective State Spaces implementation?",
    "output": "The state-spaces/mamba repository has 16,525 stars and 1,505 forks on GitHub."
  },
  {
    "input": "What language is the facebookresearch/fairseq repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2102.01192&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "In what year did Dense Passage Retrieval for Open-Domain Question Answering come out?",
    "output": "The Dense Passage Retrieval for Open-Domain Question Answering paper was published in 2020."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2403.17887?",
    "output": "arXiv Query: search_query=&id_list=2403.17887&start=0&max_results=10"
  },
  {
    "input": "Tell me the title of ArXiv paper 1910.02054.",
    "output": "ZeRO: Memory Optimizations Toward Training Trillion Parameter Models"
  },
  {
    "input": "What is the GitHub repository for Text Embeddings by Weakly-Supervised Contrastive Pre-training?",
    "output": "The code for Text Embeddings by Weakly-Supervised Contrastive Pre-training is available at https://github.com/microsoft/unilm (microsoft/unilm)."
  },
  {
    "input": "Give me an overview of Know What You Don't Know: Unanswerable Questions for SQuAD.",
    "output": "Know What You Don't Know: Unanswerable Questions for SQuAD is a 2018 paper in Computation and Language (Natural Language Processing). The implementation is available at rajpurkar/SQuAD-explorer which visually explore the stanford question answering dataset It is written in JavaScript. The repository has 570 stars, indicating high community interest."
  },
  {
    "input": "Where can I find the code for Semi-Supervised Classification with Graph Convolutional Networks?",
    "output": "The code for Semi-Supervised Classification with Graph Convolutional Networks is available at https://github.com/tkipf/gcn (tkipf/gcn)."
  },
  {
    "input": "What category is the Deep Residual Learning for Image Recognition paper in?",
    "output": "Deep Residual Learning for Image Recognition is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "Tell me the title of ArXiv paper 2005.11401.",
    "output": "RAG: Retrieval-Augmented Generation"
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2504.19413&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2504.19413&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at MoonshotAI/Kimi-Audio which kimi-audio, an open-source audio foundation model excelling in audio understanding, generation, and conversation It is written in Python. The repository has 4,367 stars, indicating high community interest."
  },
  {
    "input": "Explain the Generating Wikipedia by Summarizing Long Sequences paper.",
    "output": "Generating Wikipedia by Summarizing Long Sequences is a 2018 paper in Computation and Language (Natural Language Processing). The implementation is available at tensorflow/tensor2tensor which library of deep learning models and datasets designed to make deep learning more accessible and accelerate ml research. It is written in Python. The repository has 16,760 stars, indicating high community interest."
  },
  {
    "input": "In what year did Inductive Representation Learning on Large Graphs come out?",
    "output": "The Inductive Representation Learning on Large Graphs paper was published in 2017."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2301.13188&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2301.13188&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2502.09992&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2502.09992&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at Open-Reasoner-Zero/Open-Reasoner-Zero which official repo for open-reasoner-zero It is written in Python. The repository has 2,068 stars, indicating high community interest."
  },
  {
    "input": "Describe the hpcaitech/Open-Sora repository that implements arXiv Query: search_query=&id_list=2406.12824&start=0&max_results=10.",
    "output": "Open-Sora: Democratizing Efficient Video Production for All"
  },
  {
    "input": "Which field does Universal Language Model Fine-tuning for Text Classification belong to?",
    "output": "Universal Language Model Fine-tuning for Text Classification is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What paper has ArXiv ID 2301.11316?",
    "output": "arXiv Query: search_query=&id_list=2301.11316&start=0&max_results=10"
  },
  {
    "input": "How popular is the PyTorch Geometric: Library for Geometric Deep Learning implementation?",
    "output": "The pyg-team/pytorch_geometric repository has 23,186 stars and 3,921 forks on GitHub."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2109.01652?",
    "output": "Finetuned Language Models Are Zero-Shot Learners"
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2504.11844&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2504.11844&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2503.01141&start=0&max_results=10 implementation?",
    "output": "The LTH14/fractalgen repository has 1,196 stars and 65 forks on GitHub."
  },
  {
    "input": "Explain the Dao-AILab/flash-attention implementation of FlashAttention-2: Faster Attention with Better Parallelism.",
    "output": "Fast and memory-efficient exact attention"
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2504.21801&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2504.21801&start=0&max_results=10 is available at https://github.com/MoonshotAI/Kimi-Audio (MoonshotAI/Kimi-Audio)."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2301.04104&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2301.04104&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at google-deepmind/tracr It is written in Python. The repository has 548 stars, indicating high community interest."
  },
  {
    "input": "Give me an overview of Med-PaLM 2: Towards Expert-Level Medical Question Answering.",
    "output": "Med-PaLM 2: Towards Expert-Level Medical Question Answering is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at google-research/google-research which google research It is written in Jupyter Notebook. The repository has 36,800 stars, indicating high community interest."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2501.13824&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2501.13824&start=0&max_results=10 is available at https://github.com/plurai-ai/intellagent (plurai-ai/intellagent)."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2302.03011&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2302.03011&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at mit-han-lab/offsite-tuning which offsite-tuning: transfer learning without full model It is written in Python. The repository has 382 stars, indicating high community interest."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2504.20734&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2504.20734&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at MoonshotAI/Kimi-Audio which kimi-audio, an open-source audio foundation model excelling in audio understanding, generation, and conversation It is written in Python. The repository has 4,367 stars, indicating high community interest."
  },
  {
    "input": "What is the ArXiv category of Lookahead Optimizer: k steps forward, 1 step back?",
    "output": "Lookahead Optimizer: k steps forward, 1 step back is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2302.09778&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2302.09778&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at ali-vilab/composer which official implementation of \"composer: creative and controllable image synthesis with composable conditions\" The repository has 1,560 stars, indicating high community interest."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2303.06865&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2303.06865&start=0&max_results=10 is available at https://github.com/FMInference/FlexLLMGen (FMInference/FlexLLMGen)."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2405.02803&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2405.02803&start=0&max_results=10 is a 2024 paper in Machine Learning. The implementation is available at ibm-granite/granite-code-models which granite code models: a family of open foundation models for code intelligence The repository has 1,240 stars, indicating high community interest."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2504.02732&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2504.02732&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at MAXNORM8650/MedAgentSim which medagentsim: self-evolving multi-agent simulations for realistic clinical interactions, miccai 2025 (oral and early accepted) It is written in Python. The repository has 94 stars, indicating high community interest."
  },
  {
    "input": "Which field does Diffusion Models Beat GANs on Image Synthesis belong to?",
    "output": "Diffusion Models Beat GANs on Image Synthesis is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "Describe the allenai/codescientist repository that implements arXiv Query: search_query=&id_list=2504.00698&start=0&max_results=10.",
    "output": "CodeScientist: An automated scientific discovery system for code-based experiments"
  }
]